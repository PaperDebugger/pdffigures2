{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import re\n",
    "os.chdir('../')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "json_file = 'data/Dupin__V1_2_782756e5-c517-4dab-a954-a0e4a878beb4.json'\n",
    "with open(json_file, 'r') as file:\n",
    "    data = json.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from helpers import process_json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_text = process_json(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('data/dupin_full_text.txt', 'w') as file:\n",
    "    file.write(full_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def extract_headers_and_content(text):\n",
    "    # Split the text into lines\n",
    "    lines = text.strip().split('\\n')\n",
    "    headers = []\n",
    "    content = {}\n",
    "    current_header = None\n",
    "    current_content = []\n",
    "\n",
    "    for line in lines:\n",
    "        line = line.strip()\n",
    "\n",
    "        if not line:\n",
    "            continue  # Skip empty lines\n",
    "\n",
    "        # Try to match header pattern 1: Lines that start with uppercase letters (headers may be in uppercase)\n",
    "        match1 = re.match(r'^([A-Z ]{2,})(.*)$', line)\n",
    "        # Try to match header pattern 2: Lines that start with numbers and periods\n",
    "        match2 = re.match(r'^(\\d+(\\.\\d+)*\\s+)(.*)$', line)\n",
    "\n",
    "        if match1:\n",
    "            header = match1.group(1).strip()\n",
    "            rest = match1.group(2).strip()\n",
    "\n",
    "            if current_header:\n",
    "                # Store the previous header and its content\n",
    "                content[current_header] = '\\n'.join(current_content).strip()\n",
    "                headers.append(current_header)\n",
    "\n",
    "            current_header = header\n",
    "            current_content = []\n",
    "            if rest:\n",
    "                current_content.append(rest)\n",
    "\n",
    "        elif match2:\n",
    "            header = line.strip()\n",
    "            if current_header:\n",
    "                # Store the previous header and its content\n",
    "                content[current_header] = '\\n'.join(current_content).strip()\n",
    "                headers.append(current_header)\n",
    "\n",
    "            current_header = header\n",
    "            current_content = []\n",
    "        else:\n",
    "            if current_header:\n",
    "                current_content.append(line)\n",
    "\n",
    "    # Store the last header and its content\n",
    "    if current_header and current_content:\n",
    "        content[current_header] = '\\n'.join(current_content).strip()\n",
    "        headers.append(current_header)\n",
    "    elif current_header and current_header not in content:\n",
    "        content[current_header] = ''\n",
    "        headers.append(current_header)\n",
    "\n",
    "    return content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'ABSTRACT D': 'etecting fraudulent activities in financial and e-commerce transac-\\ntion networks is crucial, and one effective method for this is Densest Subgraph Discovery (DSD). However, deploying DSD methods in production systems faces substantial scalability challenges due to the predominantly sequential nature of existing methods, which impedes their ability to handle large-scale transaction networks and results in significant detection delays. In this paper, we introduce Dupin, a novel parallel processing framework designed for efficient DSD processing in billion-scale graphs. Dupin is powered by a processing engine that exploits the unique properties of the peeling process, with theoretical guarantees on detection quality and efficiency.Dupin provides user-friendly APIs for flexible customization of DSD objectives and ensures robust adaptability to diverse fraud detection scenarios. Empirical evaluations indicate that Dupin outperforms several existing DSD methods, with performance improvements observed in specific scenarios reaching up to two orders of magnitude. On billion-scale graphs, Dupin demonstrates the potential to enhance the prevention of fraudulent transactions by approximately 49.5 basis points and reduce density error from 30% to below 5%, as supported by our experimental results. ACM Reference Format: Anonymous Author(s). 2024. Dupin: A Parallel Densest Subgraph Discovery Framework on Massive Graphs. In Proceedings of ACM Conference (Conference’17). ACM, New York, NY, USA, 14 pages. https://doi.org/10.1145/ nnnnnnn.nnnnnnn',\n",
       " '1 INTRODUCTION': 'Graph structures pervade numerous contemporary applications, ranging from transaction and communication networks to expan- sive social media platforms. The study of densest subgraph discovery (DSD), first explored by [21], has since proven instrumental in various domains: link spam identification [4, 19], community de- tection [9, 17], and notably, fraud detection [8, 24, 42]. Central to this domain is the peeling process [2, 5, 8, 24, 46], which operates by iteratively removing vertices based on density metrics, such as vertex degree or the cumulative weight of adjacent edges. Their widespread adoption can be attributed to their inherent efficiency, robustness, and proven worst-case performance guarantees for DSD. In practice, business requirements demand the detection of fraud- ulent communities on billion-scale graphs within seconds. Recent Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for components of this work owned by others than ACM must be honored. Abstracting with credit is permitted. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. Request permissions from permissions@acm.org. Conference’17, July 2017, Washington, DC, USA © 2024 Association for Computing Machinery. ACM ISBN 978-x-xxxx-xxxx-x/YY/MM. . . $15.00 https://doi.org/10.1145/nnnnnnn.nnnnnnn studies, including those by [1] and [47], have highlighted the perva- sive issue of malicious bot activity in e-commerce, accounting for a substantial 21.4% of all traffic in 2018. These fraudulent activities often happen quickly, making them hard to detect. The inability to efficiently process and analyze such massive graphs not only ham- pers fraud detection efforts but also exposes businesses to substan- tial financial risks and reputational damage. Therefore, addressing these challenges is not merely an academic exercise; it is a press- ing necessity for enhancing the integrity of digital transactions and safeguarding consumer trust. Despite extensive research onDSD for static graphs [20–22, 30, 33, 34], dynamic graphs [2, 18, 26, 28, 43], and parallel approaches [14, 16, 41, 42], existing methods face significant challenges for practical de- ployment in real-world fraud detection systems. For instance, many existing algorithms primarily rely on static score functions for DSD, lacking the flexibility required for dynamic and evolving fraud detec- tion scenarios. Methods such as [4] and [27] are designed with fixed detection criteria, which can become less effective as fraudulent be- haviors adapt over time. Additionally, incremental algorithms remain inefficient at handling billion-scale graphs with high transaction volumes, leading to increased latency. Parallel DSD frameworks often lack efficient pruning techniques, resulting in excessive iterations that degrade performance on large-scale graphs. This paper addresses the following research question: How can we enhance the efficiency and scalability of DSD methods to effectively detect fraud in massive graphs? Challenges. We evaluated existing methods with our industry partner, Anon (anonymized for review). The following issues surfaced: Firstly, existing algorithms focus on static score functions for DSD and lack flexible customization. While this may be adequate for short-term detection, it becomes less effective over time. For example, as shown in Figure 1a, graph density can accumulate sig- nificant errors within a single day, highlighting the need for more adaptive and customizable detection methods. Secondly, existing incremental algorithms are still inefficient at handling billion-scale graphs with high transaction volumes, leading to increased latency. This limitation is evident in Figure 1b, which illustrates the system load distribution for a specific day’s transactions, underscoring the inability of current methods to efficiently manage high transaction volumes without significant delays. Thirdly, existing parallel densest subgraph discovery frameworks lack efficient pruning techniques, re- sulting in a large number of iterations. This inefficiency is especially problematic on billion-scale graphs, where performance degrades substantially due to the excessive computational overhead caused by numerous iterations. Adapting to Evolving Research Needs. Recognizing the dynamic nature of fraud detection and the rapid evolution of fraudulent tactics, we have adapted our research focus to address these challenges. Our approach incorporates flexible customization options for DSD methods, allowing for adjustments based on the changing landscape of fraudulent activities. This adaptability is crucial for maintaining the effectiveness of fraud detection systems in an environment where new threats continuously emerge. Contributions. In this work, we propose a DSD framework for fraud detection. Our key contributions are as follows: • A DSD Parallelization Framework. We propose Dupin, a novel parallel processing framework for densest subgraph discovery (DSD) that breaks the sequential dependencies inherent in tra- ditional density metrics, significantly enhancing scalability and efficiency on both weighted and unweighted graphs. Unlike existing methods that focus on specific density metrics, Dupin allows developers to define custom fraud detection semantics through flexible suspiciousness functions for edges and vertices. It supports a wide range of DSD semantics, including DG [6], DW [23], FD [24], TDS [46], and 𝑘CLiDS [14]. Importantly, Dupin not only leverages parallelism but also provides theoretical guarantees on approximation ratios for the supported density metrics. To the best of our knowledge, this is the first generic parallel processing framework that accommodates a broad class of density metrics in DSD, offering both flexibility and theoretical assurance. • Exploiting Long-Tail Properties.We identify and address the long-tail problem in the peeling process of DSD algorithms, where late iterations remove only a small fraction of vertices and con- tribute little to the quality of the densest subgraph, leading to inefficiency. To tackle this, we introduce two novel optimizations: Global Pruning Optimization, which proactively removes low-contribution vertices within each peeling iteration to reduce computational overhead, and Local Pruning Optimization, which eliminates disparate structures formed after vertex removal to enhance the density and quality of the detected subgraph. These optimizations significantly reduce the number of iterations re- quired, improving both performance and result quality. • Empirical Validation and Real-World Impact: We conduct comprehensive experiments on large-scale industry datasets to validate the effectiveness of Dupin. Our results show that Dupin achieves up to 100 times faster fraud detection compared to the state-of-the-art incremental method Spade [28] and parallel methods like PBBS [41]. Moreover,Dupin is capable of preventing up to 94.5% of potential fraud, demonstrating significant practical bene- fits and reinforcing system integrity in real-world applications. Organization. The remainder of this paper is structured as follows: Section 2 presents the background and the literature review. Section 3 introduces the architecture of Dupin. In Section 4, we propose the theoretical foundations for parallel peeling-based algorithms for DSD. Section 5 introduces long-tail pruning techniques to improve efficiency and effectiveness. The experimental evaluation of Dupin is presented in Section 6. Finally, Section 7 concludes the paper and outlines avenues for future research.',\n",
       " '2 BACKGROUND AND RELATEDWORK': '',\n",
       " '2.1 Preliminary': 'This subsection presents the preliminary. Some frequently used no- tations are summarized in Table 1. Graph 𝐺 . In this work, we operate on a weighted graph denoted as 𝐺 = (𝑉 , 𝐸), where 𝑉 represents the set of vertices and 𝐸 (⊆ 𝑉 ×𝑉 ) represents the set of edges. Each edge (𝑢𝑖 , 𝑢 𝑗 ) ∈ 𝐸 is assigned a nonnegative weight, expressed as 𝑐𝑖 𝑗 . Additionally, 𝑁 (𝑢) denotes the set of neighboring vertices connected to vertex 𝑢. Induced Subgraph. For a given subset 𝑆 of vertices in 𝑉 , we define the induced subgraph as 𝐺[𝑆] = (𝑆, 𝐸[𝑆]), where 𝐸[𝑆] = {(𝑢, 𝑣) | (𝑢, 𝑣) ∈ 𝐸 ∧ 𝑢, 𝑣 ∈ 𝑆}. The size of the subset 𝑆 is denoted by |𝑆 |. Density Metrics 𝑔 and Weight Function 𝑓 . We utilize the class of metrics𝑔 as defined in prior works [6, 24, 28], where for a given subset of vertices 𝑆 ⊆ 𝑉 , the density metric is computed as: 𝑔(𝑆) = 𝑓 (𝑆) |𝑆 | . Here, 𝑓 (𝑆) represents the total weight of the induced subgraph 𝐺[𝑆], which is defined as the sum of the weights of vertices in 𝑆 and the weights of edges in 𝐸[𝑆]. More formally: 𝑓 (𝑆) = ∑︁ 𝑢𝑖 ∈𝑆 𝑎𝑖 + ∑︁ (𝑢𝑖 ,𝑢 𝑗 )∈𝐸[𝑆] 𝑐𝑖 𝑗 (1) The weight of a vertex 𝑢𝑖 , denoted as 𝑎𝑖 (𝑎𝑖 ≥ 0), quantifies the suspiciousness of user 𝑢𝑖 . The weight of an edge (𝑢𝑖 , 𝑢 𝑗 ), represented by 𝑐𝑖 𝑗 (𝑐𝑖 𝑗 > 0), reflects the suspiciousness of the transaction between users 𝑢𝑖 and 𝑢 𝑗 . Intuitively, the density metric 𝑔(𝑆) encapsulates the overall suspiciousness density of the induced subgraph𝐺[𝑆]. A larger value of 𝑔(𝑆) indicates a higher density of suspiciousness within𝐺[𝑆]. The vertex set that maximizes the density metric 𝑔 is denoted by 𝑆∗. We next introduce five representative density metrics extensively employed in many fraud detection applications. The density metrics are mainly differed by how the weight function is defined. Dense Subgraphs (DG) [6]. The DG metric is employed to quan- tify the connectivity of substructures within a graph. It has found extensive applications in various domains, such as detecting fake comments in online platforms [29] and identifying fraudulent activities in social networks [3]. Given a subset of vertices 𝑆 ⊆ 𝑉 , the weight function of DG is defined as 𝑓 (𝑆) = |𝐸[𝑆]|, where |𝐸[𝑆]| denotes the number of edges in the induced subgraph 𝐺[𝑆]. Dense Subgraph on Weighted Graphs (DW) [23]. In transac- tion graphs, it is common to have weights assigned to the edges, representing quantities such as the transaction amount. The DW metric extends the concept of dense subgraphs to accommodate these weighted relationships. Given a subset of vertices 𝑆 ⊆ 𝑉 , the weight function of DW is defined as 𝑓 (𝑆) = ∑ (𝑢𝑖 ,𝑢 𝑗 )∈𝐸[𝑆] 𝑐𝑖 𝑗 . Fraudar (FD) [24]. In order to mitigate the effects of fraudulent actors deliberately obfuscating their activities, Hooi et al. [24] introduced the FD algorithm. This approach innovatively incorporates weights for edges and assigns prior suspiciousness scores to vertices, potentially utilizing side information to enhance detection accuracy. Given a subset of vertices 𝑆 ⊆ 𝑉 , the weight function of FD is defined as Equation 1 where 𝑎𝑖 denotes the suspicious score of vertex 𝑢𝑖 and 𝑐𝑖 𝑗 = 1 log(𝑥+𝑐) . Here 𝑥 is the degree of the object vertex between 𝑢𝑖 and 𝑢 𝑗 , and 𝑐 is a positive constant [24]. Triangle Densest Subgraph (TDS) [46]. Triangles serve as a crucial structural motif in graphs, providing valuable insights into the connectivity and cohesiveness of subgraphs. The TDS metric, in par- ticular, leverages the prevalence of triangles to quantify the density of a subgraph. Given a subset of vertices 𝑆 ⊆ 𝑉 , the weight function of TDS is defined as 𝑓 (𝑆) = 𝑡 (𝑆) where 𝑡 (𝑆) represents the total number of triangles within the induced subgraph 𝐺[𝑆]. 𝑘-Clique Densest Subgraph (𝑘CLiDS) [14]. Extending the concept of triangle densest subgraphs, 𝑘CLiDS focuses on identifying dense subgraphs based on the presence of 𝑘-Cliques. A 𝑘-Clique is a complete subgraph of 𝑘 vertices, representing a tightly-knit group of vertices. For a given subset of vertices 𝑆 ⊆ 𝑉 , the weight function of 𝑘CLiDS is defined as 𝑓 (𝑆) = 𝑘(𝑆), where 𝑘(𝑆) denotes the number of 𝑘-Cliques within the induced subgraph 𝐺[𝑆].',\n",
       " '2.2 Sequential Peeling Algorithms': 'Peeling algorithms have gained popularity for their efficiency, ro- bustness, and proven worst-case performance in various applica- tions [6, 24, 46]. Next, we provide a concise overview of the typical execution flow of these algorithms. Peeling Weight.We use𝑤𝑢𝑖 (𝑆) to indicate the decrease in the value of the weight function 𝑓 when the vertex𝑢𝑖 is removed from a vertex set 𝑆 , i.e., the peeling weight. Formally, 𝑤𝑢𝑖 (𝑆) = 𝑎𝑖 + ∑︁ (𝑢 𝑗 ∈𝑆) ∧ ((𝑢𝑖 ,𝑢 𝑗 )∈𝐸) 𝑐𝑖 𝑗 + ∑︁ (𝑢 𝑗 ∈𝑆) ∧ ((𝑢 𝑗 ,𝑢𝑖 )∈𝐸) 𝑐 𝑗𝑖 (2) Algorithm 1: Sequential Peeling Algorithm Input: A graph𝐺 = (𝑉 , 𝐸) and a density metric 𝑔(𝑆) Output: A subset of vertices 𝑆 that maximizes the density metric 𝑔(𝑆) 1 𝑆0 ← 𝑉 /* Initialize the subset with all vertices */ 2 for 𝑖 ← 1 to |𝑉 | do 3 𝑢 ← arg max𝑣∈𝑆𝑖−1 𝑔(𝑆𝑖−1 \\\\ {𝑣 }) /* The vertex to be peeled */ 4 𝑆𝑖 ← 𝑆𝑖−1 \\\\ {𝑢 } /* The selected vertex */ 5 return arg max𝑆𝑖 𝑔(𝑆𝑖 ) /* The subset that maximizes 𝑔(𝑆) */ Sequential PeelingAlgorithm (Algorithm1).The peeling process starts with the full vertex set 𝑆0 = 𝑉 (Line 1) and iteratively removes vertices to maximize the density metric 𝑔. In each step 𝑖 , a vertex𝑢𝑖 is removed from 𝑆𝑖−1 to form 𝑆𝑖 , such that 𝑔(𝑆𝑖−1 \\\\ {𝑢𝑖 }) is maximized (Lines 3∼4). This process is repeated until 𝑆𝑖 becomes empty, resulting in a sequence of nested sets 𝑆0, 𝑆1, . . . , 𝑆 |𝑉 | . The algorithm ultimately returns the subset 𝑆𝑖 ∈ {𝑆0, . . . , 𝑆 |𝑉 |} that maximizes 𝑔(𝑆𝑖 ). Example 2.1 (Seqential Peeling Algorithm Process). Consider the graph 𝐺 as shown in Figure 2. The process begins with an initial graph density of 2.33. In the first iteration, vertex 𝑢1 is peeled due to its smallest peeling weight among all vertices. Subsequently, vertices are peeled in the following order: 𝑢2, 𝑢3, 𝑢4, 𝑢5, and 𝑢6 based on the updated criteria after each peeling operation. Notably, after peeling 𝑢2, the graph density increases, which is a characteristic observation in the peeling process. The sequence of peeling continues until the last vertex, 𝑢6, is peeled, resulting in a final graph density of 0. The peeling sequence, therefore, is 𝑂 = [𝑢1, 𝑢2, 𝑢3, 𝑢4, 𝑢5, 𝑢6], completely disassembling the graph. The induced subgraph𝐺[𝑆] of 𝑆 = [𝑢3, 𝑢4, 𝑢5, 𝑢6] has the greatest density of 2.75, thus 𝐺[𝑆] is returned. Approximation. An algorithm 𝑄 is defined as an 𝛼-approximation for the Densest Subgraph Discovery (DSD), where 𝛼 ≥ 1, if for any given graph it returns a subset 𝑆 satisfying the condition: 𝑔(𝑆) ≥ 𝑔(𝑆∗) 𝛼 , where 𝑆∗ is the optimal subset that maximizes the density metric 𝑔. Theorem 2.1 ([24]). For the vertex set 𝑆𝑝 returned by Algorithm 1 and the optimal vertex set 𝑆∗, it holds that 𝑔(𝑆𝑝 ) ≥ 𝑔(𝑆∗) 2 for DG, DW and FD as the density metrics. We extend the analysis to TDS and 𝑘CLiDS. The proof of the following theorem is omitted due to space limitation. Theorem 2.2. For the vertex set 𝑆𝑝 returned by Algorithm 1 and the optimal vertex set 𝑆∗, it holds that 𝑔(𝑆𝑝 ) ≥ 𝑔(𝑆∗) 𝑘 for TDS and 𝑘CLiDS as the density metrics where 𝑘 = 3 for TDS.',\n",
       " '2.3 Related work': 'Densest Subgraph Discovery (DSD). Densest subgraph discov- ery represents a core area of research in graph theory, extensively explored in literature [7, 8, 20, 30, 39, 40]. The foundational max-flow- based algorithm by Goldberg et al. [21] set a benchmark for exact densest subgraph detection, with subsequent studies introducing more scalable exact solutions [33, 34]. While speed enhancements have been achieved through various greedy approaches [6, 8, 24, 33], these methods are inherently sequential and difficult to parallelize. For example, they often peel one vertex at a time, which results in too many peeling iterations. This sequential nature limits their scalability and efficiency on massive graphs. In graph-based fraud de- tection, techniques like COPYCATCH [4] and GETTHESCOOP [27] leverage local search heuristics to identify dense subgraphs in bi- partite networks, frequently applied in fraud, spam, and community detection across social and review platforms [24, 37, 42]. Despite their widespread use, these methods suffer from efficiency issues in very large graphs, often involving numerous iterations and lack- ing effective pruning techniques, which exacerbates the long-tail problem. Moreover, they are limited to specific definitions of dense subgraphs. Contrarily, Dupin offers a versatile, parallel approach to densest subgraph detection. It facilitates the simultaneous peeling of multiple vertices, drastically reducing the computational overhead. Additionally, Dupin provides APIs that allow users to customize defi- nitions of dense subgraphs, ensuring that the resultant graph density adheres to theoretical bounds. DSD on Dynamic Graphs. To tackle real-time fraud detection, sev- eral variants have been designed for dynamic graphs [10, 18, 26, 28, 43]. [28] addresses real-time fraud community detection on evolving graphs (edge insertions) using an incremental peeling sequence re- ordering method, while [26] extends this approach to fully dynamic graphs (edge insertions and deletions). [43] proposes an incremental algorithm that maintains and updates a dense subtensor in a ten- sor stream. [12] and [36] utilize sliding windows to detect dense subgraphs and bursting cores within specific time windows. These methods have three limitations. First, response time is an issue. Al- though they respond quickly to benign communities, the results can change drastically for newly formed fraudulent communities, caus- ing significant delays in incremental calculations for fraud detection. Second, efficiency is problematic. While incremental processing is faster than recomputation for a single transaction on billion-scale graphs, frequent incremental evaluations can be slower than a full recomputation. Third, in terms of flexibility, most methods support only a single semantic. In contrast, Dupin introduces a unified parallel framework for peeling-based dense subgraph detection.Dupin can respond within seconds on billion-scale graphs and defines multiple density metrics that can be used on the Dupin platform. Parallel DSD. Several parallel methods for DSD have been pro- posed [2, 14, 16, 39, 41, 42]. Bahmani et al. [2] proposed a parallel algorithm for DG using the MapReduce framework. FWA [15] proposed a parallel algorithm for DW based on Frank-Wolfe algo- rithm [25] using OpenMP [13]. Some works propose parallel 𝑘-core algorithms [32, 41, 44]. Others introduce parallel algorithms for 𝑘- Clique detection, with approaches offering a (1 + 𝜖)-approximation ratio and parallel implementations for the 𝑘CLiDS problem [16, 45]. There are also algorithms for 𝑘-Clique densest subgraph detection using OpenMP [14], and methods for dense subtensor detection [42]. However, these systems have two main limitations. First, they typi- cally support only a single-densitymetric and do not handle weighted graphs. In real-world applications, transactions between users can have varying degrees of importance based on amounts, frequen- cies [28], and other factors. Extending these systems to support other dense metrics with theoretical guarantees is non-trivial. Second, they often lack efficient pruning techniques, requiring numerous itera- tions and suffering from the long-tail issue, which hinders scalability on massive graphs. In contrast, Dupin defines the characteristics of a set of supported dense metrics and accommodates weighted sub- graphs, allowing for more nuanced analysis reflective of real-world data. Our method introduces efficient pruning strategies by peeling multiple vertices in parallel based on a threshold mechanism, signifi- cantly reducing the number of iterations and mitigating the long-tail problem. This makes Dupin more effective and scalable in diverse real-world scenarios compared to previous approaches. 3 THE Dupin FRAMEWORK',\n",
       " '3.1 Motivations': '',\n",
       " 'A': 'notable example of advanced fraud detection is seen in Anon, a prominent technology company in Southeast Asia known for its digital payment and food delivery services. As delineated in [28], the detection process begins with forming a transaction graph,𝐺 , derived from the platform’s transactions. This graph is incrementally updated, represented as 𝐺 = 𝐺 ⊕ ∆𝐺 . Subsequent stages utilize densest subgraph discovery algorithms to uncover fraudulent communities. A typical fraud scenario on the platform involves customer-merchant collusion, where fake accounts are created to exploit promotional incentives. These fraudulent activities manifest as densest subgraphs within the transaction graph. Efficiency. Our empirical findings reveal that the incremental execution of DSD algorithms, such as Spade [28], on a graph with 2 billion edges requires approximately 198 seconds per batch (default setting: one batch of 1K edge insertions). However, in practical business sce- narios, the number of incoming edges can peak at 400K per minute. Clearly, using incremental evaluation on billion-scale graphs does not provide sufficient throughput to meet business requirements. Latency and Prevention Ratio. Existing DSD systems face sig- nificant challenges in terms of latency and prevention ratio. Upon identification of fraudulent activities, actions such as account bans or freezes are implemented to prevent further losses. However, the aver- age latency for these systems to detect fraud and take action is often high, typically around 200 to 300 seconds for incremental algorithms and more than 1000 seconds for existing parallel algorithms, which is insufficient for real-time requirements (operational demand requires detection within seconds). Moreover, when the graph size increases from million-scale to billion-scale, the prevention ratio decreases from 92.47% to 45%. These issues underscore the necessity for more efficient and scalable algorithms that can maintain low latency and high prevention ratios, even under heavy transaction loads. Parallelization offers a viable solution to these challenges by dis- tributing the computational load across multiple processors. Mod- ern CPUs are equipped with numerous cores that are cost-effective, enabling parallel processing. If the peeling process of DSD is paral- lelized, different processors can simultaneously evaluate and peel different nodes, thereby increasing scalability and efficiency. For example, if we can improve processing speed by one to two orders of magnitude, the system would be able to meet industry requirements for real-time fraud detection, completing detection within seconds. Problem Statement. Given a graph 𝐺 = (𝑉 , 𝐸) and a community detection semantic with density metric 𝑔(𝑆), our goal is to find a vertex subset 𝑆∗ ⊆ 𝑉 that maximizes 𝑔(𝑆∗).',\n",
       " '3.2 System Architecture': 'To enable users to design complex DSD algorithms to detect fraudsters based on our framework, Dupin, we have introduced the archi- tecture, APIs, and supported density metrics in this section. Architecture of Dupin. Figure 3 presents the architecture of Dupin with user-defined functions for detecting suspicious activities.Dupin enables users to define their fraud detection semantics through a set of provided APIs. Within Dupin’s computational engine, we sup- port both sequential and parallel peeling algorithms. Additionally, algorithms for long-tail pruning techniques are implemented to ac- celerate convergence and enhance the density of subgraphs in each iteration. Dupin identifies fraudulent communities, which are then flagged for further investigation by moderators. APIs of Dupin (Figure 3). Dupin simplifies the process of defining detection semantics by allowing users to specify a few simple functions. Additionally, Dupin offers APIs that enable users to balance detection speed and accuracy effectively. The key APIs encompass several components designed to facilitate the customization and op- timization of fraud detection processes. These components provide a robust framework for users to implement their fraud detection strate- gies, ensuring both high performance and precision in identifying fraudulent activities. Details are listed below: • pWeight and updateNgh. These functions are designed for the dynamic adjustment of weights within the network. They allow the user to define the peeling weight of a vertex and specify the method for updating the peeling weight of neighboring vertices after a vertex has been peeled. • VSusp and ESusp. Given a vertex/edge, these components are re- sponsible for deciding the suspiciousness of the endpoint of the edge or the edge with a user-defined strategy. • isBenign. This component is used to decide whether a vertex is benign during the whole peeling process (Section 5). If the vertex is benign, it is peeled within the current iteration. • setEpsilon. This function is utilized to control the precision of fraud detection. During periods of high system load, moderators can increase 𝜖 to achieve higher throughput. Conversely, when the system load is low, 𝜖 can be decreased to enhance accuracy. • setK. This function specifies the 𝑘-Clique parameter, enabling the definition of the 𝑘-Clique for analysis. 1 double vsusp(const Vertex& v, const Graph& g) { 2 return g.weight[v]; // Side information on vertex 3 } 4 double esusp(const Edge& e, const Graph& g) { 5 return 1.0 / log(g.deg[e.src] + 5.0); 6 } 7 int main() { 8 Dupin dupin; 9 dupin.VSusp(vsusp); // Plug in vsusp 10 dupin.ESusp(esusp); // Plug in esusp 11 dupin.setEpsilon (0.1); 12 dupin.LoadGraph(\"graph_sample_path\"); 13 vector <Vertex > fraudsters = dupin.ParDetect (); 14 return 0; 15 } Listing 1: Implementation of FD on Dupin Example (Listing 1). To implement FD [24] on Dupin, users simply need to plug in the suspiciousness function vsusp for the vertices by calling VSusp and the suspiciousness function esusp for the edges by calling ESusp. Specifically, 1) vsusp is a constant function, i.e., given a vertex 𝑢, vsusp(𝑢) = 𝑎𝑖 and 2) esusp is a logarithmic function such that given an edge (𝑢𝑖 , 𝑢 𝑗 ), esusp(𝑢𝑖 , 𝑢 𝑗 ) = 1 log(𝑥+𝑐) , where 𝑥 is the degree of the object vertex between 𝑢𝑖 and 𝑢 𝑗 , and 𝑐 is a positive constant [24]. Developers can easily implement customized peeling algorithms with Dupin, significantly reducing the engineering effort. Density Metrics Support in Dupin. To illustrate the versatility of Dupin in handling various density metrics, we establish comprehensive sufficient conditions. These conditions ensure Dupin’s adaptability in integrating a wide range of metrics to assess graph density effectively. This foundational approach enablesDupin to sup- port diverse density calculations, crucial for nuanced fraud detection and network analysis tasks. Property 3.1. Dupin supports a density metric 𝑔(𝑆) if the following conditions are met: (1) 𝑔(𝑆) = 𝑓 (𝑆) |𝑆 | , where 𝑓 is a monotone increasing function. (2) 𝑎𝑖 ≥ 0, ensuring non-negative vertex weight function. (3) 𝑐𝑖 𝑗 ≥ 0, ensuring non-negative edge weight function.',\n",
       " '4 PARALLELABLE PEELING ALGORITHMS': 'For sequential peeling algorithms, only one vertex can be removed at a time, with each vertex’s removal depending on the removal of previous vertices. This dependency prevents the algorithm from executing multiple vertex removals simultaneously. In this section, we introduce a parallel peeling paradigm to break this dependency chain. This new approach allowsmultiple vertices to be peeled in parallel without affecting the programming abstractions that Dupin provides to end users. We then demonstrate how this paradigm offers fine-grained trade-offs between parallelism and ap- proximation guarantees for all density metrics studied in this work.',\n",
       " '4.1 Parallel Peeling Paradigm': 'Instead of peeling the vertex with the maximum peeling weight, we can peel all vertices that lead to a significant decrease in the density scores of the remaining graph. This decrease is controlled by a tunable parameter 𝜖 . A larger 𝜖 allows more vertices to be peeled in parallel, while a smaller 𝜖 provides a better approximation ratio. Algorithm 2: Dupin: Parallel Peeling Input:𝐺 = (𝑉 , 𝐸), a density metric 𝑔(𝑆), and 𝜖 ≥ 0 Output: A subset of vertices 𝑆 that maximizes the density metric 𝑔(𝑆) 1 𝑆0 ← 𝑉 /* Initialize the subset with all vertices */ 2 𝑖 ← 1 /* Initialize the index */ 3 while 𝑆𝑖−1 ̸= ∅ do 4 𝜏 = 𝑘(1 + 𝜖) · 𝑔(𝑆𝑖−1) /* The threshold for peeling */ 5 𝑈 ← {𝑢 | 𝑤𝑢 (𝑆𝑖−1) ≤ 𝜏 } /* Vertices to be peeled */ 6 𝑆𝑖 ← 𝑆𝑖−1 \\\\𝑈 /* Update the subset */ 7 𝑖 ← 𝑖 + 1 /* Increment the index */ 8 return arg max𝑆𝑖 𝑔(𝑆𝑖 ) /* The subset that maximizes 𝑔(𝑆) */ Parallel Peeling (Algorithm 2). Let 𝑆𝑖 denote the vertex set after the 𝑖-th peeling iteration. Initially, 𝑆0 = 𝑉 (Line 1). In each iteration, it removes a subset of vertices𝑈 from 𝑆𝑖−1, where ∀𝑢 ∈ 𝑈 ,𝑤𝑢 (𝑆𝑖−1) ≤ 𝑘(1 + 𝜖)𝑔(𝑆𝑖−1). Parameter 𝑘 is determined by the metric used. 𝑘 = 2 works for DG, DW and FD. 𝑘 = 3 works for TDS and 𝑘 is the size of the cliques for 𝑘CLiDS. This process is repeated recursively until no vertices are left, resulting in a series of vertex sets 𝑆0, . . . , 𝑆𝑅 , where 𝑅 is the number of iterations. The algorithm then returns the set 𝑆𝑖 (𝑖 ∈ [0, 𝑅]) that maximizes the density metric 𝑔(𝑆𝑖 ), denoted as 𝑆𝑝 . In what follows, we also show a case where 𝑘CLiDS is the density metric for DSD. 3 3 6 6 5 4 21 0 00 3 3 6 6 3 3 0 0 Density: 0.91 Density: 1.33 Density: 0 Iteration i Iteration i+1 Iteration i+2 Iteration i+3 Density: 0 τ = 3× 0.91 = 2.73 τ = 3× 1.33 = 3.99 τ = 0 τ = 0',\n",
       " '4.2 Theoretical Analysis': 'In this section, we analyze the theoretical properties of the parallel peeling process. We begin by examining the number of iterations required for parallel peeling. Built upon the result, we then determine the time complexity and the approximation ratio of the parallel peeling process. Lemma 4.1. Given any 𝜖 > 0, 𝑅 < log 1+𝜖 |𝑉 |. Proof. We prove that for any 𝜖 ≥ 0, the size of the remaining vertex set is bounded by |𝑆𝑖 |< 1 1+𝜖 |𝑆𝑖−1 | for all 𝑖 ∈ [1, 𝑅]. We first examine the peeling weights of the vertices in 𝑆𝑖−1 for the density metrics DG. DW and FD where 𝑘 = 2. As each edge weight in the peeling weight calculation contributes twice (once for each endpoint), and the vertex weights contribute once, we have∑︁ 𝑢∈𝑆𝑖−1 𝑤𝑢 (𝑆𝑖−1) ≤ 2𝑓 (𝑆𝑖−1), (3) The peeling weights of the vertices in 𝑆𝑖−1 are calculated as follows:∑︁ 𝑢∈𝑆𝑖−1 𝑤𝑢 (𝑆𝑖−1) = ∑︁ 𝑢∈𝑆𝑖−1\\\\𝑆𝑖 𝑤𝑢 (𝑆𝑖−1) + ∑︁ 𝑢∈𝑆𝑖 𝑤𝑢 (𝑆𝑖−1) ≥ ∑︁ 𝑢∈𝑆𝑖 𝑤𝑢 (𝑆𝑖−1) > 2(1 + 𝜖)|𝑆𝑖 |𝑔(𝑆𝑖−1) (4) Combining Equation 3 and Equation 4, we have: 2(1 + 𝜖)|𝑆𝑖 |𝑔(𝑆𝑖−1) < 2𝑓 (𝑆𝑖−1) = 2|𝑆𝑖−1 |𝑔(𝑆𝑖−1) (5) For all 𝑖 ∈ [1, 𝑅], |𝑆𝑖 |< 1 1 + 𝜖 |𝑆𝑖−1 | (6) Then we have: |𝑉 |= |𝑆0 |> 1 1 + 𝜖 |𝑆1 |> . . . > ( 1 1 + 𝜖 ) 𝑅 |𝑆𝑅 | (7) Therefore, we have 𝑅 < log 1+𝜖 |𝑉 |. For the density metrics TDS and 𝑘CLiDS, we use the fact that each edge contributes to the peeling weight calculation exactly 𝑘 times due to the counting of each edge within every 𝑘-Clique in the graph (𝑘 = 3 for TDS). This multiplicity of edge contributions leads to the relationship: ∑︁ 𝑢∈𝑆𝑖−1 𝑤𝑢 (𝑆𝑖−1) ≤ 𝑘 𝑓 (𝑆𝑖−1), (8) We then adapt Equations 4-7 and the proof follows. □ Time Complexity. Algorithm 2 operates for at most 𝑅 = log 1+𝜖 |𝑉 | iterations, with each iteration scanning all vertices, taking 𝑂(|𝑉 |) time. As vertices are peeled, updates are necessitated for the peeling weights of their neighboring vertices. Over the course of 𝑅 iterations, there are no more than |𝐸 | such updates for DG, DW and FD. Consequently, Algorithm 3 incurs 𝑂((|𝐸 |+|𝑉 |) log 1+𝜖 |𝑉 |) node/edge weight updates. For DG, DW and FD, the update costs are 𝑂(1). For TDS and 𝑘CLiDS, we follow previous studies [11, 14] to compute the peeling weights, and the complexity is 𝑂(𝑘 |𝐸 |𝛼(𝐺) 𝑘−2 ), where 𝛼(𝐺) is the arboricity of the graph. Hence, the overall complexity is 𝑂(𝑘 |𝐸 |𝛼(𝐺) 𝑘−2 (|𝐸 |+|𝑉 |) log 1+𝜖 |𝑉 |). The cost of peeling weight up- dates is orthogonal to the Dupin framework. Approximation Ratio. Next, we are ready to show that Dupin provides a theoretical guarantee with a 𝑘(1 + 𝜖)-approximation for all density metrics studied in this work. We denote 𝑢𝑖 ∈ 𝑆𝑝 as the first vertex in 𝑆∗ peeled by Dupin, and 𝑆 ′ as the peeling subsequence starting from 𝑢𝑖 . Theorem 4.2. For the vertex set 𝑆𝑝 returned by Dupin and the optimal vertex set 𝑆∗, the relationship 𝑔(𝑆𝑝 ) ≥ 𝑔(𝑆∗) 𝑘(1+𝜖) holds. Proof. We first prove that ∀𝑢𝑖 ∈ 𝑆∗, 𝑤𝑢𝑖 (𝑆 ∗ ) ≥ 𝑔(𝑆∗). We prove it in contradiction. We assume that 𝑤𝑢𝑖 (𝑆 ∗ ) < 𝑔(𝑆∗). By peeling 𝑢𝑖 from 𝑆∗, we have the following. 𝑔(𝑆∗ \\\\ {𝑢𝑖 }) = 𝑓 (𝑆∗) −𝑤𝑢𝑖 (𝑆 ∗ ) |𝑆∗ |−1 > 𝑓 (𝑆∗) − 𝑔(𝑆𝑖 ) |𝑆∗ |−1 = 𝑓 (𝑆∗) − 𝑔(𝑆∗) |𝑆∗ |−1 = 𝑓 (𝑆∗) − 𝑓 (𝑆∗) |𝑆∗ | |𝑆∗ |−1 = 𝑔(𝑆∗) (9) Therefore, a better solution can be obtained by removing 𝑢𝑖 from 𝑆∗, which contradicts the fact that 𝑆∗ is the optimal solution. We can conclude that𝑤𝑢𝑖 (𝑆 ∗ ) ≥ 𝑔(𝑆∗). Next, we assume that 𝑢𝑖 ∈ 𝑆𝑝 is the first vertex in 𝑆∗ peeled by the peeling algorithm. Since 𝑆∗ is the optimal vertex set, we have 𝑔(𝑆∗) ≤ 𝑤𝑢𝑖 (𝑆 ∗ ), (10) because the optimal value 𝑔(𝑆∗) must be less than or equal to the weight contribution of any single vertex in 𝑆∗, in particular 𝑢𝑖 . Since 𝑆∗ ⊆ 𝑆 ′, it is clear that 𝑤𝑢𝑖 (𝑆 ∗ ) ≤ 𝑤𝑢𝑖 (𝑆 ′ ) (11) By the peeling condition, we have 𝑤𝑢𝑖 (𝑆 ′ ) ≤ 𝑘(1 + 𝜖)𝑔(𝑆 ′) (12) Therefore, we have 𝑔(𝑆∗) ≤ 𝑤𝑢𝑖 (𝑆 ∗ ) ≤ 𝑤𝑢𝑖 (𝑆 ′ ) ≤ 𝑘(1 + 𝜖)𝑔(𝑆 ′) ≤ 𝑘(1 + 𝜖)𝑔(𝑆𝑝 ) (13) Hence, we can conclude that 𝑔(𝑆𝑝 ) ≥ 𝑔(𝑆∗) 𝑘(1+𝜖) . □ Summary. Dupin can easily balance efficiency and the worst-case approximation ratio with a single parameter, 𝜖 . A larger 𝜖 allows more vertices to be peeled in parallel, reducing the number of peeling iterations. Meanwhile, the worst-case approximation ratio of parallel peeling is only affected by a factor of (1 + 𝜖) compared to the ratio for sequential peeling w.r.t. all density metrics studied in this work.',\n",
       " '5 LONG-TAIL PRUNING': 'We observe that the parallel peeling algorithm encounters two sig- nificant issues. First, there is a long-tail problem where subsequent iterations fail to produce denser subgraphs. Instead, each iteration only peels off a small fraction of vertices, which significantly impacts the overall runtime efficiency. Second, during batch peeling, each peeling iteration often results in disparate structures. These are pri- marily caused by the peeling of certain vertices, which leaves their neighboring structure disconnected and fragmented. This phenome- non affects the continuity and coherence of the subgraphs. As shown in Table 2, on the la dataset (the largest social network dataset tested in our experiments), DG, DW, and FD experience 24.67%, 46.84%, and 3.01% of rounds as ineffective long-tail iterations, severely limiting response time. Additionally, during the peeling process, if the batch size is large (e.g., 𝜖 = 0.5), approximately 25.34%, 29.46%, and 7.16% of nodes become disconnected and sparse, signifi- cantly impacting the quality of the detected subgraphs. To address the challenges of high iteration counts in peeling algorithms, Dupin introduces two long-tail pruning techniques: Global Peeling Optimization (GPO) to maintain a global peeling threshold, and Local Peeling Optimization (LPO) to improve the density of the current peeling iteration’s subgraph, thereby increasing the peeling threshold. We tested three algorithms, DG, DW, and FD, and found that they required 17,637, 150,223, and 112,074 iterations respectively, which is significantly high. By strategically pruning long-tail ver- tices and sparse vertices, the number of iterations can be reduced by up to 46.84%. Additionally, LPO can further reduce the number of iterations by up to 92.79%. These optimizations significantly enhance the efficiency of the peeling process, as discussed in this section.',\n",
       " '5.1 Global Peeling Optimization': 'Initially, we define what constitutes a long-tail vertex during the peeling process. Peeling these long-tail vertices results in ineffective peeling iterations. To address this, we maintain a global maximum peeling threshold to determine which vertices can be peeled directly. When the global peeling threshold is greater than the current itera- tion’s threshold, we use the global peeling threshold for peeling. Algorithm 3: DupinGPO: Global Peeling Optimization Input:𝐺 = (𝑉 , 𝐸), a density metric 𝑔(𝑆), and 𝜖 ≥ 0 Output: A subset of vertices 𝑆𝑝 that maximizes the density metric 𝑔(𝑆) 1 𝑆0 ← 𝑉 /* Initialize the subset with all vertices */ 2 𝑖 ← 1 3 𝜏max ← 𝑔(𝑆 0 ) 𝑘(1+𝜖) /* Initialize the threshold */ 4 while 𝑆𝑖−1 ̸= ∅ do 5 𝜏 ← max{𝜏max, 𝑘(1 + 𝜖) · 𝑔(𝑆𝑖−1)} /* Peeling threshold */ 6 𝑈 ← {𝑢 | 𝑤𝑢 (𝑆𝑖−1) ≤ 𝜏 } /* Vertices to be peeled */ 7 𝑆𝑖 ← 𝑆𝑖−1 \\\\𝑈 /* Update the subset */ 8 𝜏max = max{𝜏max, 𝑔(𝑆𝑖 ) 𝑘(1+𝜖) } /* Refine the threshold */ 9 𝑖 ← 𝑖 + 1 10 return arg max𝑆𝑖 𝑔(𝑆𝑖 ) /* The subset that maximizes 𝑔(𝑆) */ Definition 5.1 (Long-Tail vertex). Given an 𝛼-𝑎𝑝𝑝𝑟𝑜𝑥𝑖𝑚𝑎𝑡𝑖𝑜𝑛 DSD algorithm and a set of vertices 𝑆 ⊆ 𝑉 , if𝑤𝑢 (𝑆) < 𝑔(𝑆∗) 𝛼 and 𝑆∗ is the optimal vertex set, then 𝑢 is a long-tail vertex. Lemma 5.1. If ∃𝑢 ∈ 𝑆𝑖 is a long-tail vertex, then 𝑆𝑖 ̸= 𝑆𝑃 . Proof. We prove it in contradiction by assuming that 𝑆𝑖 = 𝑆𝑃 . Since 𝑢 is a long-tail vertex in 𝑆𝑖 ,𝑤𝑢 (𝑆𝑖 ) < 𝑔(𝑆∗) 𝛼 < 𝑔(𝑆𝑃 ). By peeling 𝑢 from 𝑆𝑃 , we have the following. 𝑔(𝑆𝑃 \\\\ {𝑢}) = 𝑓 (𝑆𝑃 ) −𝑤𝑢 (𝑆𝑃 ) |𝑆𝑃 |−1 > 𝑓 (𝑆𝑃 ) − 𝑔(𝑆𝑃 ) |𝑆𝑃 |−1 = 𝑓 (𝑆𝑃 ) − 𝑓 (𝑆𝑃 ) |𝑆𝑃 | |𝑆𝑃 |−1 = 𝑔(𝑆𝑃 ) (14) A denser subgraph can be obtained by peeling 𝑢 from 𝑆𝑃 . This contradicts that 𝑆𝑃 is the optimal solution. Hence, 𝑆𝑖 ̸= 𝑆𝑃 . □ Identifying Long-Tail Vertices. Based on Lemma 5.2, we can also establish that a vertex𝑢 in set 𝑆 can be classified as a long-tail vertex if 𝑤𝑢 (𝑆) < 𝑔(𝑆∗) 𝑘(1+𝜖) . This insight allows us to implement a global peeling threshold, denoted as 𝜏max. During each iteration, we compare 𝑔(𝑆𝑖 ) 𝑘(1+𝜖) with 𝜏max. If the former exceeds the latter, Dupin updates 𝜏max to 𝑔(𝑆𝑖 ) 𝑘(1+𝜖) , thereby enhancing the efficiency of the peeling. Peeling with Global Threshold (Algorithm 3). Using FD as an example, we can set 𝑘 to 2. In Lemma 5.2, we recognize that long-tail vertices in set 𝑆𝑖 can be peeled directly during iteration 𝑖 for FD. To facilitate this, Dupin initiates with a global peeling threshold 𝜏max (Line 3). After identifying a denser subgraph post-peeling, 𝜏max is updated to 𝑔(𝑆𝑖 ) 2(1+𝜖) , refining the peeling process (Line 8). Example 5.1. In the initial depiction provided in the leftmost panel of Figure 6 (the 𝑖-th iteration), the density of the graph is 4.28. Traditionally, this scenario would necessitate two additional iterations to complete the peeling process, which could notably increase computational time. As illustrated, vertex 𝑢 cannot be peeled in the (𝑖 + 1)-th iteration and would typically require an extra iteration, the (𝑖 + 2)-th, for its removal. However, Dupin can promptly peel vertices that fall below this threshold by employing our algorithm’s global peeling threshold, which is calculated as half of the current density (𝜏max = 4.28/2 = 2.14). This approach enables the simultaneous peeling of three nodes in the (𝑖 + 1)-th iteration. Without this pruning strategy, an additional iteration would be essential to peel vertex 𝑢, underscoring our method’s enhanced efficiency in the peeling process. This optimization obviates the necessity for a prolonged iterative sequence. The example herein substantiates the efficacy of our tai- lored optimization for long-tail distribution, substantially enhancing the algorithm’s time efficiency.',\n",
       " '5.2 Local Peeling Optimization': 'Algorithm 4: DupinLPO: Local Peeling Optimization Input: A graph𝐺 = (𝑉 , 𝐸), a density metric 𝑔(𝑆), and 𝜖 ≥ 0 Output: A subset of vertices 𝑆𝑝 representing the densest subgraph 1 𝑆0 ← 𝑉 /* Initialize the subset with all vertices */ 2 𝜏max ← 𝑔(𝑆 0 ) 𝑘(1+𝜖) /* Initialize the threshold */ 3 while 𝑆𝑖−1 ̸= ∅ do 4 𝜏1 ← max{𝜏max, 𝑘(1 + 𝜖) · 𝑔(𝑆𝑖−1)} /* Peeling threshold */ 5 𝑈1 ← {𝑢 ∈ 𝑆 | 𝑤𝑢 (𝑆) ≤ 𝜏1 } 6 𝑆𝑖 ← 𝑆𝑖−1 \\\\𝑈1 7 while ∃𝑢 ∈ 𝑆𝑖 , 𝑤𝑢 (𝑆𝑖 ) < 𝑔(𝑆𝑖 ) do 8 𝜏2 ← max{𝜏max, 𝑔(𝑆𝑖 )} /* Update the threshold */ 9 𝑈2 ← {𝑢 | 𝑤𝑢 (𝑆𝑖 ) < 𝜏2 } 10 𝑆𝑖 ← 𝑆𝑖 \\\\𝑈2 11 𝜏max = max{𝜏max, 𝑔(𝑆𝑖 ) 𝑘(1+𝜖) } /* Update the threshold */ 12 return arg max𝑆𝑖 𝑔(𝑆𝑖 ) /* The subset that maximizes 𝑔(𝑆) */ It is important to note that each peeling iteration often results in disparate structures. This is primarily caused by the peeling of cer- tain vertices, which leaves their neighboring structures disconnected and fragmented. This not only affects the density of the detected sub- graph but also lowers the peeling threshold. Lemma 5.2 establishes a foundational property of vertices within a dense graph. Lemma 5.2. ∀𝑢 ∈ 𝑆 , if𝑤𝑢 (𝑆) < 𝑔(𝑆), 𝑔(𝑆 \\\\ {𝑢}) > 𝑔(𝑆). Proof. By peeling 𝑢 from 𝑆 , we have the following. 𝑔(𝑆 \\\\ {𝑢}) = 𝑓 (𝑆) −𝑤𝑢 (𝑆) |𝑆 |−1 > 𝑓 (𝑆) − 𝑔(𝑆𝑖 ) |𝑆 |−1 = 𝑓 (𝑆) − 𝑔(𝑆) |𝑆∗ |−1 = 𝑓 (𝑆) − 𝑓 (𝑆) |𝑆 | |𝑆 |−1 = 𝑔(𝑆) (15) Therefore, a denser graph can be obtained by peeling 𝑢 from 𝑆 . □ According to Lemma 5.2, we introduce local peeling optimization within the iteration. After each peeling iteration, if the peeling weight of a vertex is smaller than the current density, Dupin will trim it. We implement a local peeling optimization (Lines 7-10, Algorithm 4) before the next iteration. After a vertex 𝑢 is peeled, Dupin invokes updateNgh to update the peeling weights of 𝑢’s neighboring nodes. If any of these neighbors have a peeling weight less than the current density, they are also peeled, thereby increasing the density of the current iteration. Consequently, when an iteration yields globally optimal results, a denser structure is obtained. The benefits of this approach extend beyond density improvements; performance is also enhanced. By trimming these vertices, we reduce the number of vertices to traverse in subsequent iterations, and a higher density enables more efficient iterations due to the possibility of using larger peeling thresholds. Lemma 5.3. If 𝑢𝑖 ∈ 𝑆𝑝 is the first vertex in 𝑆∗ removed by Dupin, 𝑢𝑖 will not be removed during the local peeling optimization process. Proof. Assume, for the sake of contradiction, that 𝑢𝑖 is trimmed during the local peeling optimization process. Let 𝑆 ′ be the set of vertices left before 𝑢𝑖 is trimmed. By definition, we have: 𝑤𝑢𝑖 (𝑆 ′ ) < 𝑔(𝑆∗). Since 𝑢𝑖 is the first vertex in 𝑆∗ to be removed, 𝑆∗ ⊆ 𝑆 ′, therefore: 𝑤𝑢𝑖 (𝑆 ∗ ) ≤ 𝑤𝑢𝑖 (𝑆 ′ ) < 𝑔(𝑆∗). This contradicts Lemma 5.2, which states that ∀𝑢𝑖 ∈ 𝑆∗: 𝑤𝑢𝑖 (𝑆 ∗ ) ≥ 𝑔(𝑆∗). Otherwise, a better solution can be obtained by removing 𝑢 from 𝑆∗, which contradicts the fact that 𝑆∗ is the optimal solution. Thus, 𝑢𝑖 cannot be trimmed. Hence,𝑢𝑖 will not be removed during the local peeling optimization process. □ Due to Lemma 5.3, 𝑢𝑖 will only be removed during the peeling process. Hence, the approximation ratios (Theorem 4.2) for different density metrics are preserved. 6 EXPERIMENTAL STUDY Our experiments are run on a machine that has an X5650 CPU, 512 GB RAM. The implementation is made memory-resident and implemented in C++. All codes are compiled by GCC-9.3.0 with -𝑂3. Datasets. We report the datasets used in our experiment in Table 3. One industrial dataset is from Anon (gfg). Datasets links-anon(la) and Twitter-rv (rv)were obtained from Stanford NetworkDataset Collection [31]. Datasets kron-logn21(kron), soc-twitter(soc), Web-uk-2005(uk) and Web-sk-2005(sk) were obtained from Network Data Repository [38]. Dataset bio was from the BIOMINE [35]. Baseline Methods. In this section, we introduce five commonly used density metrics pivotal in analyzing network structures:DG [6], DW [23], FD [24], TDS [46], and𝑘CLiDS [14].We implemented these metrics in our framework, Dupin, and compared their performance with frameworks including Spade [28], kCLIST [14], GBBS [16], PBBS [41], PKMC [32], FWA [15] and ALENEX [44]. kCLIST [14] and PBBS [41] are parallel 𝑘CLiDS algorithms, while GBBS [16] supports the DG density metric. Although GBBS does not support weighted graphs, we precompute the peeling weights offline and then import them into GBBS. The time spent on this offline computation is not included in our reported runtime. FWA, ALENEX and PKMC support the DG density metric. For clarity, we denote the incremental peeling process, which involves the iterative removal of graph elements based on density scores, as Spade-X, where X corresponds to each metric, under the same experimental condi- tions described in [28], where the batch size is set to 1K, consistent with the default setting in [28]. We report the average runtime per batch for Spade. DupinGPO-X and DupinLPO-X represent our par- allel peeling algorithms that incorporate global and local peeling optimization. Default Settings. In our experiments, the parameter 𝜖 is set to 0.1 by default, and the number of threads 𝑡 is configured to 128. These settings are used consistently unless otherwise specified. 6.1 Efficiency of Dupin Spade vs Dupin. Our initial comparison focuses on the performance disparity between the incremental and parallel peeling algorithms. Specifically, Dupin-DG (resp. Dupin-DW, Dupin-FD, Dupin-TDS, andDupin-𝑘CLiDS) demonstrates a speedup factor of 6.97 (resp. 7.71, 7.71, 7.65, and 8.46) over Spade-DG (resp. Spade-DW, Spade-FD, Spade-TDS, and Spade-𝑘CLiDS). Notably, Spade-TDS and Spade𝑘CLiDS only complete computations within 7,200 seconds on gfg. The bottleneck for Spade arises in larger graphs where initial calculations of triangle and 𝑘-Clique counting are unable to finish. Dupin outpaces Spade primarily because Spade suffers from frequent re- ordering of peeling sequences when there are many increments, which proves to be slower than recalculating. GBBS vs Dupin. Previous works have not provided simultaneous support for all density metrics. To bridge this gap, we implemented DG, DW, and FD within the parallel GBBS framework. On average, Dupin-DG, Dupin-DW, and Dupin-FD demonstrated speedup factors of 6.33, 12.51, and 17.07 times, respectively, over GBBS-DG, GBBS-DW, andGBBS-FD. This performance advantage is attributed to the fundamental difference in the peeling process between the two frameworks. In GBBS, the basic unit of each peel is a bucket, where all nodes within the same bucket have an identical peeling weight. This design limits the parallelism in weighted graphs like those using DW and FD, where many buckets may contain only a single node. In contrast, Dupin peels batches of nodes at each step, achieving higher parallelism and thus consistently outperforming GBBS, particularly in weighted graph scenarios where the speedup is even more pronounced. kCLIST vs PBBS vs Dupin. Both kCLIST and PBBS support the density metrics TDS and 𝑘CLiDS. In our comparisons, Dupin demonstrates notable performance advantages. Specifically, Dupin-TDS is faster than kCLIST-TDS and PBBS-TDS by 39.85 and 62.71 times, respectively. Furthermore, Dupin-𝑘CLiDS also outperforms kCLIST𝑘CLiDS by 4.16 times. A significant finding is that PBBS-𝑘CLiDS fails to complete computations within 7200 seconds on most datasets, highlighting the efficiency of Dupin in handling more complex met- rics under stringent time constraints. Impact of DupinGPO. Next, we evaluate the acceleration effect of DupinGPO, using Dupin as a baseline for comparison. Notably, DupinGPO-DG is found to be 1.14 times faster, DupinGPO-DW is 1.11 times faster,DupinGPO-TDS is 1.10 times faster, andDupinGPO𝑘CLiDS is 1.35 times faster than their respective Dupin implementations. The acceleration is particularly significant for 𝑘CLiDS on larger datasets, such as soc, rv, kron, and sk. DupinGPO-𝑘CLiDS is 1.40, 1.54, 1.51, and 1.61 times faster than Dupin-𝑘CLiDS on soc, rv, kron, and sk, respectively. This is attributed to the greater number of iterations required due to the larger dataset sizes and the higher count of 𝑘-Cliques, necessitating more peeling iterations. In such scenarios,DupinGPO effectively prunes the tail iterations early, thereby reducing the number of rounds needed. Impact of DupinLPO. We next compared the efficiency of Dupin andDupinLPO. On average, thoughDupinLPO-DGwas 10.09% slower than Dupin-DG, the speed is still comparable. For example, on the soc dataset, DupinLPO-DG was 5.94% faster than Dupin-DG. This is because DupinLPO requires some lock operations to update the peeling weights of neighboring vertices during local peeling optimization. However, the advantage is that DupinLPO can detect up to 26.26% denser subgraphs, as detailed in Section 6.2. On larger datasets, such as soc, rv, and la, the vertices being trimmed have relatively low correlations with each other, so the impact of locks is minimal. For example, on the soc (resp. rv), DupinLPO-DG was 5.94% (resp. 9.94%) faster than Dupin-DG. In contrast, on smaller datasets, such as bio and kron, the extra cost of locks is higher. Impact of the Number of Threads 𝑡 . All methods are influenced by the concurrency level, i.e., the number of threads. Generally, the greater the number of threads, the faster the execution speed. In our experiments, using the la dataset as a benchmark, we varied the number of threads 𝑡 from 2 to 128 to observe the impact on runtime performance. For GBBS-DG, runtime stabilizes and accelerates by 3.33 times as thread count increases from 2 to 8. For GBBS-DW, performance deteriorates when the thread count exceeds 32, but from 2 to 32 threads, it accelerates by 3.50 times. Similarly, GBBS-FD shows no significant change in runtime beyond 32 threads, with an acceleration of 3.09 times from 2 to 32 threads. GBBS-TDS and GBBS-𝑘CLiDS fail to complete computations on the la dataset. In contrast, all five methods tested onDupin exhibit good scalability. As the number of threads increases from 2 to 128,Dupin-DG accelerates by 9.91 times, Dupin-DW by 12.69 times, Dupin-FD by 13.03 times, and Dupin-TDS by 28.06 times. Although Dupin-𝑘CLiDS fails to produce results when the thread count is below 16, it accelerates by 1.32 times from 32 to 128 threads. These results demonstrate Dupin’s strong scalability across varied threading levels. 6.2 Effectiveness of Dupin In addition to performance speedup, we evaluate the density of the subgraphs detected by different frameworks. Detailed density comparisons are presented in Table 5. GBBS vs Spade vs Dupin. Firstly, we compare the densities of the subgraphs identified by GBBS-DG (respectively GBBS-DW, and GBBS-FD) to those detected by Dupin in parallel execution. Our experimental results indicate that Dupin maintains comparable subgraph densities with GBBS, signifying that the increase in com- putational efficiency does not compromise the accuracy of the de- tected subgraphs. Specifically, the density of the subgraph detected byGBBS-DG (respectivelyGBBS-DW, andGBBS-FD) is 7.08% (resp. 6.48% and 7.43%) greater than that detected by Dupin-DG (respectively Dupin-DW, and Dupin-FD) on average. Spade shares similar densities with GBBS. Due to Spade’s tendency to accumulate er- rors over time, its density can become inflated. We will explain its limitations in practical applications in more detail in the case study. kCLIST vs PBBS vs Dupin. a) For 𝑘CLiDS, PBBS-𝑘CLiDS could not complete on most of our test datasets, so we did not compare their densities. kCLIST-𝑘CLiDS could not complete on the sk dataset. On the other datasets, Dupin-𝑘CLiDS’s density was only 6.97% smaller than kCLIST-𝑘CLiDS. On some datasets, such as bio and kron,Dupin𝑘CLiDS even detected subgraphs with greater density. b) For TDS, PBBS-TDS could detect subgraphs on smaller graphs but failed on billion-scale graphs such as rv, sk, and la. Dupin-TDS’s density was 2.03% greater than kCLIST-TDS. Therefore, we conclude that Dupin is comparable to existing parallel methods in terms of the density of the detected dense subgraphs. Ablation Study of Dupin. This experiment delves into the effectiveness of our optimizations, DupinGPO and DupinLPO. Our analysis primarily focuses on comparing the densities of dense subgraphs detected by both DupinGPO and DupinLPO in various settings. DupinGPO achieves the same density as Dupin across five den- sity metrics because it trims the long tail, enhancing efficiency. For DupinLPO-DG (resp. DupinLPO-DW, DupinLPO-FD, DupinLPOTDS, and DupinGPO-𝑘CLiDS), we observe that the detected sub- graphs exhibit densities 11.09% (resp. 7.32%, 8.00%, 1.01%, and 26.26%) greater than those detected by Dupin-DG, Dupin-DW, Dupin-FD, Dupin-TDS, andDupin-𝑘CLiDS, respectively, demonstrating notable outcomes. This is because DupinLPO trims the graph in each iteration, resulting in denser subgraphs compared to Dupin. Impact of the Approximation Ratio 𝜖. The approximation ratio 𝜖 serves to control accuracy. In our experiment, we vary 𝜖 from 0.1 to 1 to investigate its influence on the accuracy of Dupin. We observe that as 𝜖 increases from 0.1 to 1, the density of subgraphs detected by Dupin, DupinGPO, and DupinLPO generally decreases. ForDG, the densities ofDupin-DG,DupinGPO-DG, andDupinLPODG decrease by 22.71%, 22.71%, and 11.99%, respectively. The trends for DW and DG are similar, with the worst performance observed at 𝜖 = 0.6 for both density metrics. The trends for FD, TDS, and 𝑘CLiDS are also similar. DupinLPO detects denser subgraphs across most density metrics and is less affected by 𝜖 , due to its iterative trimming. For example, for FD, DupinLPO’s density decreases by only 6.23%, while DupinGPO and Dupin’s densities decrease by 17.37%.',\n",
       " '6.3 Case study': 'We also compare the performance in real-world scenarios, focusing on three key aspects: accuracy, latency, and prevention ratio. Accuracy. As discussed in Section 1, although Spade can quickly respond to new transactions, it tends to accumulate errors over time. Our analysis utilizes transaction data from Anon, with the transaction network comprising |𝐸 |= 2 billion edges. Under the incremental computation framework of Spade, the assumption is made that the weights of existing edges remain constant, thereby neglecting the impact of newly inserted edges on these weights, leading to error accumulation. For instance, using FD density metric, we observe that errors accumulate up to 24.4% within a day and increase to 30.3% over a week, as illustrated in Figure 9. Although Dupin sacrifices some accuracy to achieve parallel computational efficiency, our experiments show that Dupin’s errors remain relatively stable between 3% and 5%. Hence, the detection precision of Spade declines from 87.9% to 68.3%, while Dupin’s precision remains above 86%. Latency and Prevention Ratio. If a transaction is identified as fraudulent, subsequent related transactions are blocked to mitigate potential losses. We define the ratio of successfully prevented sus- picious transactions to the total number of suspicious transactions as R. As shown in Figure 6, the prevention ratio R continues to decrease as latency increases on Anon’s datasets. Using the default density metric, FD, in Anon, our results show thatDupin-FD can prevent 94.5% of fraudulent activities, GBBS-FD can prevent 3.0%, and Spade-FD can prevent 45.3%. The reason for the lower performance of GBBS-FD is its low parallelism in the peeling process, which results in long peeling times and thus an average latency of 6,014 seconds, delaying the prevention of many fraudulent activities. Although Spade-FD performs well in reordering techniques on smaller datasets, in industrial scenarios with billion-scale graphs, latency significantly worsens due to the extensive reordering range required in the peeling sequences as normal users transition to fraudsters, increasing the cost of incremental computations. Additional tests were conducted with other density metrics; for DG, Dupin-DG can prevent 78.8%, while other methods prevent varying amounts. Similarly, for DW, Dupin-DW can prevent 86.1%, while other methods demonstrate different prevention capabilities.',\n",
       " '7 CONCLUSION AND FUTUREWORKS': 'In this paper, we presented Dupin, a novel parallel fraud detection framework tailored for massive graphs. Our experiments demonstrated that Dupin’s advanced peeling algorithms and long-tail pruning techniques, DupinGPO and DupinLPO, significantly enhance the detection efficiency of dense subgraphs without sacrificing accuracy. Comparative studies with GBBS, PBBS, kCLIST, Spade, and Dupin highlighted Dupin’s superior performance in terms of com- putational efficiency and accuracy. Through case studies, we also validated that Dupin achieves lower latency and higher detection ac- curacy compared to existing fraud detection systems on billion-scale graphs. Overall, Dupinproves to be a powerful tool for large-scale fraud detection, providing a scalable and efficient solution for real-time applications. Future work will explore the integration of additional suspiciousness functions and further optimization to handle even larger datasets and more complex fraud scenarios.',\n",
       " 'REFERENCES': '[1] Distil networks: The 2019 bad bot report. https://www.bluecubesecurity.com/wp- content/uploads/bad-bot-report-2019LR.pdf. [2] B. Bahmani, R. Kumar, and S. Vassilvitskii. Densest subgraph in streaming and mapreduce. Proceedings of the VLDB Endowment, 5(5), 2012. [3] Y. Ban, X. Liu, T. Zhang, L. Huang, Y. Duan, X. Liu, and W. Xu. Badlink: Combining graph and information-theoretical features for online fraud group detection. arXiv preprint arXiv:1805.10053, 2018. [4] A. Beutel, W. Xu, V. Guruswami, C. Palow, and C. Faloutsos. Copycatch: stopping group attacks by spotting lockstep behavior in social networks. In Proceedings of the 22nd international conference on World Wide Web, pages 119–130, 2013. [5] D. Boob, Y. Gao, R. Peng, S. Sawlani, C. Tsourakakis, D.Wang, and J.Wang. Flowless: Extracting densest subgraphs without flow computations. In Proceedings of The Web Conference 2020, pages 573–583, 2020. [6] M. Charikar. Greedy approximation algorithms for finding dense components in a graph. In International Workshop on Approximation Algorithms for Combinatorial Optimization, pages 84–95. Springer, 2000. [7] M. Charikar. Greedy approximation algorithms for finding dense components in a graph. In Approximation Algorithms for Combinatorial Optimization, Third International Workshop, APPROX 2000, Saarbrücken, Germany, September 5-8, 2000, Proceedings, volume 1913 of Lecture Notes in Computer Science, pages 84–95. Springer, 2000. [8] C. Chekuri, K. Quanrud, and M. R. Torres. Densest subgraph: Supermodularity, iterative peeling, and flow. In Proceedings of the 2022 Annual ACM-SIAM Symposium on Discrete Algorithms (SODA), pages 1531–1555. SIAM, 2022. [9] J. Chen and Y. Saad. Dense subgraph extraction with application to community detection. IEEE Transactions on knowledge and data engineering, 24(7):1216–1230, 2010. [10] Y. Chen, J. Jiang, S. Sun, B. He, and M. Chen. Rush: Real-time burst subgraph detection in dynamic graphs. Proceedings of the VLDB Endowment, 17(11):3657– 3665, 2024. [11] N. Chiba and T. Nishizeki. Arboricity and subgraph listing algorithms. SIAM Journal on computing, 14(1):210–223, 1985. [12] L. Chu, Y. Zhang, Y. Yang, L. Wang, and J. Pei. Online density bursting subgraph detection from temporal graphs. Proceedings of the VLDB Endowment, 12(13):2353– 2365, 2019. [13] L. Dagum and R. Menon. Openmp: an industry standard api for shared-memory programming. IEEE computational science and engineering, 5(1):46–55, 1998. [14] M. Danisch, O. Balalau, and M. Sozio. Listing k-cliques in sparse real-world graphs. In Proceedings of the 2018 World Wide Web Conference, pages 589–598, 2018. [15] M. Danisch, T.-H. H. Chan, and M. Sozio. Large scale density-friendly graph decomposition via convex programming. In Proceedings of the 26th International Conference on World Wide Web, pages 233–242, 2017. [16] L. Dhulipala, J. Shi, T. Tseng, G. E. Blelloch, and J. Shun. The graph based benchmark suite (gbbs). In Proceedings of the 3rd Joint International Workshop on Graph Data Management Experiences & Systems (GRADES) and Network Data Analytics (NDA), pages 1–8, 2020. [17] Y. Dourisboure, F. Geraci, and M. Pellegrini. Extraction and classification of dense communities in the web. In Proceedings of the 16th international conference on World Wide Web, pages 461–470, 2007. [18] A. Epasto, S. Lattanzi, and M. Sozio. Efficient densest subgraph computation in evolving graphs. In Proceedings of the 24th international conference on world wide web, pages 300–310, 2015. [19] D. Gibson, R. Kumar, and A. Tomkins. Discovering large dense subgraphs in massive graphs. In Proceedings of the 31st international conference on Very large data bases, pages 721–732. Citeseer, 2005. [20] A. Gionis and C. E. Tsourakakis. Dense subgraph discovery: Kdd 2015 tutorial. In Proceedings of the 21th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, pages 2313–2314, 2015. [21] A. V. Goldberg. Finding a maximum density subgraph. 1984. [22] A. V. Goldberg and R. E. Tarjan. A new approach to the maximum-flow problem. Journal of the ACM (JACM), pages 921–940, 1988. [23] N. V. Gudapati, E. Malaguti, and M. Monaci. In search of dense subgraphs: How good is greedy peeling? Networks, 77(4):572–586, 2021. [24] B. Hooi, H. A. Song, A. Beutel, N. Shah, K. Shin, and C. Faloutsos. Fraudar: Bounding graph fraud in the face of camouflage. In Proceedings of the 22nd ACM SIGKDD international conference on knowledge discovery and data mining, pages 895–904, 2016. [25] M. Jaggi. Revisiting frank-wolfe: Projection-free sparse convex optimization. In International conference on machine learning, pages 427–435. PMLR, 2013. [26] J. Jiang, Y. Chen, B. He, M. Chen, and J. Chen. Spade+: A generic real-time fraud detection framework on dynamic graphs. IEEE Transactions on Knowledge and Data Engineering, 2024. [27] M. Jiang, P. Cui, A. Beutel, C. Faloutsos, and S. Yang. Inferring strange behavior from connectivity pattern in social networks. In Pacific-Asia conference on knowledge discovery and data mining, pages 126–138. Springer, 2014. [28] J. Jiaxin, L. Yuan, H. Bingsheng, H. Bryan, C. Jia, and J. K. Z. Kang. A real-time fraud detection framework on evolving graphs. PVLDB, 2023. [29] S. Kumar, W. L. Hamilton, J. Leskovec, and D. Jurafsky. Community interaction and conflict on the web. In Proceedings of the 2018 world wide web conference, pages 933–943, 2018. [30] V. E. Lee, N. Ruan, R. Jin, and C. Aggarwal. A survey of algorithms for dense subgraph discovery. In Managing and mining graph data, pages 303–336. Springer, 2010. [31] J. Leskovec and A. Krevl. SNAP Datasets: Stanford large network dataset collection. http://snap.stanford.edu/data, June 2014. [32] W. Luo, Z. Tang, Y. Fang, C. Ma, and X. Zhou. Scalable algorithms for densest subgraph discovery. In 2023 IEEE 39th International Conference on Data Engineering (ICDE), pages 287–300. IEEE, 2023. [33] C. Ma, Y. Fang, R. Cheng, L. V. Lakshmanan, and X. Han. A convex-programming approach for efficient directed densest subgraph discovery. In Proceedings of the 2022 International Conference on Management of Data, pages 845–859, 2022. [34] M. Mitzenmacher, J. Pachocki, R. Peng, C. Tsourakakis, and S. C. Xu. Scalable large near-clique detection in large-scale networks via sampling. In Proceedings of the 21th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, pages 815–824, 2015. [35] V. Podpečan, Ž. Ramšak, K. Gruden, H. Toivonen, and N. Lavrač. Interactive explo- ration of heterogeneous biological networks with biomine explorer. Bioinformatics, 06 2019. [36] H. Qin, R.-H. Li, Y. Yuan, G. Wang, L. Qin, and Z. Zhang. Mining bursting core in large temporal graphs. Proceedings of the VLDB Endowment, 2022. [37] Y. Ren, H. Zhu, J. Zhang, P. Dai, and L. Bo. Ensemfdet: An ensemble approach to fraud detection based on bipartite graph. In 2021 IEEE 37th International Conference on Data Engineering (ICDE), pages 2039–2044. IEEE, 2021. [38] R. A. Rossi and N. K. Ahmed. The network data repository with interactive graph analytics and visualization. In AAAI, 2015. [39] A. E. Sarıyüce andA. Pinar. Peeling bipartite networks for dense subgraph discovery. In Proceedings of the Eleventh ACM International Conference on Web Search and Data Mining, pages 504–512, 2018. [40] S. B. Seidman. Network structure and minimum degree. Social networks, 5(3):269– 287, 1983. [41] J. Shi, L. Dhulipala, and J. Shun. Parallel clique counting and peeling algorithms. In SIAM Conference on Applied and Computational Discrete Algorithms (ACDA21), pages 135–146. SIAM, 2021. [42] K. Shin, T. Eliassi-Rad, and C. Faloutsos. Corescope: Graph mining using k-core analysis—patterns, anomalies and algorithms. In 2016 IEEE 16th international conference on data mining (ICDM), pages 469–478. IEEE, 2016. [43] K. Shin, B. Hooi, J. Kim, and C. Faloutsos. Densealert: Incremental dense-subtensor detection in tensor streams. In Proceedings of the 23rd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, pages 1057–1066, 2017. [44] P. Sukprasert, Q. C. Liu, L. Dhulipala, and J. Shun. Practical parallel algorithms for near-optimal densest subgraphs on massive graphs. In 2024 Proceedings of the Symposium on Algorithm Engineering and Experiments (ALENEX), pages 59–73. SIAM, 2024. [45] B. Sun, M. Danisch, T. H. Chan, and M. Sozio. Kclist++: A simple algorithm for finding k-clique densest subgraphs in large graphs. Proceedings of the VLDB Endowment (PVLDB), 2020. [46] C. Tsourakakis. The k-clique densest subgraph problem. In Proceedings of the 24th international conference on world wide web, pages 1122–1132, 2015. [47] C. Ye, Y. Li, B. He, Z. Li, and J. Sun. Gpu-accelerated graph label propagation for real-time fraud detection. In Proceedings of the 2021 International Conference on Management of Data, pages 2348–2356, 2021.'}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "extract_headers_and_content(full_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_document(text):\n",
    "    # Initialize variables\n",
    "    sections = {}\n",
    "    current_section = None\n",
    "    current_content = []\n",
    "\n",
    "    # Split text into lines\n",
    "    lines = text.split('\\n')\n",
    "\n",
    "    # Define heading patterns\n",
    "    numbered_heading_pattern = re.compile(r'^(\\d+(?:\\.\\d+)*)\\s+(.*)$')\n",
    "    # Known headings\n",
    "    known_headings = {\n",
    "        \"Abstract\",\n",
    "        \"Title\",\n",
    "        \"Introduction\",\n",
    "        \"Background\",\n",
    "        \"Related Work\",\n",
    "        \"Evaluation\",\n",
    "        \"Figure\",\n",
    "        \"Table\",\n",
    "        \"Motivation\",\n",
    "        \"Technical Section\",\n",
    "        \"Conclusion\",\n",
    "        \"References\",\n",
    "        \"Appendix\",\n",
    "        # \"Format\",\n",
    "        # \"Acknowledgements\",\n",
    "        # \"Methodology\",\n",
    "        # \"Results\",\n",
    "        # \"Discussion\",\n",
    "    }\n",
    "    known_headings_lower = [h.lower() for h in known_headings]\n",
    "\n",
    "    for line in lines:\n",
    "        line = line.strip()\n",
    "        if not line:\n",
    "            continue  # Skip empty lines\n",
    "\n",
    "        # Check for numbered heading\n",
    "        m = numbered_heading_pattern.match(line)\n",
    "        if m:\n",
    "            # Save current section\n",
    "            if current_section is not None:\n",
    "                content = ' '.join(current_content).strip()\n",
    "                if content:\n",
    "                    sections[current_section] = content\n",
    "            # Start new section\n",
    "            section_number = m.group(1)\n",
    "            section_title = m.group(2).strip()\n",
    "            current_section = f\"{section_number} {section_title}\"\n",
    "            current_content = []\n",
    "            continue\n",
    "\n",
    "        # Check for unnumbered heading (known headings)\n",
    "        if line.lower() in known_headings_lower:\n",
    "            # Save current section\n",
    "            if current_section is not None:\n",
    "                content = ' '.join(current_content).strip()\n",
    "                if content:\n",
    "                    sections[current_section] = content\n",
    "            # Start new section\n",
    "            current_section = line.strip()\n",
    "            current_content = []\n",
    "            continue\n",
    "\n",
    "        # Append line to current content\n",
    "        if current_section is None:\n",
    "            # If no current section, start with 'Abstract'\n",
    "            current_section = 'Abstract'\n",
    "        current_content.append(line)\n",
    "\n",
    "    # Save the last section\n",
    "    if current_section is not None:\n",
    "        content = ' '.join(current_content).strip()\n",
    "        if content:\n",
    "            sections[current_section] = content\n",
    "    return sections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Abstract': 'ABSTRACT Detecting fraudulent activities in financial and e-commerce transac- tion networks is crucial, and one effective method for this is Densest Subgraph Discovery (DSD). However, deploying DSD methods in production systems faces substantial scalability challenges due to the predominantly sequential nature of existing methods, which impedes their ability to handle large-scale transaction networks and results in significant detection delays. In this paper, we introduce Dupin, a novel parallel processing framework designed for efficient DSD processing in billion-scale graphs. Dupin is powered by a processing engine that exploits the unique properties of the peeling process, with theoretical guarantees on detection quality and efficiency.Dupin provides user-friendly APIs for flexible customization of DSD objectives and ensures robust adaptability to diverse fraud detection scenarios. Empirical evaluations indicate that Dupin outperforms several existing DSD methods, with performance improvements observed in specific scenarios reaching up to two orders of magnitude. On billion-scale graphs, Dupin demonstrates the potential to enhance the prevention of fraudulent transactions by approximately 49.5 basis points and reduce density error from 30% to below 5%, as supported by our experimental results. ACM Reference Format: Anonymous Author(s). 2024. Dupin: A Parallel Densest Subgraph Discovery Framework on Massive Graphs. In Proceedings of ACM Conference (Conference’17). ACM, New York, NY, USA, 14 pages. https://doi.org/10.1145/ nnnnnnn.nnnnnnn',\n",
       " '1 INTRODUCTION': 'Graph structures pervade numerous contemporary applications, ranging from transaction and communication networks to expan- sive social media platforms. The study of densest subgraph discovery (DSD), first explored by [21], has since proven instrumental in various domains: link spam identification [4, 19], community de- tection [9, 17], and notably, fraud detection [8, 24, 42]. Central to this domain is the peeling process [2, 5, 8, 24, 46], which operates by iteratively removing vertices based on density metrics, such as vertex degree or the cumulative weight of adjacent edges. Their widespread adoption can be attributed to their inherent efficiency, robustness, and proven worst-case performance guarantees for DSD. In practice, business requirements demand the detection of fraud- ulent communities on billion-scale graphs within seconds. Recent Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for components of this work owned by others than ACM must be honored. Abstracting with credit is permitted. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. Request permissions from permissions@acm.org. Conference’17, July 2017, Washington, DC, USA © 2024 Association for Computing Machinery. ACM ISBN 978-x-xxxx-xxxx-x/YY/MM. . . $15.00 https://doi.org/10.1145/nnnnnnn.nnnnnnn studies, including those by [1] and [47], have highlighted the perva- sive issue of malicious bot activity in e-commerce, accounting for a substantial 21.4% of all traffic in 2018. These fraudulent activities often happen quickly, making them hard to detect. The inability to efficiently process and analyze such massive graphs not only ham- pers fraud detection efforts but also exposes businesses to substan- tial financial risks and reputational damage. Therefore, addressing these challenges is not merely an academic exercise; it is a press- ing necessity for enhancing the integrity of digital transactions and safeguarding consumer trust. Despite extensive research onDSD for static graphs [20–22, 30, 33, 34], dynamic graphs [2, 18, 26, 28, 43], and parallel approaches [14, 16, 41, 42], existing methods face significant challenges for practical de- ployment in real-world fraud detection systems. For instance, many existing algorithms primarily rely on static score functions for DSD, lacking the flexibility required for dynamic and evolving fraud detec- tion scenarios. Methods such as [4] and [27] are designed with fixed detection criteria, which can become less effective as fraudulent be- haviors adapt over time. Additionally, incremental algorithms remain inefficient at handling billion-scale graphs with high transaction volumes, leading to increased latency. Parallel DSD frameworks often lack efficient pruning techniques, resulting in excessive iterations that degrade performance on large-scale graphs. This paper addresses the following research question: How can we enhance the efficiency and scalability of DSD methods to effectively detect fraud in massive graphs? Challenges. We evaluated existing methods with our industry partner, Anon (anonymized for review). The following issues surfaced: Firstly, existing algorithms focus on static score functions for DSD and lack flexible customization. While this may be adequate for short-term detection, it becomes less effective over time. For example, as shown in Figure 1a, graph density can accumulate sig- nificant errors within a single day, highlighting the need for more adaptive and customizable detection methods. Secondly, existing incremental algorithms are still inefficient at handling billion-scale graphs with high transaction volumes, leading to increased latency. This limitation is evident in Figure 1b, which illustrates the system load distribution for a specific day’s transactions, underscoring the inability of current methods to efficiently manage high transaction volumes without significant delays. Thirdly, existing parallel densest subgraph discovery frameworks lack efficient pruning techniques, re- sulting in a large number of iterations. This inefficiency is especially problematic on billion-scale graphs, where performance degrades substantially due to the excessive computational overhead caused by numerous iterations. Adapting to Evolving Research Needs. Recognizing the dynamic nature of fraud detection and the rapid evolution of fraudulent tactics, we have adapted our research focus to address these challenges. Our approach incorporates flexible customization options for DSD methods, allowing for adjustments based on the changing landscape of fraudulent activities. This adaptability is crucial for maintaining the effectiveness of fraud detection systems in an environment where new threats continuously emerge. Contributions. In this work, we propose a DSD framework for fraud detection. Our key contributions are as follows: • A DSD Parallelization Framework. We propose Dupin, a novel parallel processing framework for densest subgraph discovery (DSD) that breaks the sequential dependencies inherent in tra- ditional density metrics, significantly enhancing scalability and efficiency on both weighted and unweighted graphs. Unlike existing methods that focus on specific density metrics, Dupin allows developers to define custom fraud detection semantics through flexible suspiciousness functions for edges and vertices. It supports a wide range of DSD semantics, including DG [6], DW [23], FD [24], TDS [46], and 𝑘CLiDS [14]. Importantly, Dupin not only leverages parallelism but also provides theoretical guarantees on approximation ratios for the supported density metrics. To the best of our knowledge, this is the first generic parallel processing framework that accommodates a broad class of density metrics in DSD, offering both flexibility and theoretical assurance. • Exploiting Long-Tail Properties.We identify and address the long-tail problem in the peeling process of DSD algorithms, where late iterations remove only a small fraction of vertices and con- tribute little to the quality of the densest subgraph, leading to inefficiency. To tackle this, we introduce two novel optimizations: Global Pruning Optimization, which proactively removes low-contribution vertices within each peeling iteration to reduce computational overhead, and Local Pruning Optimization, which eliminates disparate structures formed after vertex removal to enhance the density and quality of the detected subgraph. These optimizations significantly reduce the number of iterations re- quired, improving both performance and result quality. • Empirical Validation and Real-World Impact: We conduct comprehensive experiments on large-scale industry datasets to validate the effectiveness of Dupin. Our results show that Dupin achieves up to 100 times faster fraud detection compared to the state-of-the-art incremental method Spade [28] and parallel methods like PBBS [41]. Moreover,Dupin is capable of preventing up to 94.5% of potential fraud, demonstrating significant practical bene- fits and reinforcing system integrity in real-world applications. Organization. The remainder of this paper is structured as follows: Section 2 presents the background and the literature review. Section 3 introduces the architecture of Dupin. In Section 4, we propose the theoretical foundations for parallel peeling-based algorithms for DSD. Section 5 introduces long-tail pruning techniques to improve efficiency and effectiveness. The experimental evaluation of Dupin is presented in Section 6. Finally, Section 7 concludes the paper and outlines avenues for future research.',\n",
       " '2.1 Preliminary': 'This subsection presents the preliminary. Some frequently used no- tations are summarized in Table 1. Graph 𝐺 . In this work, we operate on a weighted graph denoted as 𝐺 = (𝑉 , 𝐸), where 𝑉 represents the set of vertices and 𝐸 (⊆ 𝑉 ×𝑉 ) represents the set of edges. Each edge (𝑢𝑖 , 𝑢 𝑗 ) ∈ 𝐸 is assigned a nonnegative weight, expressed as 𝑐𝑖 𝑗 . Additionally, 𝑁 (𝑢) denotes the set of neighboring vertices connected to vertex 𝑢. Induced Subgraph. For a given subset 𝑆 of vertices in 𝑉 , we define the induced subgraph as 𝐺[𝑆] = (𝑆, 𝐸[𝑆]), where 𝐸[𝑆] = {(𝑢, 𝑣) | (𝑢, 𝑣) ∈ 𝐸 ∧ 𝑢, 𝑣 ∈ 𝑆}. The size of the subset 𝑆 is denoted by |𝑆 |. Density Metrics 𝑔 and Weight Function 𝑓 . We utilize the class of metrics𝑔 as defined in prior works [6, 24, 28], where for a given subset of vertices 𝑆 ⊆ 𝑉 , the density metric is computed as: 𝑔(𝑆) = 𝑓 (𝑆) |𝑆 | . Here, 𝑓 (𝑆) represents the total weight of the induced subgraph 𝐺[𝑆], which is defined as the sum of the weights of vertices in 𝑆 and the weights of edges in 𝐸[𝑆]. More formally: 𝑓 (𝑆) = ∑︁ 𝑢𝑖 ∈𝑆 𝑎𝑖 + ∑︁ (𝑢𝑖 ,𝑢 𝑗 )∈𝐸[𝑆] 𝑐𝑖 𝑗 (1) The weight of a vertex 𝑢𝑖 , denoted as 𝑎𝑖 (𝑎𝑖 ≥ 0), quantifies the suspiciousness of user 𝑢𝑖 . The weight of an edge (𝑢𝑖 , 𝑢 𝑗 ), represented by 𝑐𝑖 𝑗 (𝑐𝑖 𝑗 > 0), reflects the suspiciousness of the transaction between users 𝑢𝑖 and 𝑢 𝑗 . Intuitively, the density metric 𝑔(𝑆) encapsulates the overall suspiciousness density of the induced subgraph𝐺[𝑆]. A larger value of 𝑔(𝑆) indicates a higher density of suspiciousness within𝐺[𝑆]. The vertex set that maximizes the density metric 𝑔 is denoted by 𝑆∗. We next introduce five representative density metrics extensively employed in many fraud detection applications. The density metrics are mainly differed by how the weight function is defined. Dense Subgraphs (DG) [6]. The DG metric is employed to quan- tify the connectivity of substructures within a graph. It has found extensive applications in various domains, such as detecting fake comments in online platforms [29] and identifying fraudulent activities in social networks [3]. Given a subset of vertices 𝑆 ⊆ 𝑉 , the weight function of DG is defined as 𝑓 (𝑆) = |𝐸[𝑆]|, where |𝐸[𝑆]| denotes the number of edges in the induced subgraph 𝐺[𝑆]. Dense Subgraph on Weighted Graphs (DW) [23]. In transac- tion graphs, it is common to have weights assigned to the edges, representing quantities such as the transaction amount. The DW metric extends the concept of dense subgraphs to accommodate these weighted relationships. Given a subset of vertices 𝑆 ⊆ 𝑉 , the weight function of DW is defined as 𝑓 (𝑆) = ∑ (𝑢𝑖 ,𝑢 𝑗 )∈𝐸[𝑆] 𝑐𝑖 𝑗 . Fraudar (FD) [24]. In order to mitigate the effects of fraudulent actors deliberately obfuscating their activities, Hooi et al. [24] introduced the FD algorithm. This approach innovatively incorporates weights for edges and assigns prior suspiciousness scores to vertices, potentially utilizing side information to enhance detection accuracy. Given a subset of vertices 𝑆 ⊆ 𝑉 , the weight function of FD is defined as Equation 1 where 𝑎𝑖 denotes the suspicious score of vertex 𝑢𝑖 and 𝑐𝑖 𝑗 = 1 log(𝑥+𝑐) . Here 𝑥 is the degree of the object vertex between 𝑢𝑖 and 𝑢 𝑗 , and 𝑐 is a positive constant [24]. Triangle Densest Subgraph (TDS) [46]. Triangles serve as a crucial structural motif in graphs, providing valuable insights into the connectivity and cohesiveness of subgraphs. The TDS metric, in par- ticular, leverages the prevalence of triangles to quantify the density of a subgraph. Given a subset of vertices 𝑆 ⊆ 𝑉 , the weight function of TDS is defined as 𝑓 (𝑆) = 𝑡 (𝑆) where 𝑡 (𝑆) represents the total number of triangles within the induced subgraph 𝐺[𝑆]. 𝑘-Clique Densest Subgraph (𝑘CLiDS) [14]. Extending the concept of triangle densest subgraphs, 𝑘CLiDS focuses on identifying dense subgraphs based on the presence of 𝑘-Cliques. A 𝑘-Clique is a complete subgraph of 𝑘 vertices, representing a tightly-knit group of vertices. For a given subset of vertices 𝑆 ⊆ 𝑉 , the weight function of 𝑘CLiDS is defined as 𝑓 (𝑆) = 𝑘(𝑆), where 𝑘(𝑆) denotes the number of 𝑘-Cliques within the induced subgraph 𝐺[𝑆].',\n",
       " '2.2 Sequential Peeling Algorithms': 'Peeling algorithms have gained popularity for their efficiency, ro- bustness, and proven worst-case performance in various applica- tions [6, 24, 46]. Next, we provide a concise overview of the typical execution flow of these algorithms. Peeling Weight.We use𝑤𝑢𝑖 (𝑆) to indicate the decrease in the value of the weight function 𝑓 when the vertex𝑢𝑖 is removed from a vertex set 𝑆 , i.e., the peeling weight. Formally, 𝑤𝑢𝑖 (𝑆) = 𝑎𝑖 + ∑︁ (𝑢 𝑗 ∈𝑆) ∧ ((𝑢𝑖 ,𝑢 𝑗 )∈𝐸) 𝑐𝑖 𝑗 + ∑︁ (𝑢 𝑗 ∈𝑆) ∧ ((𝑢 𝑗 ,𝑢𝑖 )∈𝐸) 𝑐 𝑗𝑖 (2) Algorithm 1: Sequential Peeling Algorithm Input: A graph𝐺 = (𝑉 , 𝐸) and a density metric 𝑔(𝑆) Output: A subset of vertices 𝑆 that maximizes the density metric 𝑔(𝑆) 1 𝑆0 ← 𝑉 /* Initialize the subset with all vertices */ 2 for 𝑖 ← 1 to |𝑉 | do 3 𝑢 ← arg max𝑣∈𝑆𝑖−1 𝑔(𝑆𝑖−1 \\\\ {𝑣 }) /* The vertex to be peeled */ 4 𝑆𝑖 ← 𝑆𝑖−1 \\\\ {𝑢 } /* The selected vertex */ 5 return arg max𝑆𝑖 𝑔(𝑆𝑖 ) /* The subset that maximizes 𝑔(𝑆) */ Sequential PeelingAlgorithm (Algorithm1).The peeling process starts with the full vertex set 𝑆0 = 𝑉 (Line 1) and iteratively removes vertices to maximize the density metric 𝑔. In each step 𝑖 , a vertex𝑢𝑖 is removed from 𝑆𝑖−1 to form 𝑆𝑖 , such that 𝑔(𝑆𝑖−1 \\\\ {𝑢𝑖 }) is maximized (Lines 3∼4). This process is repeated until 𝑆𝑖 becomes empty, resulting in a sequence of nested sets 𝑆0, 𝑆1, . . . , 𝑆 |𝑉 | . The algorithm ultimately returns the subset 𝑆𝑖 ∈ {𝑆0, . . . , 𝑆 |𝑉 |} that maximizes 𝑔(𝑆𝑖 ). Example 2.1 (Seqential Peeling Algorithm Process). Consider the graph 𝐺 as shown in Figure 2. The process begins with an initial graph density of 2.33. In the first iteration, vertex 𝑢1 is peeled due to its smallest peeling weight among all vertices. Subsequently, vertices are peeled in the following order: 𝑢2, 𝑢3, 𝑢4, 𝑢5, and 𝑢6 based on the updated criteria after each peeling operation. Notably, after peeling 𝑢2, the graph density increases, which is a characteristic observation in the peeling process. The sequence of peeling continues until the last vertex, 𝑢6, is peeled, resulting in a final graph density of 0. The peeling sequence, therefore, is 𝑂 = [𝑢1, 𝑢2, 𝑢3, 𝑢4, 𝑢5, 𝑢6], completely disassembling the graph. The induced subgraph𝐺[𝑆] of 𝑆 = [𝑢3, 𝑢4, 𝑢5, 𝑢6] has the greatest density of 2.75, thus 𝐺[𝑆] is returned. Approximation. An algorithm 𝑄 is defined as an 𝛼-approximation for the Densest Subgraph Discovery (DSD), where 𝛼 ≥ 1, if for any given graph it returns a subset 𝑆 satisfying the condition: 𝑔(𝑆) ≥ 𝑔(𝑆∗) 𝛼 , where 𝑆∗ is the optimal subset that maximizes the density metric 𝑔. Theorem 2.1 ([24]). For the vertex set 𝑆𝑝 returned by Algorithm 1 and the optimal vertex set 𝑆∗, it holds that 𝑔(𝑆𝑝 ) ≥ 𝑔(𝑆∗) 2 for DG, DW and FD as the density metrics. We extend the analysis to TDS and 𝑘CLiDS. The proof of the following theorem is omitted due to space limitation. Theorem 2.2. For the vertex set 𝑆𝑝 returned by Algorithm 1 and the optimal vertex set 𝑆∗, it holds that 𝑔(𝑆𝑝 ) ≥ 𝑔(𝑆∗) 𝑘 for TDS and 𝑘CLiDS as the density metrics where 𝑘 = 3 for TDS.',\n",
       " '2.3 Related work': 'Densest Subgraph Discovery (DSD). Densest subgraph discov- ery represents a core area of research in graph theory, extensively explored in literature [7, 8, 20, 30, 39, 40]. The foundational max-flow- based algorithm by Goldberg et al. [21] set a benchmark for exact densest subgraph detection, with subsequent studies introducing more scalable exact solutions [33, 34]. While speed enhancements have been achieved through various greedy approaches [6, 8, 24, 33], these methods are inherently sequential and difficult to parallelize. For example, they often peel one vertex at a time, which results in too many peeling iterations. This sequential nature limits their scalability and efficiency on massive graphs. In graph-based fraud de- tection, techniques like COPYCATCH [4] and GETTHESCOOP [27] leverage local search heuristics to identify dense subgraphs in bi- partite networks, frequently applied in fraud, spam, and community detection across social and review platforms [24, 37, 42]. Despite their widespread use, these methods suffer from efficiency issues in very large graphs, often involving numerous iterations and lack- ing effective pruning techniques, which exacerbates the long-tail problem. Moreover, they are limited to specific definitions of dense subgraphs. Contrarily, Dupin offers a versatile, parallel approach to densest subgraph detection. It facilitates the simultaneous peeling of multiple vertices, drastically reducing the computational overhead. Additionally, Dupin provides APIs that allow users to customize defi- nitions of dense subgraphs, ensuring that the resultant graph density adheres to theoretical bounds. DSD on Dynamic Graphs. To tackle real-time fraud detection, sev- eral variants have been designed for dynamic graphs [10, 18, 26, 28, 43]. [28] addresses real-time fraud community detection on evolving graphs (edge insertions) using an incremental peeling sequence re- ordering method, while [26] extends this approach to fully dynamic graphs (edge insertions and deletions). [43] proposes an incremental algorithm that maintains and updates a dense subtensor in a ten- sor stream. [12] and [36] utilize sliding windows to detect dense subgraphs and bursting cores within specific time windows. These methods have three limitations. First, response time is an issue. Al- though they respond quickly to benign communities, the results can change drastically for newly formed fraudulent communities, caus- ing significant delays in incremental calculations for fraud detection. Second, efficiency is problematic. While incremental processing is faster than recomputation for a single transaction on billion-scale graphs, frequent incremental evaluations can be slower than a full recomputation. Third, in terms of flexibility, most methods support only a single semantic. In contrast, Dupin introduces a unified parallel framework for peeling-based dense subgraph detection.Dupin can respond within seconds on billion-scale graphs and defines multiple density metrics that can be used on the Dupin platform. Parallel DSD. Several parallel methods for DSD have been pro- posed [2, 14, 16, 39, 41, 42]. Bahmani et al. [2] proposed a parallel algorithm for DG using the MapReduce framework. FWA [15] proposed a parallel algorithm for DW based on Frank-Wolfe algo- rithm [25] using OpenMP [13]. Some works propose parallel 𝑘-core algorithms [32, 41, 44]. Others introduce parallel algorithms for 𝑘- Clique detection, with approaches offering a (1 + 𝜖)-approximation ratio and parallel implementations for the 𝑘CLiDS problem [16, 45]. There are also algorithms for 𝑘-Clique densest subgraph detection using OpenMP [14], and methods for dense subtensor detection [42]. However, these systems have two main limitations. First, they typi- cally support only a single-densitymetric and do not handle weighted graphs. In real-world applications, transactions between users can have varying degrees of importance based on amounts, frequen- cies [28], and other factors. Extending these systems to support other dense metrics with theoretical guarantees is non-trivial. Second, they often lack efficient pruning techniques, requiring numerous itera- tions and suffering from the long-tail issue, which hinders scalability on massive graphs. In contrast, Dupin defines the characteristics of a set of supported dense metrics and accommodates weighted sub- graphs, allowing for more nuanced analysis reflective of real-world data. Our method introduces efficient pruning strategies by peeling multiple vertices in parallel based on a threshold mechanism, signifi- cantly reducing the number of iterations and mitigating the long-tail problem. This makes Dupin more effective and scalable in diverse real-world scenarios compared to previous approaches. 3 THE Dupin FRAMEWORK',\n",
       " '3.1 Motivations': 'A notable example of advanced fraud detection is seen in Anon, a prominent technology company in Southeast Asia known for its digital payment and food delivery services. As delineated in [28], the detection process begins with forming a transaction graph,𝐺 , derived from the platform’s transactions. This graph is incrementally updated, represented as 𝐺 = 𝐺 ⊕ ∆𝐺 . Subsequent stages utilize densest subgraph discovery algorithms to uncover fraudulent communities. A typical fraud scenario on the platform involves customer-merchant collusion, where fake accounts are created to exploit promotional incentives. These fraudulent activities manifest as densest subgraphs within the transaction graph. Efficiency. Our empirical findings reveal that the incremental execution of DSD algorithms, such as Spade [28], on a graph with 2 billion edges requires approximately 198 seconds per batch (default setting: one batch of 1K edge insertions). However, in practical business sce- narios, the number of incoming edges can peak at 400K per minute. Clearly, using incremental evaluation on billion-scale graphs does not provide sufficient throughput to meet business requirements. Latency and Prevention Ratio. Existing DSD systems face sig- nificant challenges in terms of latency and prevention ratio. Upon identification of fraudulent activities, actions such as account bans or freezes are implemented to prevent further losses. However, the aver- age latency for these systems to detect fraud and take action is often high, typically around 200 to 300 seconds for incremental algorithms and more than 1000 seconds for existing parallel algorithms, which is insufficient for real-time requirements (operational demand requires detection within seconds). Moreover, when the graph size increases from million-scale to billion-scale, the prevention ratio decreases from 92.47% to 45%. These issues underscore the necessity for more efficient and scalable algorithms that can maintain low latency and high prevention ratios, even under heavy transaction loads. Parallelization offers a viable solution to these challenges by dis- tributing the computational load across multiple processors. Mod- ern CPUs are equipped with numerous cores that are cost-effective, enabling parallel processing. If the peeling process of DSD is paral- lelized, different processors can simultaneously evaluate and peel different nodes, thereby increasing scalability and efficiency. For example, if we can improve processing speed by one to two orders of magnitude, the system would be able to meet industry requirements for real-time fraud detection, completing detection within seconds. Problem Statement. Given a graph 𝐺 = (𝑉 , 𝐸) and a community detection semantic with density metric 𝑔(𝑆), our goal is to find a vertex subset 𝑆∗ ⊆ 𝑉 that maximizes 𝑔(𝑆∗).',\n",
       " '3.2 System Architecture': 'To enable users to design complex DSD algorithms to detect fraudsters based on our framework, Dupin, we have introduced the archi- tecture, APIs, and supported density metrics in this section. Architecture of Dupin. Figure 3 presents the architecture of Dupin with user-defined functions for detecting suspicious activities.Dupin enables users to define their fraud detection semantics through a set of provided APIs. Within Dupin’s computational engine, we sup- port both sequential and parallel peeling algorithms. Additionally, algorithms for long-tail pruning techniques are implemented to ac- celerate convergence and enhance the density of subgraphs in each iteration. Dupin identifies fraudulent communities, which are then flagged for further investigation by moderators. APIs of Dupin (Figure 3). Dupin simplifies the process of defining detection semantics by allowing users to specify a few simple functions. Additionally, Dupin offers APIs that enable users to balance detection speed and accuracy effectively. The key APIs encompass several components designed to facilitate the customization and op- timization of fraud detection processes. These components provide a robust framework for users to implement their fraud detection strate- gies, ensuring both high performance and precision in identifying fraudulent activities. Details are listed below: • pWeight and updateNgh. These functions are designed for the dynamic adjustment of weights within the network. They allow the user to define the peeling weight of a vertex and specify the method for updating the peeling weight of neighboring vertices after a vertex has been peeled. • VSusp and ESusp. Given a vertex/edge, these components are re- sponsible for deciding the suspiciousness of the endpoint of the edge or the edge with a user-defined strategy. • isBenign. This component is used to decide whether a vertex is benign during the whole peeling process (Section 5). If the vertex is benign, it is peeled within the current iteration. • setEpsilon. This function is utilized to control the precision of fraud detection. During periods of high system load, moderators can increase 𝜖 to achieve higher throughput. Conversely, when the system load is low, 𝜖 can be decreased to enhance accuracy. • setK. This function specifies the 𝑘-Clique parameter, enabling the definition of the 𝑘-Clique for analysis. 1 double vsusp(const Vertex& v, const Graph& g) { 2 return g.weight[v]; // Side information on vertex 3 } 4 double esusp(const Edge& e, const Graph& g) { 5 return 1.0 / log(g.deg[e.src] + 5.0); 6 } 7 int main() { 8 Dupin dupin; 9 dupin.VSusp(vsusp); // Plug in vsusp 10 dupin.ESusp(esusp); // Plug in esusp 11 dupin.setEpsilon (0.1); 12 dupin.LoadGraph(\"graph_sample_path\"); 13 vector <Vertex > fraudsters = dupin.ParDetect (); 14 return 0; 15 } Listing 1: Implementation of FD on Dupin Example (Listing 1). To implement FD [24] on Dupin, users simply need to plug in the suspiciousness function vsusp for the vertices by calling VSusp and the suspiciousness function esusp for the edges by calling ESusp. Specifically, 1) vsusp is a constant function, i.e., given a vertex 𝑢, vsusp(𝑢) = 𝑎𝑖 and 2) esusp is a logarithmic function such that given an edge (𝑢𝑖 , 𝑢 𝑗 ), esusp(𝑢𝑖 , 𝑢 𝑗 ) = 1 log(𝑥+𝑐) , where 𝑥 is the degree of the object vertex between 𝑢𝑖 and 𝑢 𝑗 , and 𝑐 is a positive constant [24]. Developers can easily implement customized peeling algorithms with Dupin, significantly reducing the engineering effort. Density Metrics Support in Dupin. To illustrate the versatility of Dupin in handling various density metrics, we establish comprehensive sufficient conditions. These conditions ensure Dupin’s adaptability in integrating a wide range of metrics to assess graph density effectively. This foundational approach enablesDupin to sup- port diverse density calculations, crucial for nuanced fraud detection and network analysis tasks. Property 3.1. Dupin supports a density metric 𝑔(𝑆) if the following conditions are met: (1) 𝑔(𝑆) = 𝑓 (𝑆) |𝑆 | , where 𝑓 is a monotone increasing function. (2) 𝑎𝑖 ≥ 0, ensuring non-negative vertex weight function. (3) 𝑐𝑖 𝑗 ≥ 0, ensuring non-negative edge weight function.',\n",
       " '4 PARALLELABLE PEELING ALGORITHMS': 'For sequential peeling algorithms, only one vertex can be removed at a time, with each vertex’s removal depending on the removal of previous vertices. This dependency prevents the algorithm from executing multiple vertex removals simultaneously. In this section, we introduce a parallel peeling paradigm to break this dependency chain. This new approach allowsmultiple vertices to be peeled in parallel without affecting the programming abstractions that Dupin provides to end users. We then demonstrate how this paradigm offers fine-grained trade-offs between parallelism and ap- proximation guarantees for all density metrics studied in this work.',\n",
       " '4.1 Parallel Peeling Paradigm': 'Instead of peeling the vertex with the maximum peeling weight, we can peel all vertices that lead to a significant decrease in the density scores of the remaining graph. This decrease is controlled by a tunable parameter 𝜖 . A larger 𝜖 allows more vertices to be peeled in parallel, while a smaller 𝜖 provides a better approximation ratio. Algorithm 2: Dupin: Parallel Peeling Input:𝐺 = (𝑉 , 𝐸), a density metric 𝑔(𝑆), and 𝜖 ≥ 0 Output: A subset of vertices 𝑆 that maximizes the density metric 𝑔(𝑆) 1 𝑆0 ← 𝑉 /* Initialize the subset with all vertices */ 2 𝑖 ← 1 /* Initialize the index */ 3 while 𝑆𝑖−1 ̸= ∅ do 4 𝜏 = 𝑘(1 + 𝜖) · 𝑔(𝑆𝑖−1) /* The threshold for peeling */ 5 𝑈 ← {𝑢 | 𝑤𝑢 (𝑆𝑖−1) ≤ 𝜏 } /* Vertices to be peeled */ 6 𝑆𝑖 ← 𝑆𝑖−1 \\\\𝑈 /* Update the subset */ 7 𝑖 ← 𝑖 + 1 /* Increment the index */ 8 return arg max𝑆𝑖 𝑔(𝑆𝑖 ) /* The subset that maximizes 𝑔(𝑆) */ Parallel Peeling (Algorithm 2). Let 𝑆𝑖 denote the vertex set after the 𝑖-th peeling iteration. Initially, 𝑆0 = 𝑉 (Line 1). In each iteration, it removes a subset of vertices𝑈 from 𝑆𝑖−1, where ∀𝑢 ∈ 𝑈 ,𝑤𝑢 (𝑆𝑖−1) ≤ 𝑘(1 + 𝜖)𝑔(𝑆𝑖−1). Parameter 𝑘 is determined by the metric used. 𝑘 = 2 works for DG, DW and FD. 𝑘 = 3 works for TDS and 𝑘 is the size of the cliques for 𝑘CLiDS. This process is repeated recursively until no vertices are left, resulting in a series of vertex sets 𝑆0, . . . , 𝑆𝑅 , where 𝑅 is the number of iterations. The algorithm then returns the set 𝑆𝑖 (𝑖 ∈ [0, 𝑅]) that maximizes the density metric 𝑔(𝑆𝑖 ), denoted as 𝑆𝑝 . In what follows, we also show a case where 𝑘CLiDS is the density metric for DSD. 3 3 6 6 5 4 21 0 00 3 3 6 6 3 3 0 0 Density: 0.91 Density: 1.33 Density: 0 Iteration i Iteration i+1 Iteration i+2 Iteration i+3 Density: 0 τ = 3× 0.91 = 2.73 τ = 3× 1.33 = 3.99 τ = 0 τ = 0',\n",
       " '4.2 Theoretical Analysis': 'In this section, we analyze the theoretical properties of the parallel peeling process. We begin by examining the number of iterations required for parallel peeling. Built upon the result, we then determine the time complexity and the approximation ratio of the parallel peeling process. Lemma 4.1. Given any 𝜖 > 0, 𝑅 < log 1+𝜖 |𝑉 |. Proof. We prove that for any 𝜖 ≥ 0, the size of the remaining vertex set is bounded by |𝑆𝑖 |< 1 1+𝜖 |𝑆𝑖−1 | for all 𝑖 ∈ [1, 𝑅]. We first examine the peeling weights of the vertices in 𝑆𝑖−1 for the density metrics DG. DW and FD where 𝑘 = 2. As each edge weight in the peeling weight calculation contributes twice (once for each endpoint), and the vertex weights contribute once, we have∑︁ 𝑢∈𝑆𝑖−1 𝑤𝑢 (𝑆𝑖−1) ≤ 2𝑓 (𝑆𝑖−1), (3) The peeling weights of the vertices in 𝑆𝑖−1 are calculated as follows:∑︁ 𝑢∈𝑆𝑖−1 𝑤𝑢 (𝑆𝑖−1) = ∑︁ 𝑢∈𝑆𝑖−1\\\\𝑆𝑖 𝑤𝑢 (𝑆𝑖−1) + ∑︁ 𝑢∈𝑆𝑖 𝑤𝑢 (𝑆𝑖−1) ≥ ∑︁ 𝑢∈𝑆𝑖 𝑤𝑢 (𝑆𝑖−1) > 2(1 + 𝜖)|𝑆𝑖 |𝑔(𝑆𝑖−1) (4) Combining Equation 3 and Equation 4, we have: 2(1 + 𝜖)|𝑆𝑖 |𝑔(𝑆𝑖−1) < 2𝑓 (𝑆𝑖−1) = 2|𝑆𝑖−1 |𝑔(𝑆𝑖−1) (5) For all 𝑖 ∈ [1, 𝑅], |𝑆𝑖 |< 1 1 + 𝜖 |𝑆𝑖−1 | (6) Then we have: |𝑉 |= |𝑆0 |> 1 1 + 𝜖 |𝑆1 |> . . . > ( 1 1 + 𝜖 ) 𝑅 |𝑆𝑅 | (7) Therefore, we have 𝑅 < log 1+𝜖 |𝑉 |. For the density metrics TDS and 𝑘CLiDS, we use the fact that each edge contributes to the peeling weight calculation exactly 𝑘 times due to the counting of each edge within every 𝑘-Clique in the graph (𝑘 = 3 for TDS). This multiplicity of edge contributions leads to the relationship: ∑︁ 𝑢∈𝑆𝑖−1 𝑤𝑢 (𝑆𝑖−1) ≤ 𝑘 𝑓 (𝑆𝑖−1), (8) We then adapt Equations 4-7 and the proof follows. □ Time Complexity. Algorithm 2 operates for at most 𝑅 = log 1+𝜖 |𝑉 | iterations, with each iteration scanning all vertices, taking 𝑂(|𝑉 |) time. As vertices are peeled, updates are necessitated for the peeling weights of their neighboring vertices. Over the course of 𝑅 iterations, there are no more than |𝐸 | such updates for DG, DW and FD. Consequently, Algorithm 3 incurs 𝑂((|𝐸 |+|𝑉 |) log 1+𝜖 |𝑉 |) node/edge weight updates. For DG, DW and FD, the update costs are 𝑂(1). For TDS and 𝑘CLiDS, we follow previous studies [11, 14] to compute the peeling weights, and the complexity is 𝑂(𝑘 |𝐸 |𝛼(𝐺) 𝑘−2 ), where 𝛼(𝐺) is the arboricity of the graph. Hence, the overall complexity is 𝑂(𝑘 |𝐸 |𝛼(𝐺) 𝑘−2 (|𝐸 |+|𝑉 |) log 1+𝜖 |𝑉 |). The cost of peeling weight up- dates is orthogonal to the Dupin framework. Approximation Ratio. Next, we are ready to show that Dupin provides a theoretical guarantee with a 𝑘(1 + 𝜖)-approximation for all density metrics studied in this work. We denote 𝑢𝑖 ∈ 𝑆𝑝 as the first vertex in 𝑆∗ peeled by Dupin, and 𝑆 ′ as the peeling subsequence starting from 𝑢𝑖 . Theorem 4.2. For the vertex set 𝑆𝑝 returned by Dupin and the optimal vertex set 𝑆∗, the relationship 𝑔(𝑆𝑝 ) ≥ 𝑔(𝑆∗) 𝑘(1+𝜖) holds. Proof. We first prove that ∀𝑢𝑖 ∈ 𝑆∗, 𝑤𝑢𝑖 (𝑆 ∗ ) ≥ 𝑔(𝑆∗). We prove it in contradiction. We assume that 𝑤𝑢𝑖 (𝑆 ∗ ) < 𝑔(𝑆∗). By peeling 𝑢𝑖 from 𝑆∗, we have the following. 𝑔(𝑆∗ \\\\ {𝑢𝑖 }) = 𝑓 (𝑆∗) −𝑤𝑢𝑖 (𝑆 ∗ ) |𝑆∗ |−1 > 𝑓 (𝑆∗) − 𝑔(𝑆𝑖 ) |𝑆∗ |−1 = 𝑓 (𝑆∗) − 𝑔(𝑆∗) |𝑆∗ |−1 = 𝑓 (𝑆∗) − 𝑓 (𝑆∗) |𝑆∗ | |𝑆∗ |−1 = 𝑔(𝑆∗) (9) Therefore, a better solution can be obtained by removing 𝑢𝑖 from 𝑆∗, which contradicts the fact that 𝑆∗ is the optimal solution. We can conclude that𝑤𝑢𝑖 (𝑆 ∗ ) ≥ 𝑔(𝑆∗). Next, we assume that 𝑢𝑖 ∈ 𝑆𝑝 is the first vertex in 𝑆∗ peeled by the peeling algorithm. Since 𝑆∗ is the optimal vertex set, we have 𝑔(𝑆∗) ≤ 𝑤𝑢𝑖 (𝑆 ∗ ), (10) because the optimal value 𝑔(𝑆∗) must be less than or equal to the weight contribution of any single vertex in 𝑆∗, in particular 𝑢𝑖 . Since 𝑆∗ ⊆ 𝑆 ′, it is clear that 𝑤𝑢𝑖 (𝑆 ∗ ) ≤ 𝑤𝑢𝑖 (𝑆 ′ ) (11) By the peeling condition, we have 𝑤𝑢𝑖 (𝑆 ′ ) ≤ 𝑘(1 + 𝜖)𝑔(𝑆 ′) (12) Therefore, we have 𝑔(𝑆∗) ≤ 𝑤𝑢𝑖 (𝑆 ∗ ) ≤ 𝑤𝑢𝑖 (𝑆 ′ ) ≤ 𝑘(1 + 𝜖)𝑔(𝑆 ′) ≤ 𝑘(1 + 𝜖)𝑔(𝑆𝑝 ) (13) Hence, we can conclude that 𝑔(𝑆𝑝 ) ≥ 𝑔(𝑆∗) 𝑘(1+𝜖) . □ Summary. Dupin can easily balance efficiency and the worst-case approximation ratio with a single parameter, 𝜖 . A larger 𝜖 allows more vertices to be peeled in parallel, reducing the number of peeling iterations. Meanwhile, the worst-case approximation ratio of parallel peeling is only affected by a factor of (1 + 𝜖) compared to the ratio for sequential peeling w.r.t. all density metrics studied in this work.',\n",
       " '5 LONG-TAIL PRUNING': 'We observe that the parallel peeling algorithm encounters two sig- nificant issues. First, there is a long-tail problem where subsequent iterations fail to produce denser subgraphs. Instead, each iteration only peels off a small fraction of vertices, which significantly impacts the overall runtime efficiency. Second, during batch peeling, each peeling iteration often results in disparate structures. These are pri- marily caused by the peeling of certain vertices, which leaves their neighboring structure disconnected and fragmented. This phenome- non affects the continuity and coherence of the subgraphs. As shown in Table 2, on the la dataset (the largest social network dataset tested in our experiments), DG, DW, and FD experience 24.67%, 46.84%, and 3.01% of rounds as ineffective long-tail iterations, severely limiting response time. Additionally, during the peeling process, if the batch size is large (e.g., 𝜖 = 0.5), approximately 25.34%, 29.46%, and 7.16% of nodes become disconnected and sparse, signifi- cantly impacting the quality of the detected subgraphs. To address the challenges of high iteration counts in peeling algorithms, Dupin introduces two long-tail pruning techniques: Global Peeling Optimization (GPO) to maintain a global peeling threshold, and Local Peeling Optimization (LPO) to improve the density of the current peeling iteration’s subgraph, thereby increasing the peeling threshold. We tested three algorithms, DG, DW, and FD, and found that they required 17,637, 150,223, and 112,074 iterations respectively, which is significantly high. By strategically pruning long-tail ver- tices and sparse vertices, the number of iterations can be reduced by up to 46.84%. Additionally, LPO can further reduce the number of iterations by up to 92.79%. These optimizations significantly enhance the efficiency of the peeling process, as discussed in this section.',\n",
       " '5.1 Global Peeling Optimization': 'Initially, we define what constitutes a long-tail vertex during the peeling process. Peeling these long-tail vertices results in ineffective peeling iterations. To address this, we maintain a global maximum peeling threshold to determine which vertices can be peeled directly. When the global peeling threshold is greater than the current itera- tion’s threshold, we use the global peeling threshold for peeling. Algorithm 3: DupinGPO: Global Peeling Optimization Input:𝐺 = (𝑉 , 𝐸), a density metric 𝑔(𝑆), and 𝜖 ≥ 0 Output: A subset of vertices 𝑆𝑝 that maximizes the density metric 𝑔(𝑆) 1 𝑆0 ← 𝑉 /* Initialize the subset with all vertices */ 2 𝑖 ← 1 3 𝜏max ← 𝑔(𝑆 0 ) 𝑘(1+𝜖) /* Initialize the threshold */ 4 while 𝑆𝑖−1 ̸= ∅ do 5 𝜏 ← max{𝜏max, 𝑘(1 + 𝜖) · 𝑔(𝑆𝑖−1)} /* Peeling threshold */ 6 𝑈 ← {𝑢 | 𝑤𝑢 (𝑆𝑖−1) ≤ 𝜏 } /* Vertices to be peeled */ 7 𝑆𝑖 ← 𝑆𝑖−1 \\\\𝑈 /* Update the subset */ 8 𝜏max = max{𝜏max, 𝑔(𝑆𝑖 ) 𝑘(1+𝜖) } /* Refine the threshold */ 9 𝑖 ← 𝑖 + 1 10 return arg max𝑆𝑖 𝑔(𝑆𝑖 ) /* The subset that maximizes 𝑔(𝑆) */ Definition 5.1 (Long-Tail vertex). Given an 𝛼-𝑎𝑝𝑝𝑟𝑜𝑥𝑖𝑚𝑎𝑡𝑖𝑜𝑛 DSD algorithm and a set of vertices 𝑆 ⊆ 𝑉 , if𝑤𝑢 (𝑆) < 𝑔(𝑆∗) 𝛼 and 𝑆∗ is the optimal vertex set, then 𝑢 is a long-tail vertex. Lemma 5.1. If ∃𝑢 ∈ 𝑆𝑖 is a long-tail vertex, then 𝑆𝑖 ̸= 𝑆𝑃 . Proof. We prove it in contradiction by assuming that 𝑆𝑖 = 𝑆𝑃 . Since 𝑢 is a long-tail vertex in 𝑆𝑖 ,𝑤𝑢 (𝑆𝑖 ) < 𝑔(𝑆∗) 𝛼 < 𝑔(𝑆𝑃 ). By peeling 𝑢 from 𝑆𝑃 , we have the following. 𝑔(𝑆𝑃 \\\\ {𝑢}) = 𝑓 (𝑆𝑃 ) −𝑤𝑢 (𝑆𝑃 ) |𝑆𝑃 |−1 > 𝑓 (𝑆𝑃 ) − 𝑔(𝑆𝑃 ) |𝑆𝑃 |−1 = 𝑓 (𝑆𝑃 ) − 𝑓 (𝑆𝑃 ) |𝑆𝑃 | |𝑆𝑃 |−1 = 𝑔(𝑆𝑃 ) (14) A denser subgraph can be obtained by peeling 𝑢 from 𝑆𝑃 . This contradicts that 𝑆𝑃 is the optimal solution. Hence, 𝑆𝑖 ̸= 𝑆𝑃 . □ Identifying Long-Tail Vertices. Based on Lemma 5.2, we can also establish that a vertex𝑢 in set 𝑆 can be classified as a long-tail vertex if 𝑤𝑢 (𝑆) < 𝑔(𝑆∗) 𝑘(1+𝜖) . This insight allows us to implement a global peeling threshold, denoted as 𝜏max. During each iteration, we compare 𝑔(𝑆𝑖 ) 𝑘(1+𝜖) with 𝜏max. If the former exceeds the latter, Dupin updates 𝜏max to 𝑔(𝑆𝑖 ) 𝑘(1+𝜖) , thereby enhancing the efficiency of the peeling. Peeling with Global Threshold (Algorithm 3). Using FD as an example, we can set 𝑘 to 2. In Lemma 5.2, we recognize that long-tail vertices in set 𝑆𝑖 can be peeled directly during iteration 𝑖 for FD. To facilitate this, Dupin initiates with a global peeling threshold 𝜏max (Line 3). After identifying a denser subgraph post-peeling, 𝜏max is updated to 𝑔(𝑆𝑖 ) 2(1+𝜖) , refining the peeling process (Line 8). Example 5.1. In the initial depiction provided in the leftmost panel of Figure 6 (the 𝑖-th iteration), the density of the graph is 4.28. Traditionally, this scenario would necessitate two additional iterations to complete the peeling process, which could notably increase computational time. As illustrated, vertex 𝑢 cannot be peeled in the (𝑖 + 1)-th iteration and would typically require an extra iteration, the (𝑖 + 2)-th, for its removal. However, Dupin can promptly peel vertices that fall below this threshold by employing our algorithm’s global peeling threshold, which is calculated as half of the current density (𝜏max = 4.28/2 = 2.14). This approach enables the simultaneous peeling of three nodes in the (𝑖 + 1)-th iteration. Without this pruning strategy, an additional iteration would be essential to peel vertex 𝑢, underscoring our method’s enhanced efficiency in the peeling process. This optimization obviates the necessity for a prolonged iterative sequence. The example herein substantiates the efficacy of our tai- lored optimization for long-tail distribution, substantially enhancing the algorithm’s time efficiency.',\n",
       " '5.2 Local Peeling Optimization': 'Algorithm 4: DupinLPO: Local Peeling Optimization Input: A graph𝐺 = (𝑉 , 𝐸), a density metric 𝑔(𝑆), and 𝜖 ≥ 0 Output: A subset of vertices 𝑆𝑝 representing the densest subgraph 1 𝑆0 ← 𝑉 /* Initialize the subset with all vertices */ 2 𝜏max ← 𝑔(𝑆 0 ) 𝑘(1+𝜖) /* Initialize the threshold */ 3 while 𝑆𝑖−1 ̸= ∅ do 4 𝜏1 ← max{𝜏max, 𝑘(1 + 𝜖) · 𝑔(𝑆𝑖−1)} /* Peeling threshold */ 5 𝑈1 ← {𝑢 ∈ 𝑆 | 𝑤𝑢 (𝑆) ≤ 𝜏1 } 6 𝑆𝑖 ← 𝑆𝑖−1 \\\\𝑈1 7 while ∃𝑢 ∈ 𝑆𝑖 , 𝑤𝑢 (𝑆𝑖 ) < 𝑔(𝑆𝑖 ) do 8 𝜏2 ← max{𝜏max, 𝑔(𝑆𝑖 )} /* Update the threshold */ 9 𝑈2 ← {𝑢 | 𝑤𝑢 (𝑆𝑖 ) < 𝜏2 } 10 𝑆𝑖 ← 𝑆𝑖 \\\\𝑈2 11 𝜏max = max{𝜏max, 𝑔(𝑆𝑖 ) 𝑘(1+𝜖) } /* Update the threshold */ 12 return arg max𝑆𝑖 𝑔(𝑆𝑖 ) /* The subset that maximizes 𝑔(𝑆) */ It is important to note that each peeling iteration often results in disparate structures. This is primarily caused by the peeling of cer- tain vertices, which leaves their neighboring structures disconnected and fragmented. This not only affects the density of the detected sub- graph but also lowers the peeling threshold. Lemma 5.2 establishes a foundational property of vertices within a dense graph. Lemma 5.2. ∀𝑢 ∈ 𝑆 , if𝑤𝑢 (𝑆) < 𝑔(𝑆), 𝑔(𝑆 \\\\ {𝑢}) > 𝑔(𝑆). Proof. By peeling 𝑢 from 𝑆 , we have the following. 𝑔(𝑆 \\\\ {𝑢}) = 𝑓 (𝑆) −𝑤𝑢 (𝑆) |𝑆 |−1 > 𝑓 (𝑆) − 𝑔(𝑆𝑖 ) |𝑆 |−1 = 𝑓 (𝑆) − 𝑔(𝑆) |𝑆∗ |−1 = 𝑓 (𝑆) − 𝑓 (𝑆) |𝑆 | |𝑆 |−1 = 𝑔(𝑆) (15) Therefore, a denser graph can be obtained by peeling 𝑢 from 𝑆 . □ According to Lemma 5.2, we introduce local peeling optimization within the iteration. After each peeling iteration, if the peeling weight of a vertex is smaller than the current density, Dupin will trim it. We implement a local peeling optimization (Lines 7-10, Algorithm 4) before the next iteration. After a vertex 𝑢 is peeled, Dupin invokes updateNgh to update the peeling weights of 𝑢’s neighboring nodes. If any of these neighbors have a peeling weight less than the current density, they are also peeled, thereby increasing the density of the current iteration. Consequently, when an iteration yields globally optimal results, a denser structure is obtained. The benefits of this approach extend beyond density improvements; performance is also enhanced. By trimming these vertices, we reduce the number of vertices to traverse in subsequent iterations, and a higher density enables more efficient iterations due to the possibility of using larger peeling thresholds. Lemma 5.3. If 𝑢𝑖 ∈ 𝑆𝑝 is the first vertex in 𝑆∗ removed by Dupin, 𝑢𝑖 will not be removed during the local peeling optimization process. Proof. Assume, for the sake of contradiction, that 𝑢𝑖 is trimmed during the local peeling optimization process. Let 𝑆 ′ be the set of vertices left before 𝑢𝑖 is trimmed. By definition, we have: 𝑤𝑢𝑖 (𝑆 ′ ) < 𝑔(𝑆∗). Since 𝑢𝑖 is the first vertex in 𝑆∗ to be removed, 𝑆∗ ⊆ 𝑆 ′, therefore: 𝑤𝑢𝑖 (𝑆 ∗ ) ≤ 𝑤𝑢𝑖 (𝑆 ′ ) < 𝑔(𝑆∗). This contradicts Lemma 5.2, which states that ∀𝑢𝑖 ∈ 𝑆∗: 𝑤𝑢𝑖 (𝑆 ∗ ) ≥ 𝑔(𝑆∗). Otherwise, a better solution can be obtained by removing 𝑢 from 𝑆∗, which contradicts the fact that 𝑆∗ is the optimal solution. Thus, 𝑢𝑖 cannot be trimmed. Hence,𝑢𝑖 will not be removed during the local peeling optimization process. □ Due to Lemma 5.3, 𝑢𝑖 will only be removed during the peeling process. Hence, the approximation ratios (Theorem 4.2) for different density metrics are preserved. 6 EXPERIMENTAL STUDY Our experiments are run on a machine that has an X5650 CPU, 512 GB RAM. The implementation is made memory-resident and implemented in C++. All codes are compiled by GCC-9.3.0 with -𝑂3. Datasets. We report the datasets used in our experiment in Table 3. One industrial dataset is from Anon (gfg). Datasets links-anon(la) and Twitter-rv (rv)were obtained from Stanford NetworkDataset Collection [31]. Datasets kron-logn21(kron), soc-twitter(soc), Web-uk-2005(uk) and Web-sk-2005(sk) were obtained from Network Data Repository [38]. Dataset bio was from the BIOMINE [35]. Baseline Methods. In this section, we introduce five commonly used density metrics pivotal in analyzing network structures:DG [6], DW [23], FD [24], TDS [46], and𝑘CLiDS [14].We implemented these metrics in our framework, Dupin, and compared their performance with frameworks including Spade [28], kCLIST [14], GBBS [16], PBBS [41], PKMC [32], FWA [15] and ALENEX [44]. kCLIST [14] and PBBS [41] are parallel 𝑘CLiDS algorithms, while GBBS [16] supports the DG density metric. Although GBBS does not support weighted graphs, we precompute the peeling weights offline and then import them into GBBS. The time spent on this offline computation is not included in our reported runtime. FWA, ALENEX and PKMC support the DG density metric. For clarity, we denote the incremental peeling process, which involves the iterative removal of graph elements based on density scores, as Spade-X, where X corresponds to each metric, under the same experimental condi- tions described in [28], where the batch size is set to 1K, consistent with the default setting in [28]. We report the average runtime per batch for Spade. DupinGPO-X and DupinLPO-X represent our par- allel peeling algorithms that incorporate global and local peeling optimization. Default Settings. In our experiments, the parameter 𝜖 is set to 0.1 by default, and the number of threads 𝑡 is configured to 128. These settings are used consistently unless otherwise specified. 6.1 Efficiency of Dupin Spade vs Dupin. Our initial comparison focuses on the performance disparity between the incremental and parallel peeling algorithms. Specifically, Dupin-DG (resp. Dupin-DW, Dupin-FD, Dupin-TDS, andDupin-𝑘CLiDS) demonstrates a speedup factor of 6.97 (resp. 7.71, 7.71, 7.65, and 8.46) over Spade-DG (resp. Spade-DW, Spade-FD, Spade-TDS, and Spade-𝑘CLiDS). Notably, Spade-TDS and Spade𝑘CLiDS only complete computations within 7,200 seconds on gfg. The bottleneck for Spade arises in larger graphs where initial calculations of triangle and 𝑘-Clique counting are unable to finish. Dupin outpaces Spade primarily because Spade suffers from frequent re- ordering of peeling sequences when there are many increments, which proves to be slower than recalculating. GBBS vs Dupin. Previous works have not provided simultaneous support for all density metrics. To bridge this gap, we implemented DG, DW, and FD within the parallel GBBS framework. On average, Dupin-DG, Dupin-DW, and Dupin-FD demonstrated speedup factors of 6.33, 12.51, and 17.07 times, respectively, over GBBS-DG, GBBS-DW, andGBBS-FD. This performance advantage is attributed to the fundamental difference in the peeling process between the two frameworks. In GBBS, the basic unit of each peel is a bucket, where all nodes within the same bucket have an identical peeling weight. This design limits the parallelism in weighted graphs like those using DW and FD, where many buckets may contain only a single node. In contrast, Dupin peels batches of nodes at each step, achieving higher parallelism and thus consistently outperforming GBBS, particularly in weighted graph scenarios where the speedup is even more pronounced. kCLIST vs PBBS vs Dupin. Both kCLIST and PBBS support the density metrics TDS and 𝑘CLiDS. In our comparisons, Dupin demonstrates notable performance advantages. Specifically, Dupin-TDS is faster than kCLIST-TDS and PBBS-TDS by 39.85 and 62.71 times, respectively. Furthermore, Dupin-𝑘CLiDS also outperforms kCLIST𝑘CLiDS by 4.16 times. A significant finding is that PBBS-𝑘CLiDS fails to complete computations within 7200 seconds on most datasets, highlighting the efficiency of Dupin in handling more complex met- rics under stringent time constraints. Impact of DupinGPO. Next, we evaluate the acceleration effect of DupinGPO, using Dupin as a baseline for comparison. Notably, DupinGPO-DG is found to be 1.14 times faster, DupinGPO-DW is 1.11 times faster,DupinGPO-TDS is 1.10 times faster, andDupinGPO𝑘CLiDS is 1.35 times faster than their respective Dupin implementations. The acceleration is particularly significant for 𝑘CLiDS on larger datasets, such as soc, rv, kron, and sk. DupinGPO-𝑘CLiDS is 1.40, 1.54, 1.51, and 1.61 times faster than Dupin-𝑘CLiDS on soc, rv, kron, and sk, respectively. This is attributed to the greater number of iterations required due to the larger dataset sizes and the higher count of 𝑘-Cliques, necessitating more peeling iterations. In such scenarios,DupinGPO effectively prunes the tail iterations early, thereby reducing the number of rounds needed. Impact of DupinLPO. We next compared the efficiency of Dupin andDupinLPO. On average, thoughDupinLPO-DGwas 10.09% slower than Dupin-DG, the speed is still comparable. For example, on the soc dataset, DupinLPO-DG was 5.94% faster than Dupin-DG. This is because DupinLPO requires some lock operations to update the peeling weights of neighboring vertices during local peeling optimization. However, the advantage is that DupinLPO can detect up to 26.26% denser subgraphs, as detailed in Section 6.2. On larger datasets, such as soc, rv, and la, the vertices being trimmed have relatively low correlations with each other, so the impact of locks is minimal. For example, on the soc (resp. rv), DupinLPO-DG was 5.94% (resp. 9.94%) faster than Dupin-DG. In contrast, on smaller datasets, such as bio and kron, the extra cost of locks is higher. Impact of the Number of Threads 𝑡 . All methods are influenced by the concurrency level, i.e., the number of threads. Generally, the greater the number of threads, the faster the execution speed. In our experiments, using the la dataset as a benchmark, we varied the number of threads 𝑡 from 2 to 128 to observe the impact on runtime performance. For GBBS-DG, runtime stabilizes and accelerates by 3.33 times as thread count increases from 2 to 8. For GBBS-DW, performance deteriorates when the thread count exceeds 32, but from 2 to 32 threads, it accelerates by 3.50 times. Similarly, GBBS-FD shows no significant change in runtime beyond 32 threads, with an acceleration of 3.09 times from 2 to 32 threads. GBBS-TDS and GBBS-𝑘CLiDS fail to complete computations on the la dataset. In contrast, all five methods tested onDupin exhibit good scalability. As the number of threads increases from 2 to 128,Dupin-DG accelerates by 9.91 times, Dupin-DW by 12.69 times, Dupin-FD by 13.03 times, and Dupin-TDS by 28.06 times. Although Dupin-𝑘CLiDS fails to produce results when the thread count is below 16, it accelerates by 1.32 times from 32 to 128 threads. These results demonstrate Dupin’s strong scalability across varied threading levels. 6.2 Effectiveness of Dupin In addition to performance speedup, we evaluate the density of the subgraphs detected by different frameworks. Detailed density comparisons are presented in Table 5. GBBS vs Spade vs Dupin. Firstly, we compare the densities of the subgraphs identified by GBBS-DG (respectively GBBS-DW, and GBBS-FD) to those detected by Dupin in parallel execution. Our experimental results indicate that Dupin maintains comparable subgraph densities with GBBS, signifying that the increase in com- putational efficiency does not compromise the accuracy of the de- tected subgraphs. Specifically, the density of the subgraph detected byGBBS-DG (respectivelyGBBS-DW, andGBBS-FD) is 7.08% (resp. 6.48% and 7.43%) greater than that detected by Dupin-DG (respectively Dupin-DW, and Dupin-FD) on average. Spade shares similar densities with GBBS. Due to Spade’s tendency to accumulate er- rors over time, its density can become inflated. We will explain its limitations in practical applications in more detail in the case study. kCLIST vs PBBS vs Dupin. a) For 𝑘CLiDS, PBBS-𝑘CLiDS could not complete on most of our test datasets, so we did not compare their densities. kCLIST-𝑘CLiDS could not complete on the sk dataset. On the other datasets, Dupin-𝑘CLiDS’s density was only 6.97% smaller than kCLIST-𝑘CLiDS. On some datasets, such as bio and kron,Dupin𝑘CLiDS even detected subgraphs with greater density. b) For TDS, PBBS-TDS could detect subgraphs on smaller graphs but failed on billion-scale graphs such as rv, sk, and la. Dupin-TDS’s density was 2.03% greater than kCLIST-TDS. Therefore, we conclude that Dupin is comparable to existing parallel methods in terms of the density of the detected dense subgraphs. Ablation Study of Dupin. This experiment delves into the effectiveness of our optimizations, DupinGPO and DupinLPO. Our analysis primarily focuses on comparing the densities of dense subgraphs detected by both DupinGPO and DupinLPO in various settings. DupinGPO achieves the same density as Dupin across five den- sity metrics because it trims the long tail, enhancing efficiency. For DupinLPO-DG (resp. DupinLPO-DW, DupinLPO-FD, DupinLPOTDS, and DupinGPO-𝑘CLiDS), we observe that the detected sub- graphs exhibit densities 11.09% (resp. 7.32%, 8.00%, 1.01%, and 26.26%) greater than those detected by Dupin-DG, Dupin-DW, Dupin-FD, Dupin-TDS, andDupin-𝑘CLiDS, respectively, demonstrating notable outcomes. This is because DupinLPO trims the graph in each iteration, resulting in denser subgraphs compared to Dupin. Impact of the Approximation Ratio 𝜖. The approximation ratio 𝜖 serves to control accuracy. In our experiment, we vary 𝜖 from 0.1 to 1 to investigate its influence on the accuracy of Dupin. We observe that as 𝜖 increases from 0.1 to 1, the density of subgraphs detected by Dupin, DupinGPO, and DupinLPO generally decreases. ForDG, the densities ofDupin-DG,DupinGPO-DG, andDupinLPODG decrease by 22.71%, 22.71%, and 11.99%, respectively. The trends for DW and DG are similar, with the worst performance observed at 𝜖 = 0.6 for both density metrics. The trends for FD, TDS, and 𝑘CLiDS are also similar. DupinLPO detects denser subgraphs across most density metrics and is less affected by 𝜖 , due to its iterative trimming. For example, for FD, DupinLPO’s density decreases by only 6.23%, while DupinGPO and Dupin’s densities decrease by 17.37%.',\n",
       " '6.3 Case study': 'We also compare the performance in real-world scenarios, focusing on three key aspects: accuracy, latency, and prevention ratio. Accuracy. As discussed in Section 1, although Spade can quickly respond to new transactions, it tends to accumulate errors over time. Our analysis utilizes transaction data from Anon, with the transaction network comprising |𝐸 |= 2 billion edges. Under the incremental computation framework of Spade, the assumption is made that the weights of existing edges remain constant, thereby neglecting the impact of newly inserted edges on these weights, leading to error accumulation. For instance, using FD density metric, we observe that errors accumulate up to 24.4% within a day and increase to 30.3% over a week, as illustrated in Figure 9. Although Dupin sacrifices some accuracy to achieve parallel computational efficiency, our experiments show that Dupin’s errors remain relatively stable between 3% and 5%. Hence, the detection precision of Spade declines from 87.9% to 68.3%, while Dupin’s precision remains above 86%. Latency and Prevention Ratio. If a transaction is identified as fraudulent, subsequent related transactions are blocked to mitigate potential losses. We define the ratio of successfully prevented sus- picious transactions to the total number of suspicious transactions as R. As shown in Figure 6, the prevention ratio R continues to decrease as latency increases on Anon’s datasets. Using the default density metric, FD, in Anon, our results show thatDupin-FD can prevent 94.5% of fraudulent activities, GBBS-FD can prevent 3.0%, and Spade-FD can prevent 45.3%. The reason for the lower performance of GBBS-FD is its low parallelism in the peeling process, which results in long peeling times and thus an average latency of 6,014 seconds, delaying the prevention of many fraudulent activities. Although Spade-FD performs well in reordering techniques on smaller datasets, in industrial scenarios with billion-scale graphs, latency significantly worsens due to the extensive reordering range required in the peeling sequences as normal users transition to fraudsters, increasing the cost of incremental computations. Additional tests were conducted with other density metrics; for DG, Dupin-DG can prevent 78.8%, while other methods prevent varying amounts. Similarly, for DW, Dupin-DW can prevent 86.1%, while other methods demonstrate different prevention capabilities.',\n",
       " '7 CONCLUSION AND FUTUREWORKS': 'In this paper, we presented Dupin, a novel parallel fraud detection framework tailored for massive graphs. Our experiments demonstrated that Dupin’s advanced peeling algorithms and long-tail pruning techniques, DupinGPO and DupinLPO, significantly enhance the detection efficiency of dense subgraphs without sacrificing accuracy. Comparative studies with GBBS, PBBS, kCLIST, Spade, and Dupin highlighted Dupin’s superior performance in terms of com- putational efficiency and accuracy. Through case studies, we also validated that Dupin achieves lower latency and higher detection ac- curacy compared to existing fraud detection systems on billion-scale graphs. Overall, Dupinproves to be a powerful tool for large-scale fraud detection, providing a scalable and efficient solution for real-time applications. Future work will explore the integration of additional suspiciousness functions and further optimization to handle even larger datasets and more complex fraud scenarios.',\n",
       " 'REFERENCES': '[1] Distil networks: The 2019 bad bot report. https://www.bluecubesecurity.com/wp- content/uploads/bad-bot-report-2019LR.pdf. [2] B. Bahmani, R. Kumar, and S. Vassilvitskii. Densest subgraph in streaming and mapreduce. Proceedings of the VLDB Endowment, 5(5), 2012. [3] Y. Ban, X. Liu, T. Zhang, L. Huang, Y. Duan, X. Liu, and W. Xu. Badlink: Combining graph and information-theoretical features for online fraud group detection. arXiv preprint arXiv:1805.10053, 2018. [4] A. Beutel, W. Xu, V. Guruswami, C. Palow, and C. Faloutsos. Copycatch: stopping group attacks by spotting lockstep behavior in social networks. In Proceedings of the 22nd international conference on World Wide Web, pages 119–130, 2013. [5] D. Boob, Y. Gao, R. Peng, S. Sawlani, C. Tsourakakis, D.Wang, and J.Wang. Flowless: Extracting densest subgraphs without flow computations. In Proceedings of The Web Conference 2020, pages 573–583, 2020. [6] M. Charikar. Greedy approximation algorithms for finding dense components in a graph. In International Workshop on Approximation Algorithms for Combinatorial Optimization, pages 84–95. Springer, 2000. [7] M. Charikar. Greedy approximation algorithms for finding dense components in a graph. In Approximation Algorithms for Combinatorial Optimization, Third International Workshop, APPROX 2000, Saarbrücken, Germany, September 5-8, 2000, Proceedings, volume 1913 of Lecture Notes in Computer Science, pages 84–95. Springer, 2000. [8] C. Chekuri, K. Quanrud, and M. R. Torres. Densest subgraph: Supermodularity, iterative peeling, and flow. In Proceedings of the 2022 Annual ACM-SIAM Symposium on Discrete Algorithms (SODA), pages 1531–1555. SIAM, 2022. [9] J. Chen and Y. Saad. Dense subgraph extraction with application to community detection. IEEE Transactions on knowledge and data engineering, 24(7):1216–1230, 2010. [10] Y. Chen, J. Jiang, S. Sun, B. He, and M. Chen. Rush: Real-time burst subgraph detection in dynamic graphs. Proceedings of the VLDB Endowment, 17(11):3657– 3665, 2024. [11] N. Chiba and T. Nishizeki. Arboricity and subgraph listing algorithms. SIAM Journal on computing, 14(1):210–223, 1985. [12] L. Chu, Y. Zhang, Y. Yang, L. Wang, and J. Pei. Online density bursting subgraph detection from temporal graphs. Proceedings of the VLDB Endowment, 12(13):2353– 2365, 2019. [13] L. Dagum and R. Menon. Openmp: an industry standard api for shared-memory programming. IEEE computational science and engineering, 5(1):46–55, 1998. [14] M. Danisch, O. Balalau, and M. Sozio. Listing k-cliques in sparse real-world graphs. In Proceedings of the 2018 World Wide Web Conference, pages 589–598, 2018. [15] M. Danisch, T.-H. H. Chan, and M. Sozio. Large scale density-friendly graph decomposition via convex programming. In Proceedings of the 26th International Conference on World Wide Web, pages 233–242, 2017. [16] L. Dhulipala, J. Shi, T. Tseng, G. E. Blelloch, and J. Shun. The graph based benchmark suite (gbbs). In Proceedings of the 3rd Joint International Workshop on Graph Data Management Experiences & Systems (GRADES) and Network Data Analytics (NDA), pages 1–8, 2020. [17] Y. Dourisboure, F. Geraci, and M. Pellegrini. Extraction and classification of dense communities in the web. In Proceedings of the 16th international conference on World Wide Web, pages 461–470, 2007. [18] A. Epasto, S. Lattanzi, and M. Sozio. Efficient densest subgraph computation in evolving graphs. In Proceedings of the 24th international conference on world wide web, pages 300–310, 2015. [19] D. Gibson, R. Kumar, and A. Tomkins. Discovering large dense subgraphs in massive graphs. In Proceedings of the 31st international conference on Very large data bases, pages 721–732. Citeseer, 2005. [20] A. Gionis and C. E. Tsourakakis. Dense subgraph discovery: Kdd 2015 tutorial. In Proceedings of the 21th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, pages 2313–2314, 2015. [21] A. V. Goldberg. Finding a maximum density subgraph. 1984. [22] A. V. Goldberg and R. E. Tarjan. A new approach to the maximum-flow problem. Journal of the ACM (JACM), pages 921–940, 1988. [23] N. V. Gudapati, E. Malaguti, and M. Monaci. In search of dense subgraphs: How good is greedy peeling? Networks, 77(4):572–586, 2021. [24] B. Hooi, H. A. Song, A. Beutel, N. Shah, K. Shin, and C. Faloutsos. Fraudar: Bounding graph fraud in the face of camouflage. In Proceedings of the 22nd ACM SIGKDD international conference on knowledge discovery and data mining, pages 895–904, 2016. [25] M. Jaggi. Revisiting frank-wolfe: Projection-free sparse convex optimization. In International conference on machine learning, pages 427–435. PMLR, 2013. [26] J. Jiang, Y. Chen, B. He, M. Chen, and J. Chen. Spade+: A generic real-time fraud detection framework on dynamic graphs. IEEE Transactions on Knowledge and Data Engineering, 2024. [27] M. Jiang, P. Cui, A. Beutel, C. Faloutsos, and S. Yang. Inferring strange behavior from connectivity pattern in social networks. In Pacific-Asia conference on knowledge discovery and data mining, pages 126–138. Springer, 2014. [28] J. Jiaxin, L. Yuan, H. Bingsheng, H. Bryan, C. Jia, and J. K. Z. Kang. A real-time fraud detection framework on evolving graphs. PVLDB, 2023. [29] S. Kumar, W. L. Hamilton, J. Leskovec, and D. Jurafsky. Community interaction and conflict on the web. In Proceedings of the 2018 world wide web conference, pages 933–943, 2018. [30] V. E. Lee, N. Ruan, R. Jin, and C. Aggarwal. A survey of algorithms for dense subgraph discovery. In Managing and mining graph data, pages 303–336. Springer, 2010. [31] J. Leskovec and A. Krevl. SNAP Datasets: Stanford large network dataset collection. http://snap.stanford.edu/data, June 2014. [32] W. Luo, Z. Tang, Y. Fang, C. Ma, and X. Zhou. Scalable algorithms for densest subgraph discovery. In 2023 IEEE 39th International Conference on Data Engineering (ICDE), pages 287–300. IEEE, 2023. [33] C. Ma, Y. Fang, R. Cheng, L. V. Lakshmanan, and X. Han. A convex-programming approach for efficient directed densest subgraph discovery. In Proceedings of the 2022 International Conference on Management of Data, pages 845–859, 2022. [34] M. Mitzenmacher, J. Pachocki, R. Peng, C. Tsourakakis, and S. C. Xu. Scalable large near-clique detection in large-scale networks via sampling. In Proceedings of the 21th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, pages 815–824, 2015. [35] V. Podpečan, Ž. Ramšak, K. Gruden, H. Toivonen, and N. Lavrač. Interactive explo- ration of heterogeneous biological networks with biomine explorer. Bioinformatics, 06 2019. [36] H. Qin, R.-H. Li, Y. Yuan, G. Wang, L. Qin, and Z. Zhang. Mining bursting core in large temporal graphs. Proceedings of the VLDB Endowment, 2022. [37] Y. Ren, H. Zhu, J. Zhang, P. Dai, and L. Bo. Ensemfdet: An ensemble approach to fraud detection based on bipartite graph. In 2021 IEEE 37th International Conference on Data Engineering (ICDE), pages 2039–2044. IEEE, 2021. [38] R. A. Rossi and N. K. Ahmed. The network data repository with interactive graph analytics and visualization. In AAAI, 2015. [39] A. E. Sarıyüce andA. Pinar. Peeling bipartite networks for dense subgraph discovery. In Proceedings of the Eleventh ACM International Conference on Web Search and Data Mining, pages 504–512, 2018. [40] S. B. Seidman. Network structure and minimum degree. Social networks, 5(3):269– 287, 1983. [41] J. Shi, L. Dhulipala, and J. Shun. Parallel clique counting and peeling algorithms. In SIAM Conference on Applied and Computational Discrete Algorithms (ACDA21), pages 135–146. SIAM, 2021. [42] K. Shin, T. Eliassi-Rad, and C. Faloutsos. Corescope: Graph mining using k-core analysis—patterns, anomalies and algorithms. In 2016 IEEE 16th international conference on data mining (ICDM), pages 469–478. IEEE, 2016. [43] K. Shin, B. Hooi, J. Kim, and C. Faloutsos. Densealert: Incremental dense-subtensor detection in tensor streams. In Proceedings of the 23rd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, pages 1057–1066, 2017. [44] P. Sukprasert, Q. C. Liu, L. Dhulipala, and J. Shun. Practical parallel algorithms for near-optimal densest subgraphs on massive graphs. In 2024 Proceedings of the Symposium on Algorithm Engineering and Experiments (ALENEX), pages 59–73. SIAM, 2024. [45] B. Sun, M. Danisch, T. H. Chan, and M. Sozio. Kclist++: A simple algorithm for finding k-clique densest subgraphs in large graphs. Proceedings of the VLDB Endowment (PVLDB), 2020. [46] C. Tsourakakis. The k-clique densest subgraph problem. In Proceedings of the 24th international conference on world wide web, pages 1122–1132, 2015. [47] C. Ye, Y. Li, B. He, Z. Li, and J. Sun. Gpu-accelerated graph label propagation for real-time fraud detection. In Proceedings of the 2021 International Conference on Management of Data, pages 2348–2356, 2021.'}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "parse_document(full_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_document(text):\n",
    "    # Initialize variables\n",
    "    sections = {}\n",
    "    current_section = None\n",
    "    current_content = []\n",
    "\n",
    "    # Split text into lines\n",
    "    lines = text.split('\\n')\n",
    "\n",
    "    # Define heading patterns\n",
    "    numbered_heading_pattern = re.compile(r'^(\\d+(?:\\.\\d+)*)\\s+(.*)$')\n",
    "    # Known headings\n",
    "    known_headings = {'Abstract', 'Acknowledgements', 'References', 'Conclusion', 'Introduction', 'Background', 'Related Work', 'Methodology', 'Results', 'Discussion', 'Appendix'}\n",
    "    known_headings_lower = [h.lower() for h in known_headings]\n",
    "\n",
    "    for line in lines:\n",
    "        line = line.strip()\n",
    "        if not line:\n",
    "            continue  # Skip empty lines\n",
    "\n",
    "        # Check for numbered heading\n",
    "        m = numbered_heading_pattern.match(line)\n",
    "        if m:\n",
    "            # Save current section\n",
    "            if current_section is not None:\n",
    "                content = ' '.join(current_content).strip()\n",
    "                # Save the section even if content is empty\n",
    "                sections[current_section] = content\n",
    "            # Start new section\n",
    "            section_number = m.group(1)\n",
    "            section_title = m.group(2).strip()\n",
    "            current_section = f\"{section_number} {section_title}\"\n",
    "            current_content = []\n",
    "            continue\n",
    "\n",
    "        # Check for unnumbered heading (known headings)\n",
    "        if line.lower() in known_headings_lower:\n",
    "            # Save current section\n",
    "            if current_section is not None:\n",
    "                content = ' '.join(current_content).strip()\n",
    "                sections[current_section] = content\n",
    "            # Start new section\n",
    "            current_section = line.strip()\n",
    "            current_content = []\n",
    "            continue\n",
    "\n",
    "        # Append line to current content\n",
    "        if current_section is None:\n",
    "            # If no current section, start with 'Abstract'\n",
    "            current_section = 'Abstract'\n",
    "        current_content.append(line)\n",
    "\n",
    "    # Save the last section\n",
    "    if current_section is not None:\n",
    "        content = ' '.join(current_content).strip()\n",
    "        sections[current_section] = content\n",
    "    return sections\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('data/dupin_full_text.txt', 'r', encoding='utf-8') as f:\n",
    "    text = f.read()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Abstract': 'ABSTRACT Detecting fraudulent activities in financial and e-commerce transac- tion networks is crucial, and one effective method for this is Densest Subgraph Discovery (DSD). However, deploying DSD methods in production systems faces substantial scalability challenges due to the predominantly sequential nature of existing methods, which impedes their ability to handle large-scale transaction networks and results in significant detection delays. In this paper, we introduce Dupin, a novel parallel processing framework designed for efficient DSD processing in billion-scale graphs. Dupin is powered by a processing engine that exploits the unique properties of the peeling process, with theoretical guarantees on detection quality and efficiency.Dupin provides user-friendly APIs for flexible customization of DSD objectives and ensures robust adaptability to diverse fraud detection scenarios. Empirical evaluations indicate that Dupin outperforms several existing DSD methods, with performance improvements observed in specific scenarios reaching up to two orders of magnitude. On billion-scale graphs, Dupin demonstrates the potential to enhance the prevention of fraudulent transactions by approximately 49.5 basis points and reduce density error from 30% to below 5%, as supported by our experimental results. ACM Reference Format: Anonymous Author(s). 2024. Dupin: A Parallel Densest Subgraph Discovery Framework on Massive Graphs. In Proceedings of ACM Conference (Conference’17). ACM, New York, NY, USA, 14 pages. https://doi.org/10.1145/ nnnnnnn.nnnnnnn',\n",
       " '1 INTRODUCTION': 'Graph structures pervade numerous contemporary applications, ranging from transaction and communication networks to expan- sive social media platforms. The study of densest subgraph discovery (DSD), first explored by [21], has since proven instrumental in various domains: link spam identification [4, 19], community de- tection [9, 17], and notably, fraud detection [8, 24, 42]. Central to this domain is the peeling process [2, 5, 8, 24, 46], which operates by iteratively removing vertices based on density metrics, such as vertex degree or the cumulative weight of adjacent edges. Their widespread adoption can be attributed to their inherent efficiency, robustness, and proven worst-case performance guarantees for DSD. In practice, business requirements demand the detection of fraud- ulent communities on billion-scale graphs within seconds. Recent Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for components of this work owned by others than ACM must be honored. Abstracting with credit is permitted. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. Request permissions from permissions@acm.org. Conference’17, July 2017, Washington, DC, USA © 2024 Association for Computing Machinery. ACM ISBN 978-x-xxxx-xxxx-x/YY/MM. . . $15.00 https://doi.org/10.1145/nnnnnnn.nnnnnnn studies, including those by [1] and [47], have highlighted the perva- sive issue of malicious bot activity in e-commerce, accounting for a substantial 21.4% of all traffic in 2018. These fraudulent activities often happen quickly, making them hard to detect. The inability to efficiently process and analyze such massive graphs not only ham- pers fraud detection efforts but also exposes businesses to substan- tial financial risks and reputational damage. Therefore, addressing these challenges is not merely an academic exercise; it is a press- ing necessity for enhancing the integrity of digital transactions and safeguarding consumer trust. Despite extensive research onDSD for static graphs [20–22, 30, 33, 34], dynamic graphs [2, 18, 26, 28, 43], and parallel approaches [14, 16, 41, 42], existing methods face significant challenges for practical de- ployment in real-world fraud detection systems. For instance, many existing algorithms primarily rely on static score functions for DSD, lacking the flexibility required for dynamic and evolving fraud detec- tion scenarios. Methods such as [4] and [27] are designed with fixed detection criteria, which can become less effective as fraudulent be- haviors adapt over time. Additionally, incremental algorithms remain inefficient at handling billion-scale graphs with high transaction volumes, leading to increased latency. Parallel DSD frameworks often lack efficient pruning techniques, resulting in excessive iterations that degrade performance on large-scale graphs. This paper addresses the following research question: How can we enhance the efficiency and scalability of DSD methods to effectively detect fraud in massive graphs? Challenges. We evaluated existing methods with our industry partner, Anon (anonymized for review). The following issues surfaced: Firstly, existing algorithms focus on static score functions for DSD and lack flexible customization. While this may be adequate for short-term detection, it becomes less effective over time. For example, as shown in Figure 1a, graph density can accumulate sig- nificant errors within a single day, highlighting the need for more adaptive and customizable detection methods. Secondly, existing incremental algorithms are still inefficient at handling billion-scale graphs with high transaction volumes, leading to increased latency. This limitation is evident in Figure 1b, which illustrates the system load distribution for a specific day’s transactions, underscoring the inability of current methods to efficiently manage high transaction volumes without significant delays. Thirdly, existing parallel densest subgraph discovery frameworks lack efficient pruning techniques, re- sulting in a large number of iterations. This inefficiency is especially problematic on billion-scale graphs, where performance degrades substantially due to the excessive computational overhead caused by numerous iterations. Adapting to Evolving Research Needs. Recognizing the dynamic nature of fraud detection and the rapid evolution of fraudulent tactics, we have adapted our research focus to address these challenges. Our approach incorporates flexible customization options for DSD methods, allowing for adjustments based on the changing landscape of fraudulent activities. This adaptability is crucial for maintaining the effectiveness of fraud detection systems in an environment where new threats continuously emerge. Contributions. In this work, we propose a DSD framework for fraud detection. Our key contributions are as follows: • A DSD Parallelization Framework. We propose Dupin, a novel parallel processing framework for densest subgraph discovery (DSD) that breaks the sequential dependencies inherent in tra- ditional density metrics, significantly enhancing scalability and efficiency on both weighted and unweighted graphs. Unlike existing methods that focus on specific density metrics, Dupin allows developers to define custom fraud detection semantics through flexible suspiciousness functions for edges and vertices. It supports a wide range of DSD semantics, including DG [6], DW [23], FD [24], TDS [46], and 𝑘CLiDS [14]. Importantly, Dupin not only leverages parallelism but also provides theoretical guarantees on approximation ratios for the supported density metrics. To the best of our knowledge, this is the first generic parallel processing framework that accommodates a broad class of density metrics in DSD, offering both flexibility and theoretical assurance. • Exploiting Long-Tail Properties.We identify and address the long-tail problem in the peeling process of DSD algorithms, where late iterations remove only a small fraction of vertices and con- tribute little to the quality of the densest subgraph, leading to inefficiency. To tackle this, we introduce two novel optimizations: Global Pruning Optimization, which proactively removes low-contribution vertices within each peeling iteration to reduce computational overhead, and Local Pruning Optimization, which eliminates disparate structures formed after vertex removal to enhance the density and quality of the detected subgraph. These optimizations significantly reduce the number of iterations re- quired, improving both performance and result quality. • Empirical Validation and Real-World Impact: We conduct comprehensive experiments on large-scale industry datasets to validate the effectiveness of Dupin. Our results show that Dupin achieves up to 100 times faster fraud detection compared to the state-of-the-art incremental method Spade [28] and parallel methods like PBBS [41]. Moreover,Dupin is capable of preventing up to 94.5% of potential fraud, demonstrating significant practical bene- fits and reinforcing system integrity in real-world applications. Organization. The remainder of this paper is structured as follows: Section 2 presents the background and the literature review. Section 3 introduces the architecture of Dupin. In Section 4, we propose the theoretical foundations for parallel peeling-based algorithms for DSD. Section 5 introduces long-tail pruning techniques to improve efficiency and effectiveness. The experimental evaluation of Dupin is presented in Section 6. Finally, Section 7 concludes the paper and outlines avenues for future research.',\n",
       " '2 BACKGROUND AND RELATEDWORK': '',\n",
       " '2.1 Preliminary': 'This subsection presents the preliminary. Some frequently used no- tations are summarized in Table 1. Graph 𝐺 . In this work, we operate on a weighted graph denoted as 𝐺 = (𝑉 , 𝐸), where 𝑉 represents the set of vertices and 𝐸 (⊆ 𝑉 ×𝑉 ) represents the set of edges. Each edge (𝑢𝑖 , 𝑢 𝑗 ) ∈ 𝐸 is assigned a nonnegative weight, expressed as 𝑐𝑖 𝑗 . Additionally, 𝑁 (𝑢) denotes the set of neighboring vertices connected to vertex 𝑢. Induced Subgraph. For a given subset 𝑆 of vertices in 𝑉 , we define the induced subgraph as 𝐺[𝑆] = (𝑆, 𝐸[𝑆]), where 𝐸[𝑆] = {(𝑢, 𝑣) | (𝑢, 𝑣) ∈ 𝐸 ∧ 𝑢, 𝑣 ∈ 𝑆}. The size of the subset 𝑆 is denoted by |𝑆 |. Density Metrics 𝑔 and Weight Function 𝑓 . We utilize the class of metrics𝑔 as defined in prior works [6, 24, 28], where for a given subset of vertices 𝑆 ⊆ 𝑉 , the density metric is computed as: 𝑔(𝑆) = 𝑓 (𝑆) |𝑆 | . Here, 𝑓 (𝑆) represents the total weight of the induced subgraph 𝐺[𝑆], which is defined as the sum of the weights of vertices in 𝑆 and the weights of edges in 𝐸[𝑆]. More formally: 𝑓 (𝑆) = ∑︁ 𝑢𝑖 ∈𝑆 𝑎𝑖 + ∑︁ (𝑢𝑖 ,𝑢 𝑗 )∈𝐸[𝑆] 𝑐𝑖 𝑗 (1) The weight of a vertex 𝑢𝑖 , denoted as 𝑎𝑖 (𝑎𝑖 ≥ 0), quantifies the suspiciousness of user 𝑢𝑖 . The weight of an edge (𝑢𝑖 , 𝑢 𝑗 ), represented by 𝑐𝑖 𝑗 (𝑐𝑖 𝑗 > 0), reflects the suspiciousness of the transaction between users 𝑢𝑖 and 𝑢 𝑗 . Intuitively, the density metric 𝑔(𝑆) encapsulates the overall suspiciousness density of the induced subgraph𝐺[𝑆]. A larger value of 𝑔(𝑆) indicates a higher density of suspiciousness within𝐺[𝑆]. The vertex set that maximizes the density metric 𝑔 is denoted by 𝑆∗. We next introduce five representative density metrics extensively employed in many fraud detection applications. The density metrics are mainly differed by how the weight function is defined. Dense Subgraphs (DG) [6]. The DG metric is employed to quan- tify the connectivity of substructures within a graph. It has found extensive applications in various domains, such as detecting fake comments in online platforms [29] and identifying fraudulent activities in social networks [3]. Given a subset of vertices 𝑆 ⊆ 𝑉 , the weight function of DG is defined as 𝑓 (𝑆) = |𝐸[𝑆]|, where |𝐸[𝑆]| denotes the number of edges in the induced subgraph 𝐺[𝑆]. Dense Subgraph on Weighted Graphs (DW) [23]. In transac- tion graphs, it is common to have weights assigned to the edges, representing quantities such as the transaction amount. The DW metric extends the concept of dense subgraphs to accommodate these weighted relationships. Given a subset of vertices 𝑆 ⊆ 𝑉 , the weight function of DW is defined as 𝑓 (𝑆) = ∑ (𝑢𝑖 ,𝑢 𝑗 )∈𝐸[𝑆] 𝑐𝑖 𝑗 . Fraudar (FD) [24]. In order to mitigate the effects of fraudulent actors deliberately obfuscating their activities, Hooi et al. [24] introduced the FD algorithm. This approach innovatively incorporates weights for edges and assigns prior suspiciousness scores to vertices, potentially utilizing side information to enhance detection accuracy. Given a subset of vertices 𝑆 ⊆ 𝑉 , the weight function of FD is defined as Equation 1 where 𝑎𝑖 denotes the suspicious score of vertex 𝑢𝑖 and 𝑐𝑖 𝑗 = 1 log(𝑥+𝑐) . Here 𝑥 is the degree of the object vertex between 𝑢𝑖 and 𝑢 𝑗 , and 𝑐 is a positive constant [24]. Triangle Densest Subgraph (TDS) [46]. Triangles serve as a crucial structural motif in graphs, providing valuable insights into the connectivity and cohesiveness of subgraphs. The TDS metric, in par- ticular, leverages the prevalence of triangles to quantify the density of a subgraph. Given a subset of vertices 𝑆 ⊆ 𝑉 , the weight function of TDS is defined as 𝑓 (𝑆) = 𝑡 (𝑆) where 𝑡 (𝑆) represents the total number of triangles within the induced subgraph 𝐺[𝑆]. 𝑘-Clique Densest Subgraph (𝑘CLiDS) [14]. Extending the concept of triangle densest subgraphs, 𝑘CLiDS focuses on identifying dense subgraphs based on the presence of 𝑘-Cliques. A 𝑘-Clique is a complete subgraph of 𝑘 vertices, representing a tightly-knit group of vertices. For a given subset of vertices 𝑆 ⊆ 𝑉 , the weight function of 𝑘CLiDS is defined as 𝑓 (𝑆) = 𝑘(𝑆), where 𝑘(𝑆) denotes the number of 𝑘-Cliques within the induced subgraph 𝐺[𝑆].',\n",
       " '2.2 Sequential Peeling Algorithms': 'Peeling algorithms have gained popularity for their efficiency, ro- bustness, and proven worst-case performance in various applica- tions [6, 24, 46]. Next, we provide a concise overview of the typical execution flow of these algorithms. Peeling Weight.We use𝑤𝑢𝑖 (𝑆) to indicate the decrease in the value of the weight function 𝑓 when the vertex𝑢𝑖 is removed from a vertex set 𝑆 , i.e., the peeling weight. Formally, 𝑤𝑢𝑖 (𝑆) = 𝑎𝑖 + ∑︁ (𝑢 𝑗 ∈𝑆) ∧ ((𝑢𝑖 ,𝑢 𝑗 )∈𝐸) 𝑐𝑖 𝑗 + ∑︁ (𝑢 𝑗 ∈𝑆) ∧ ((𝑢 𝑗 ,𝑢𝑖 )∈𝐸) 𝑐 𝑗𝑖 (2) Algorithm 1: Sequential Peeling Algorithm Input: A graph𝐺 = (𝑉 , 𝐸) and a density metric 𝑔(𝑆) Output: A subset of vertices 𝑆 that maximizes the density metric 𝑔(𝑆) 1 𝑆0 ← 𝑉 /* Initialize the subset with all vertices */ 2 for 𝑖 ← 1 to |𝑉 | do 3 𝑢 ← arg max𝑣∈𝑆𝑖−1 𝑔(𝑆𝑖−1 \\\\ {𝑣 }) /* The vertex to be peeled */ 4 𝑆𝑖 ← 𝑆𝑖−1 \\\\ {𝑢 } /* The selected vertex */ 5 return arg max𝑆𝑖 𝑔(𝑆𝑖 ) /* The subset that maximizes 𝑔(𝑆) */ Sequential PeelingAlgorithm (Algorithm1).The peeling process starts with the full vertex set 𝑆0 = 𝑉 (Line 1) and iteratively removes vertices to maximize the density metric 𝑔. In each step 𝑖 , a vertex𝑢𝑖 is removed from 𝑆𝑖−1 to form 𝑆𝑖 , such that 𝑔(𝑆𝑖−1 \\\\ {𝑢𝑖 }) is maximized (Lines 3∼4). This process is repeated until 𝑆𝑖 becomes empty, resulting in a sequence of nested sets 𝑆0, 𝑆1, . . . , 𝑆 |𝑉 | . The algorithm ultimately returns the subset 𝑆𝑖 ∈ {𝑆0, . . . , 𝑆 |𝑉 |} that maximizes 𝑔(𝑆𝑖 ). Example 2.1 (Seqential Peeling Algorithm Process). Consider the graph 𝐺 as shown in Figure 2. The process begins with an initial graph density of 2.33. In the first iteration, vertex 𝑢1 is peeled due to its smallest peeling weight among all vertices. Subsequently, vertices are peeled in the following order: 𝑢2, 𝑢3, 𝑢4, 𝑢5, and 𝑢6 based on the updated criteria after each peeling operation. Notably, after peeling 𝑢2, the graph density increases, which is a characteristic observation in the peeling process. The sequence of peeling continues until the last vertex, 𝑢6, is peeled, resulting in a final graph density of 0. The peeling sequence, therefore, is 𝑂 = [𝑢1, 𝑢2, 𝑢3, 𝑢4, 𝑢5, 𝑢6], completely disassembling the graph. The induced subgraph𝐺[𝑆] of 𝑆 = [𝑢3, 𝑢4, 𝑢5, 𝑢6] has the greatest density of 2.75, thus 𝐺[𝑆] is returned. Approximation. An algorithm 𝑄 is defined as an 𝛼-approximation for the Densest Subgraph Discovery (DSD), where 𝛼 ≥ 1, if for any given graph it returns a subset 𝑆 satisfying the condition: 𝑔(𝑆) ≥ 𝑔(𝑆∗) 𝛼 , where 𝑆∗ is the optimal subset that maximizes the density metric 𝑔. Theorem 2.1 ([24]). For the vertex set 𝑆𝑝 returned by Algorithm 1 and the optimal vertex set 𝑆∗, it holds that 𝑔(𝑆𝑝 ) ≥ 𝑔(𝑆∗) 2 for DG, DW and FD as the density metrics. We extend the analysis to TDS and 𝑘CLiDS. The proof of the following theorem is omitted due to space limitation. Theorem 2.2. For the vertex set 𝑆𝑝 returned by Algorithm 1 and the optimal vertex set 𝑆∗, it holds that 𝑔(𝑆𝑝 ) ≥ 𝑔(𝑆∗) 𝑘 for TDS and 𝑘CLiDS as the density metrics where 𝑘 = 3 for TDS.',\n",
       " '2.3 Related work': 'Densest Subgraph Discovery (DSD). Densest subgraph discov- ery represents a core area of research in graph theory, extensively explored in literature [7, 8, 20, 30, 39, 40]. The foundational max-flow- based algorithm by Goldberg et al. [21] set a benchmark for exact densest subgraph detection, with subsequent studies introducing more scalable exact solutions [33, 34]. While speed enhancements have been achieved through various greedy approaches [6, 8, 24, 33], these methods are inherently sequential and difficult to parallelize. For example, they often peel one vertex at a time, which results in too many peeling iterations. This sequential nature limits their scalability and efficiency on massive graphs. In graph-based fraud de- tection, techniques like COPYCATCH [4] and GETTHESCOOP [27] leverage local search heuristics to identify dense subgraphs in bi- partite networks, frequently applied in fraud, spam, and community detection across social and review platforms [24, 37, 42]. Despite their widespread use, these methods suffer from efficiency issues in very large graphs, often involving numerous iterations and lack- ing effective pruning techniques, which exacerbates the long-tail problem. Moreover, they are limited to specific definitions of dense subgraphs. Contrarily, Dupin offers a versatile, parallel approach to densest subgraph detection. It facilitates the simultaneous peeling of multiple vertices, drastically reducing the computational overhead. Additionally, Dupin provides APIs that allow users to customize defi- nitions of dense subgraphs, ensuring that the resultant graph density adheres to theoretical bounds. DSD on Dynamic Graphs. To tackle real-time fraud detection, sev- eral variants have been designed for dynamic graphs [10, 18, 26, 28, 43]. [28] addresses real-time fraud community detection on evolving graphs (edge insertions) using an incremental peeling sequence re- ordering method, while [26] extends this approach to fully dynamic graphs (edge insertions and deletions). [43] proposes an incremental algorithm that maintains and updates a dense subtensor in a ten- sor stream. [12] and [36] utilize sliding windows to detect dense subgraphs and bursting cores within specific time windows. These methods have three limitations. First, response time is an issue. Al- though they respond quickly to benign communities, the results can change drastically for newly formed fraudulent communities, caus- ing significant delays in incremental calculations for fraud detection. Second, efficiency is problematic. While incremental processing is faster than recomputation for a single transaction on billion-scale graphs, frequent incremental evaluations can be slower than a full recomputation. Third, in terms of flexibility, most methods support only a single semantic. In contrast, Dupin introduces a unified parallel framework for peeling-based dense subgraph detection.Dupin can respond within seconds on billion-scale graphs and defines multiple density metrics that can be used on the Dupin platform. Parallel DSD. Several parallel methods for DSD have been pro- posed [2, 14, 16, 39, 41, 42]. Bahmani et al. [2] proposed a parallel algorithm for DG using the MapReduce framework. FWA [15] proposed a parallel algorithm for DW based on Frank-Wolfe algo- rithm [25] using OpenMP [13]. Some works propose parallel 𝑘-core algorithms [32, 41, 44]. Others introduce parallel algorithms for 𝑘- Clique detection, with approaches offering a (1 + 𝜖)-approximation ratio and parallel implementations for the 𝑘CLiDS problem [16, 45]. There are also algorithms for 𝑘-Clique densest subgraph detection using OpenMP [14], and methods for dense subtensor detection [42]. However, these systems have two main limitations. First, they typi- cally support only a single-densitymetric and do not handle weighted graphs. In real-world applications, transactions between users can have varying degrees of importance based on amounts, frequen- cies [28], and other factors. Extending these systems to support other dense metrics with theoretical guarantees is non-trivial. Second, they often lack efficient pruning techniques, requiring numerous itera- tions and suffering from the long-tail issue, which hinders scalability on massive graphs. In contrast, Dupin defines the characteristics of a set of supported dense metrics and accommodates weighted sub- graphs, allowing for more nuanced analysis reflective of real-world data. Our method introduces efficient pruning strategies by peeling multiple vertices in parallel based on a threshold mechanism, signifi- cantly reducing the number of iterations and mitigating the long-tail problem. This makes Dupin more effective and scalable in diverse real-world scenarios compared to previous approaches. 3 THE Dupin FRAMEWORK',\n",
       " '3.1 Motivations': 'A notable example of advanced fraud detection is seen in Anon, a prominent technology company in Southeast Asia known for its digital payment and food delivery services. As delineated in [28], the detection process begins with forming a transaction graph,𝐺 , derived from the platform’s transactions. This graph is incrementally updated, represented as 𝐺 = 𝐺 ⊕ ∆𝐺 . Subsequent stages utilize densest subgraph discovery algorithms to uncover fraudulent communities. A typical fraud scenario on the platform involves customer-merchant collusion, where fake accounts are created to exploit promotional incentives. These fraudulent activities manifest as densest subgraphs within the transaction graph. Efficiency. Our empirical findings reveal that the incremental execution of DSD algorithms, such as Spade [28], on a graph with 2 billion edges requires approximately 198 seconds per batch (default setting: one batch of 1K edge insertions). However, in practical business sce- narios, the number of incoming edges can peak at 400K per minute. Clearly, using incremental evaluation on billion-scale graphs does not provide sufficient throughput to meet business requirements. Latency and Prevention Ratio. Existing DSD systems face sig- nificant challenges in terms of latency and prevention ratio. Upon identification of fraudulent activities, actions such as account bans or freezes are implemented to prevent further losses. However, the aver- age latency for these systems to detect fraud and take action is often high, typically around 200 to 300 seconds for incremental algorithms and more than 1000 seconds for existing parallel algorithms, which is insufficient for real-time requirements (operational demand requires detection within seconds). Moreover, when the graph size increases from million-scale to billion-scale, the prevention ratio decreases from 92.47% to 45%. These issues underscore the necessity for more efficient and scalable algorithms that can maintain low latency and high prevention ratios, even under heavy transaction loads. Parallelization offers a viable solution to these challenges by dis- tributing the computational load across multiple processors. Mod- ern CPUs are equipped with numerous cores that are cost-effective, enabling parallel processing. If the peeling process of DSD is paral- lelized, different processors can simultaneously evaluate and peel different nodes, thereby increasing scalability and efficiency. For example, if we can improve processing speed by one to two orders of magnitude, the system would be able to meet industry requirements for real-time fraud detection, completing detection within seconds. Problem Statement. Given a graph 𝐺 = (𝑉 , 𝐸) and a community detection semantic with density metric 𝑔(𝑆), our goal is to find a vertex subset 𝑆∗ ⊆ 𝑉 that maximizes 𝑔(𝑆∗).',\n",
       " '3.2 System Architecture': 'To enable users to design complex DSD algorithms to detect fraudsters based on our framework, Dupin, we have introduced the archi- tecture, APIs, and supported density metrics in this section. Architecture of Dupin. Figure 3 presents the architecture of Dupin with user-defined functions for detecting suspicious activities.Dupin enables users to define their fraud detection semantics through a set of provided APIs. Within Dupin’s computational engine, we sup- port both sequential and parallel peeling algorithms. Additionally, algorithms for long-tail pruning techniques are implemented to ac- celerate convergence and enhance the density of subgraphs in each iteration. Dupin identifies fraudulent communities, which are then flagged for further investigation by moderators. APIs of Dupin (Figure 3). Dupin simplifies the process of defining detection semantics by allowing users to specify a few simple functions. Additionally, Dupin offers APIs that enable users to balance detection speed and accuracy effectively. The key APIs encompass several components designed to facilitate the customization and op- timization of fraud detection processes. These components provide a robust framework for users to implement their fraud detection strate- gies, ensuring both high performance and precision in identifying fraudulent activities. Details are listed below: • pWeight and updateNgh. These functions are designed for the dynamic adjustment of weights within the network. They allow the user to define the peeling weight of a vertex and specify the method for updating the peeling weight of neighboring vertices after a vertex has been peeled. • VSusp and ESusp. Given a vertex/edge, these components are re- sponsible for deciding the suspiciousness of the endpoint of the edge or the edge with a user-defined strategy. • isBenign. This component is used to decide whether a vertex is benign during the whole peeling process (Section 5). If the vertex is benign, it is peeled within the current iteration. • setEpsilon. This function is utilized to control the precision of fraud detection. During periods of high system load, moderators can increase 𝜖 to achieve higher throughput. Conversely, when the system load is low, 𝜖 can be decreased to enhance accuracy. • setK. This function specifies the 𝑘-Clique parameter, enabling the definition of the 𝑘-Clique for analysis. 1 double vsusp(const Vertex& v, const Graph& g) { 2 return g.weight[v]; // Side information on vertex 3 } 4 double esusp(const Edge& e, const Graph& g) { 5 return 1.0 / log(g.deg[e.src] + 5.0); 6 } 7 int main() { 8 Dupin dupin; 9 dupin.VSusp(vsusp); // Plug in vsusp 10 dupin.ESusp(esusp); // Plug in esusp 11 dupin.setEpsilon (0.1); 12 dupin.LoadGraph(\"graph_sample_path\"); 13 vector <Vertex > fraudsters = dupin.ParDetect (); 14 return 0; 15 } Listing 1: Implementation of FD on Dupin Example (Listing 1). To implement FD [24] on Dupin, users simply need to plug in the suspiciousness function vsusp for the vertices by calling VSusp and the suspiciousness function esusp for the edges by calling ESusp. Specifically, 1) vsusp is a constant function, i.e., given a vertex 𝑢, vsusp(𝑢) = 𝑎𝑖 and 2) esusp is a logarithmic function such that given an edge (𝑢𝑖 , 𝑢 𝑗 ), esusp(𝑢𝑖 , 𝑢 𝑗 ) = 1 log(𝑥+𝑐) , where 𝑥 is the degree of the object vertex between 𝑢𝑖 and 𝑢 𝑗 , and 𝑐 is a positive constant [24]. Developers can easily implement customized peeling algorithms with Dupin, significantly reducing the engineering effort. Density Metrics Support in Dupin. To illustrate the versatility of Dupin in handling various density metrics, we establish comprehensive sufficient conditions. These conditions ensure Dupin’s adaptability in integrating a wide range of metrics to assess graph density effectively. This foundational approach enablesDupin to sup- port diverse density calculations, crucial for nuanced fraud detection and network analysis tasks. Property 3.1. Dupin supports a density metric 𝑔(𝑆) if the following conditions are met: (1) 𝑔(𝑆) = 𝑓 (𝑆) |𝑆 | , where 𝑓 is a monotone increasing function. (2) 𝑎𝑖 ≥ 0, ensuring non-negative vertex weight function. (3) 𝑐𝑖 𝑗 ≥ 0, ensuring non-negative edge weight function.',\n",
       " '4 PARALLELABLE PEELING ALGORITHMS': 'For sequential peeling algorithms, only one vertex can be removed at a time, with each vertex’s removal depending on the removal of previous vertices. This dependency prevents the algorithm from executing multiple vertex removals simultaneously. In this section, we introduce a parallel peeling paradigm to break this dependency chain. This new approach allowsmultiple vertices to be peeled in parallel without affecting the programming abstractions that Dupin provides to end users. We then demonstrate how this paradigm offers fine-grained trade-offs between parallelism and ap- proximation guarantees for all density metrics studied in this work.',\n",
       " '4.1 Parallel Peeling Paradigm': 'Instead of peeling the vertex with the maximum peeling weight, we can peel all vertices that lead to a significant decrease in the density scores of the remaining graph. This decrease is controlled by a tunable parameter 𝜖 . A larger 𝜖 allows more vertices to be peeled in parallel, while a smaller 𝜖 provides a better approximation ratio. Algorithm 2: Dupin: Parallel Peeling Input:𝐺 = (𝑉 , 𝐸), a density metric 𝑔(𝑆), and 𝜖 ≥ 0 Output: A subset of vertices 𝑆 that maximizes the density metric 𝑔(𝑆) 1 𝑆0 ← 𝑉 /* Initialize the subset with all vertices */ 2 𝑖 ← 1 /* Initialize the index */ 3 while 𝑆𝑖−1 ̸= ∅ do 4 𝜏 = 𝑘(1 + 𝜖) · 𝑔(𝑆𝑖−1) /* The threshold for peeling */ 5 𝑈 ← {𝑢 | 𝑤𝑢 (𝑆𝑖−1) ≤ 𝜏 } /* Vertices to be peeled */ 6 𝑆𝑖 ← 𝑆𝑖−1 \\\\𝑈 /* Update the subset */ 7 𝑖 ← 𝑖 + 1 /* Increment the index */ 8 return arg max𝑆𝑖 𝑔(𝑆𝑖 ) /* The subset that maximizes 𝑔(𝑆) */ Parallel Peeling (Algorithm 2). Let 𝑆𝑖 denote the vertex set after the 𝑖-th peeling iteration. Initially, 𝑆0 = 𝑉 (Line 1). In each iteration, it removes a subset of vertices𝑈 from 𝑆𝑖−1, where ∀𝑢 ∈ 𝑈 ,𝑤𝑢 (𝑆𝑖−1) ≤ 𝑘(1 + 𝜖)𝑔(𝑆𝑖−1). Parameter 𝑘 is determined by the metric used. 𝑘 = 2 works for DG, DW and FD. 𝑘 = 3 works for TDS and 𝑘 is the size of the cliques for 𝑘CLiDS. This process is repeated recursively until no vertices are left, resulting in a series of vertex sets 𝑆0, . . . , 𝑆𝑅 , where 𝑅 is the number of iterations. The algorithm then returns the set 𝑆𝑖 (𝑖 ∈ [0, 𝑅]) that maximizes the density metric 𝑔(𝑆𝑖 ), denoted as 𝑆𝑝 . In what follows, we also show a case where 𝑘CLiDS is the density metric for DSD. 3 3 6 6 5 4 21 0 00 3 3 6 6 3 3 0 0 Density: 0.91 Density: 1.33 Density: 0 Iteration i Iteration i+1 Iteration i+2 Iteration i+3 Density: 0 τ = 3× 0.91 = 2.73 τ = 3× 1.33 = 3.99 τ = 0 τ = 0',\n",
       " '4.2 Theoretical Analysis': 'In this section, we analyze the theoretical properties of the parallel peeling process. We begin by examining the number of iterations required for parallel peeling. Built upon the result, we then determine the time complexity and the approximation ratio of the parallel peeling process. Lemma 4.1. Given any 𝜖 > 0, 𝑅 < log 1+𝜖 |𝑉 |. Proof. We prove that for any 𝜖 ≥ 0, the size of the remaining vertex set is bounded by |𝑆𝑖 |< 1 1+𝜖 |𝑆𝑖−1 | for all 𝑖 ∈ [1, 𝑅]. We first examine the peeling weights of the vertices in 𝑆𝑖−1 for the density metrics DG. DW and FD where 𝑘 = 2. As each edge weight in the peeling weight calculation contributes twice (once for each endpoint), and the vertex weights contribute once, we have∑︁ 𝑢∈𝑆𝑖−1 𝑤𝑢 (𝑆𝑖−1) ≤ 2𝑓 (𝑆𝑖−1), (3) The peeling weights of the vertices in 𝑆𝑖−1 are calculated as follows:∑︁ 𝑢∈𝑆𝑖−1 𝑤𝑢 (𝑆𝑖−1) = ∑︁ 𝑢∈𝑆𝑖−1\\\\𝑆𝑖 𝑤𝑢 (𝑆𝑖−1) + ∑︁ 𝑢∈𝑆𝑖 𝑤𝑢 (𝑆𝑖−1) ≥ ∑︁ 𝑢∈𝑆𝑖 𝑤𝑢 (𝑆𝑖−1) > 2(1 + 𝜖)|𝑆𝑖 |𝑔(𝑆𝑖−1) (4) Combining Equation 3 and Equation 4, we have: 2(1 + 𝜖)|𝑆𝑖 |𝑔(𝑆𝑖−1) < 2𝑓 (𝑆𝑖−1) = 2|𝑆𝑖−1 |𝑔(𝑆𝑖−1) (5) For all 𝑖 ∈ [1, 𝑅], |𝑆𝑖 |< 1 1 + 𝜖 |𝑆𝑖−1 | (6) Then we have: |𝑉 |= |𝑆0 |> 1 1 + 𝜖 |𝑆1 |> . . . > ( 1 1 + 𝜖 ) 𝑅 |𝑆𝑅 | (7) Therefore, we have 𝑅 < log 1+𝜖 |𝑉 |. For the density metrics TDS and 𝑘CLiDS, we use the fact that each edge contributes to the peeling weight calculation exactly 𝑘 times due to the counting of each edge within every 𝑘-Clique in the graph (𝑘 = 3 for TDS). This multiplicity of edge contributions leads to the relationship: ∑︁ 𝑢∈𝑆𝑖−1 𝑤𝑢 (𝑆𝑖−1) ≤ 𝑘 𝑓 (𝑆𝑖−1), (8) We then adapt Equations 4-7 and the proof follows. □ Time Complexity. Algorithm 2 operates for at most 𝑅 = log 1+𝜖 |𝑉 | iterations, with each iteration scanning all vertices, taking 𝑂(|𝑉 |) time. As vertices are peeled, updates are necessitated for the peeling weights of their neighboring vertices. Over the course of 𝑅 iterations, there are no more than |𝐸 | such updates for DG, DW and FD. Consequently, Algorithm 3 incurs 𝑂((|𝐸 |+|𝑉 |) log 1+𝜖 |𝑉 |) node/edge weight updates. For DG, DW and FD, the update costs are 𝑂(1). For TDS and 𝑘CLiDS, we follow previous studies [11, 14] to compute the peeling weights, and the complexity is 𝑂(𝑘 |𝐸 |𝛼(𝐺) 𝑘−2 ), where 𝛼(𝐺) is the arboricity of the graph. Hence, the overall complexity is 𝑂(𝑘 |𝐸 |𝛼(𝐺) 𝑘−2 (|𝐸 |+|𝑉 |) log 1+𝜖 |𝑉 |). The cost of peeling weight up- dates is orthogonal to the Dupin framework. Approximation Ratio. Next, we are ready to show that Dupin provides a theoretical guarantee with a 𝑘(1 + 𝜖)-approximation for all density metrics studied in this work. We denote 𝑢𝑖 ∈ 𝑆𝑝 as the first vertex in 𝑆∗ peeled by Dupin, and 𝑆 ′ as the peeling subsequence starting from 𝑢𝑖 . Theorem 4.2. For the vertex set 𝑆𝑝 returned by Dupin and the optimal vertex set 𝑆∗, the relationship 𝑔(𝑆𝑝 ) ≥ 𝑔(𝑆∗) 𝑘(1+𝜖) holds. Proof. We first prove that ∀𝑢𝑖 ∈ 𝑆∗, 𝑤𝑢𝑖 (𝑆 ∗ ) ≥ 𝑔(𝑆∗). We prove it in contradiction. We assume that 𝑤𝑢𝑖 (𝑆 ∗ ) < 𝑔(𝑆∗). By peeling 𝑢𝑖 from 𝑆∗, we have the following. 𝑔(𝑆∗ \\\\ {𝑢𝑖 }) = 𝑓 (𝑆∗) −𝑤𝑢𝑖 (𝑆 ∗ ) |𝑆∗ |−1 > 𝑓 (𝑆∗) − 𝑔(𝑆𝑖 ) |𝑆∗ |−1 = 𝑓 (𝑆∗) − 𝑔(𝑆∗) |𝑆∗ |−1 = 𝑓 (𝑆∗) − 𝑓 (𝑆∗) |𝑆∗ | |𝑆∗ |−1 = 𝑔(𝑆∗) (9) Therefore, a better solution can be obtained by removing 𝑢𝑖 from 𝑆∗, which contradicts the fact that 𝑆∗ is the optimal solution. We can conclude that𝑤𝑢𝑖 (𝑆 ∗ ) ≥ 𝑔(𝑆∗). Next, we assume that 𝑢𝑖 ∈ 𝑆𝑝 is the first vertex in 𝑆∗ peeled by the peeling algorithm. Since 𝑆∗ is the optimal vertex set, we have 𝑔(𝑆∗) ≤ 𝑤𝑢𝑖 (𝑆 ∗ ), (10) because the optimal value 𝑔(𝑆∗) must be less than or equal to the weight contribution of any single vertex in 𝑆∗, in particular 𝑢𝑖 . Since 𝑆∗ ⊆ 𝑆 ′, it is clear that 𝑤𝑢𝑖 (𝑆 ∗ ) ≤ 𝑤𝑢𝑖 (𝑆 ′ ) (11) By the peeling condition, we have 𝑤𝑢𝑖 (𝑆 ′ ) ≤ 𝑘(1 + 𝜖)𝑔(𝑆 ′) (12) Therefore, we have 𝑔(𝑆∗) ≤ 𝑤𝑢𝑖 (𝑆 ∗ ) ≤ 𝑤𝑢𝑖 (𝑆 ′ ) ≤ 𝑘(1 + 𝜖)𝑔(𝑆 ′) ≤ 𝑘(1 + 𝜖)𝑔(𝑆𝑝 ) (13) Hence, we can conclude that 𝑔(𝑆𝑝 ) ≥ 𝑔(𝑆∗) 𝑘(1+𝜖) . □ Summary. Dupin can easily balance efficiency and the worst-case approximation ratio with a single parameter, 𝜖 . A larger 𝜖 allows more vertices to be peeled in parallel, reducing the number of peeling iterations. Meanwhile, the worst-case approximation ratio of parallel peeling is only affected by a factor of (1 + 𝜖) compared to the ratio for sequential peeling w.r.t. all density metrics studied in this work.',\n",
       " '5 LONG-TAIL PRUNING': 'We observe that the parallel peeling algorithm encounters two sig- nificant issues. First, there is a long-tail problem where subsequent iterations fail to produce denser subgraphs. Instead, each iteration only peels off a small fraction of vertices, which significantly impacts the overall runtime efficiency. Second, during batch peeling, each peeling iteration often results in disparate structures. These are pri- marily caused by the peeling of certain vertices, which leaves their neighboring structure disconnected and fragmented. This phenome- non affects the continuity and coherence of the subgraphs. As shown in Table 2, on the la dataset (the largest social network dataset tested in our experiments), DG, DW, and FD experience 24.67%, 46.84%, and 3.01% of rounds as ineffective long-tail iterations, severely limiting response time. Additionally, during the peeling process, if the batch size is large (e.g., 𝜖 = 0.5), approximately 25.34%, 29.46%, and 7.16% of nodes become disconnected and sparse, signifi- cantly impacting the quality of the detected subgraphs. To address the challenges of high iteration counts in peeling algorithms, Dupin introduces two long-tail pruning techniques: Global Peeling Optimization (GPO) to maintain a global peeling threshold, and Local Peeling Optimization (LPO) to improve the density of the current peeling iteration’s subgraph, thereby increasing the peeling threshold. We tested three algorithms, DG, DW, and FD, and found that they required 17,637, 150,223, and 112,074 iterations respectively, which is significantly high. By strategically pruning long-tail ver- tices and sparse vertices, the number of iterations can be reduced by up to 46.84%. Additionally, LPO can further reduce the number of iterations by up to 92.79%. These optimizations significantly enhance the efficiency of the peeling process, as discussed in this section.',\n",
       " '5.1 Global Peeling Optimization': 'Initially, we define what constitutes a long-tail vertex during the peeling process. Peeling these long-tail vertices results in ineffective peeling iterations. To address this, we maintain a global maximum peeling threshold to determine which vertices can be peeled directly. When the global peeling threshold is greater than the current itera- tion’s threshold, we use the global peeling threshold for peeling. Algorithm 3: DupinGPO: Global Peeling Optimization Input:𝐺 = (𝑉 , 𝐸), a density metric 𝑔(𝑆), and 𝜖 ≥ 0 Output: A subset of vertices 𝑆𝑝 that maximizes the density metric 𝑔(𝑆) 1 𝑆0 ← 𝑉 /* Initialize the subset with all vertices */ 2 𝑖 ← 1 3 𝜏max ← 𝑔(𝑆 0 ) 𝑘(1+𝜖) /* Initialize the threshold */ 4 while 𝑆𝑖−1 ̸= ∅ do 5 𝜏 ← max{𝜏max, 𝑘(1 + 𝜖) · 𝑔(𝑆𝑖−1)} /* Peeling threshold */ 6 𝑈 ← {𝑢 | 𝑤𝑢 (𝑆𝑖−1) ≤ 𝜏 } /* Vertices to be peeled */ 7 𝑆𝑖 ← 𝑆𝑖−1 \\\\𝑈 /* Update the subset */ 8 𝜏max = max{𝜏max, 𝑔(𝑆𝑖 ) 𝑘(1+𝜖) } /* Refine the threshold */ 9 𝑖 ← 𝑖 + 1 10 return arg max𝑆𝑖 𝑔(𝑆𝑖 ) /* The subset that maximizes 𝑔(𝑆) */ Definition 5.1 (Long-Tail vertex). Given an 𝛼-𝑎𝑝𝑝𝑟𝑜𝑥𝑖𝑚𝑎𝑡𝑖𝑜𝑛 DSD algorithm and a set of vertices 𝑆 ⊆ 𝑉 , if𝑤𝑢 (𝑆) < 𝑔(𝑆∗) 𝛼 and 𝑆∗ is the optimal vertex set, then 𝑢 is a long-tail vertex. Lemma 5.1. If ∃𝑢 ∈ 𝑆𝑖 is a long-tail vertex, then 𝑆𝑖 ̸= 𝑆𝑃 . Proof. We prove it in contradiction by assuming that 𝑆𝑖 = 𝑆𝑃 . Since 𝑢 is a long-tail vertex in 𝑆𝑖 ,𝑤𝑢 (𝑆𝑖 ) < 𝑔(𝑆∗) 𝛼 < 𝑔(𝑆𝑃 ). By peeling 𝑢 from 𝑆𝑃 , we have the following. 𝑔(𝑆𝑃 \\\\ {𝑢}) = 𝑓 (𝑆𝑃 ) −𝑤𝑢 (𝑆𝑃 ) |𝑆𝑃 |−1 > 𝑓 (𝑆𝑃 ) − 𝑔(𝑆𝑃 ) |𝑆𝑃 |−1 = 𝑓 (𝑆𝑃 ) − 𝑓 (𝑆𝑃 ) |𝑆𝑃 | |𝑆𝑃 |−1 = 𝑔(𝑆𝑃 ) (14) A denser subgraph can be obtained by peeling 𝑢 from 𝑆𝑃 . This contradicts that 𝑆𝑃 is the optimal solution. Hence, 𝑆𝑖 ̸= 𝑆𝑃 . □ Identifying Long-Tail Vertices. Based on Lemma 5.2, we can also establish that a vertex𝑢 in set 𝑆 can be classified as a long-tail vertex if 𝑤𝑢 (𝑆) < 𝑔(𝑆∗) 𝑘(1+𝜖) . This insight allows us to implement a global peeling threshold, denoted as 𝜏max. During each iteration, we compare 𝑔(𝑆𝑖 ) 𝑘(1+𝜖) with 𝜏max. If the former exceeds the latter, Dupin updates 𝜏max to 𝑔(𝑆𝑖 ) 𝑘(1+𝜖) , thereby enhancing the efficiency of the peeling. Peeling with Global Threshold (Algorithm 3). Using FD as an example, we can set 𝑘 to 2. In Lemma 5.2, we recognize that long-tail vertices in set 𝑆𝑖 can be peeled directly during iteration 𝑖 for FD. To facilitate this, Dupin initiates with a global peeling threshold 𝜏max (Line 3). After identifying a denser subgraph post-peeling, 𝜏max is updated to 𝑔(𝑆𝑖 ) 2(1+𝜖) , refining the peeling process (Line 8). Example 5.1. In the initial depiction provided in the leftmost panel of Figure 6 (the 𝑖-th iteration), the density of the graph is 4.28. Traditionally, this scenario would necessitate two additional iterations to complete the peeling process, which could notably increase computational time. As illustrated, vertex 𝑢 cannot be peeled in the (𝑖 + 1)-th iteration and would typically require an extra iteration, the (𝑖 + 2)-th, for its removal. However, Dupin can promptly peel vertices that fall below this threshold by employing our algorithm’s global peeling threshold, which is calculated as half of the current density (𝜏max = 4.28/2 = 2.14). This approach enables the simultaneous peeling of three nodes in the (𝑖 + 1)-th iteration. Without this pruning strategy, an additional iteration would be essential to peel vertex 𝑢, underscoring our method’s enhanced efficiency in the peeling process. This optimization obviates the necessity for a prolonged iterative sequence. The example herein substantiates the efficacy of our tai- lored optimization for long-tail distribution, substantially enhancing the algorithm’s time efficiency.',\n",
       " '5.2 Local Peeling Optimization': 'Algorithm 4: DupinLPO: Local Peeling Optimization Input: A graph𝐺 = (𝑉 , 𝐸), a density metric 𝑔(𝑆), and 𝜖 ≥ 0 Output: A subset of vertices 𝑆𝑝 representing the densest subgraph 1 𝑆0 ← 𝑉 /* Initialize the subset with all vertices */ 2 𝜏max ← 𝑔(𝑆 0 ) 𝑘(1+𝜖) /* Initialize the threshold */ 3 while 𝑆𝑖−1 ̸= ∅ do 4 𝜏1 ← max{𝜏max, 𝑘(1 + 𝜖) · 𝑔(𝑆𝑖−1)} /* Peeling threshold */ 5 𝑈1 ← {𝑢 ∈ 𝑆 | 𝑤𝑢 (𝑆) ≤ 𝜏1 } 6 𝑆𝑖 ← 𝑆𝑖−1 \\\\𝑈1 7 while ∃𝑢 ∈ 𝑆𝑖 , 𝑤𝑢 (𝑆𝑖 ) < 𝑔(𝑆𝑖 ) do 8 𝜏2 ← max{𝜏max, 𝑔(𝑆𝑖 )} /* Update the threshold */ 9 𝑈2 ← {𝑢 | 𝑤𝑢 (𝑆𝑖 ) < 𝜏2 } 10 𝑆𝑖 ← 𝑆𝑖 \\\\𝑈2 11 𝜏max = max{𝜏max, 𝑔(𝑆𝑖 ) 𝑘(1+𝜖) } /* Update the threshold */ 12 return arg max𝑆𝑖 𝑔(𝑆𝑖 ) /* The subset that maximizes 𝑔(𝑆) */ It is important to note that each peeling iteration often results in disparate structures. This is primarily caused by the peeling of cer- tain vertices, which leaves their neighboring structures disconnected and fragmented. This not only affects the density of the detected sub- graph but also lowers the peeling threshold. Lemma 5.2 establishes a foundational property of vertices within a dense graph. Lemma 5.2. ∀𝑢 ∈ 𝑆 , if𝑤𝑢 (𝑆) < 𝑔(𝑆), 𝑔(𝑆 \\\\ {𝑢}) > 𝑔(𝑆). Proof. By peeling 𝑢 from 𝑆 , we have the following. 𝑔(𝑆 \\\\ {𝑢}) = 𝑓 (𝑆) −𝑤𝑢 (𝑆) |𝑆 |−1 > 𝑓 (𝑆) − 𝑔(𝑆𝑖 ) |𝑆 |−1 = 𝑓 (𝑆) − 𝑔(𝑆) |𝑆∗ |−1 = 𝑓 (𝑆) − 𝑓 (𝑆) |𝑆 | |𝑆 |−1 = 𝑔(𝑆) (15) Therefore, a denser graph can be obtained by peeling 𝑢 from 𝑆 . □ According to Lemma 5.2, we introduce local peeling optimization within the iteration. After each peeling iteration, if the peeling weight of a vertex is smaller than the current density, Dupin will trim it. We implement a local peeling optimization (Lines 7-10, Algorithm 4) before the next iteration. After a vertex 𝑢 is peeled, Dupin invokes updateNgh to update the peeling weights of 𝑢’s neighboring nodes. If any of these neighbors have a peeling weight less than the current density, they are also peeled, thereby increasing the density of the current iteration. Consequently, when an iteration yields globally optimal results, a denser structure is obtained. The benefits of this approach extend beyond density improvements; performance is also enhanced. By trimming these vertices, we reduce the number of vertices to traverse in subsequent iterations, and a higher density enables more efficient iterations due to the possibility of using larger peeling thresholds. Lemma 5.3. If 𝑢𝑖 ∈ 𝑆𝑝 is the first vertex in 𝑆∗ removed by Dupin, 𝑢𝑖 will not be removed during the local peeling optimization process. Proof. Assume, for the sake of contradiction, that 𝑢𝑖 is trimmed during the local peeling optimization process. Let 𝑆 ′ be the set of vertices left before 𝑢𝑖 is trimmed. By definition, we have: 𝑤𝑢𝑖 (𝑆 ′ ) < 𝑔(𝑆∗). Since 𝑢𝑖 is the first vertex in 𝑆∗ to be removed, 𝑆∗ ⊆ 𝑆 ′, therefore: 𝑤𝑢𝑖 (𝑆 ∗ ) ≤ 𝑤𝑢𝑖 (𝑆 ′ ) < 𝑔(𝑆∗). This contradicts Lemma 5.2, which states that ∀𝑢𝑖 ∈ 𝑆∗: 𝑤𝑢𝑖 (𝑆 ∗ ) ≥ 𝑔(𝑆∗). Otherwise, a better solution can be obtained by removing 𝑢 from 𝑆∗, which contradicts the fact that 𝑆∗ is the optimal solution. Thus, 𝑢𝑖 cannot be trimmed. Hence,𝑢𝑖 will not be removed during the local peeling optimization process. □ Due to Lemma 5.3, 𝑢𝑖 will only be removed during the peeling process. Hence, the approximation ratios (Theorem 4.2) for different density metrics are preserved. 6 EXPERIMENTAL STUDY Our experiments are run on a machine that has an X5650 CPU, 512 GB RAM. The implementation is made memory-resident and implemented in C++. All codes are compiled by GCC-9.3.0 with -𝑂3. Datasets. We report the datasets used in our experiment in Table 3. One industrial dataset is from Anon (gfg). Datasets links-anon(la) and Twitter-rv (rv)were obtained from Stanford NetworkDataset Collection [31]. Datasets kron-logn21(kron), soc-twitter(soc), Web-uk-2005(uk) and Web-sk-2005(sk) were obtained from Network Data Repository [38]. Dataset bio was from the BIOMINE [35]. Baseline Methods. In this section, we introduce five commonly used density metrics pivotal in analyzing network structures:DG [6], DW [23], FD [24], TDS [46], and𝑘CLiDS [14].We implemented these metrics in our framework, Dupin, and compared their performance with frameworks including Spade [28], kCLIST [14], GBBS [16], PBBS [41], PKMC [32], FWA [15] and ALENEX [44]. kCLIST [14] and PBBS [41] are parallel 𝑘CLiDS algorithms, while GBBS [16] supports the DG density metric. Although GBBS does not support weighted graphs, we precompute the peeling weights offline and then import them into GBBS. The time spent on this offline computation is not included in our reported runtime. FWA, ALENEX and PKMC support the DG density metric. For clarity, we denote the incremental peeling process, which involves the iterative removal of graph elements based on density scores, as Spade-X, where X corresponds to each metric, under the same experimental condi- tions described in [28], where the batch size is set to 1K, consistent with the default setting in [28]. We report the average runtime per batch for Spade. DupinGPO-X and DupinLPO-X represent our par- allel peeling algorithms that incorporate global and local peeling optimization. Default Settings. In our experiments, the parameter 𝜖 is set to 0.1 by default, and the number of threads 𝑡 is configured to 128. These settings are used consistently unless otherwise specified. 6.1 Efficiency of Dupin Spade vs Dupin. Our initial comparison focuses on the performance disparity between the incremental and parallel peeling algorithms. Specifically, Dupin-DG (resp. Dupin-DW, Dupin-FD, Dupin-TDS, andDupin-𝑘CLiDS) demonstrates a speedup factor of 6.97 (resp. 7.71, 7.71, 7.65, and 8.46) over Spade-DG (resp. Spade-DW, Spade-FD, Spade-TDS, and Spade-𝑘CLiDS). Notably, Spade-TDS and Spade𝑘CLiDS only complete computations within 7,200 seconds on gfg. The bottleneck for Spade arises in larger graphs where initial calculations of triangle and 𝑘-Clique counting are unable to finish. Dupin outpaces Spade primarily because Spade suffers from frequent re- ordering of peeling sequences when there are many increments, which proves to be slower than recalculating. GBBS vs Dupin. Previous works have not provided simultaneous support for all density metrics. To bridge this gap, we implemented DG, DW, and FD within the parallel GBBS framework. On average, Dupin-DG, Dupin-DW, and Dupin-FD demonstrated speedup factors of 6.33, 12.51, and 17.07 times, respectively, over GBBS-DG, GBBS-DW, andGBBS-FD. This performance advantage is attributed to the fundamental difference in the peeling process between the two frameworks. In GBBS, the basic unit of each peel is a bucket, where all nodes within the same bucket have an identical peeling weight. This design limits the parallelism in weighted graphs like those using DW and FD, where many buckets may contain only a single node. In contrast, Dupin peels batches of nodes at each step, achieving higher parallelism and thus consistently outperforming GBBS, particularly in weighted graph scenarios where the speedup is even more pronounced. kCLIST vs PBBS vs Dupin. Both kCLIST and PBBS support the density metrics TDS and 𝑘CLiDS. In our comparisons, Dupin demonstrates notable performance advantages. Specifically, Dupin-TDS is faster than kCLIST-TDS and PBBS-TDS by 39.85 and 62.71 times, respectively. Furthermore, Dupin-𝑘CLiDS also outperforms kCLIST𝑘CLiDS by 4.16 times. A significant finding is that PBBS-𝑘CLiDS fails to complete computations within 7200 seconds on most datasets, highlighting the efficiency of Dupin in handling more complex met- rics under stringent time constraints. Impact of DupinGPO. Next, we evaluate the acceleration effect of DupinGPO, using Dupin as a baseline for comparison. Notably, DupinGPO-DG is found to be 1.14 times faster, DupinGPO-DW is 1.11 times faster,DupinGPO-TDS is 1.10 times faster, andDupinGPO𝑘CLiDS is 1.35 times faster than their respective Dupin implementations. The acceleration is particularly significant for 𝑘CLiDS on larger datasets, such as soc, rv, kron, and sk. DupinGPO-𝑘CLiDS is 1.40, 1.54, 1.51, and 1.61 times faster than Dupin-𝑘CLiDS on soc, rv, kron, and sk, respectively. This is attributed to the greater number of iterations required due to the larger dataset sizes and the higher count of 𝑘-Cliques, necessitating more peeling iterations. In such scenarios,DupinGPO effectively prunes the tail iterations early, thereby reducing the number of rounds needed. Impact of DupinLPO. We next compared the efficiency of Dupin andDupinLPO. On average, thoughDupinLPO-DGwas 10.09% slower than Dupin-DG, the speed is still comparable. For example, on the soc dataset, DupinLPO-DG was 5.94% faster than Dupin-DG. This is because DupinLPO requires some lock operations to update the peeling weights of neighboring vertices during local peeling optimization. However, the advantage is that DupinLPO can detect up to 26.26% denser subgraphs, as detailed in Section 6.2. On larger datasets, such as soc, rv, and la, the vertices being trimmed have relatively low correlations with each other, so the impact of locks is minimal. For example, on the soc (resp. rv), DupinLPO-DG was 5.94% (resp. 9.94%) faster than Dupin-DG. In contrast, on smaller datasets, such as bio and kron, the extra cost of locks is higher. Impact of the Number of Threads 𝑡 . All methods are influenced by the concurrency level, i.e., the number of threads. Generally, the greater the number of threads, the faster the execution speed. In our experiments, using the la dataset as a benchmark, we varied the number of threads 𝑡 from 2 to 128 to observe the impact on runtime performance. For GBBS-DG, runtime stabilizes and accelerates by 3.33 times as thread count increases from 2 to 8. For GBBS-DW, performance deteriorates when the thread count exceeds 32, but from 2 to 32 threads, it accelerates by 3.50 times. Similarly, GBBS-FD shows no significant change in runtime beyond 32 threads, with an acceleration of 3.09 times from 2 to 32 threads. GBBS-TDS and GBBS-𝑘CLiDS fail to complete computations on the la dataset. In contrast, all five methods tested onDupin exhibit good scalability. As the number of threads increases from 2 to 128,Dupin-DG accelerates by 9.91 times, Dupin-DW by 12.69 times, Dupin-FD by 13.03 times, and Dupin-TDS by 28.06 times. Although Dupin-𝑘CLiDS fails to produce results when the thread count is below 16, it accelerates by 1.32 times from 32 to 128 threads. These results demonstrate Dupin’s strong scalability across varied threading levels. 6.2 Effectiveness of Dupin In addition to performance speedup, we evaluate the density of the subgraphs detected by different frameworks. Detailed density comparisons are presented in Table 5. GBBS vs Spade vs Dupin. Firstly, we compare the densities of the subgraphs identified by GBBS-DG (respectively GBBS-DW, and GBBS-FD) to those detected by Dupin in parallel execution. Our experimental results indicate that Dupin maintains comparable subgraph densities with GBBS, signifying that the increase in com- putational efficiency does not compromise the accuracy of the de- tected subgraphs. Specifically, the density of the subgraph detected byGBBS-DG (respectivelyGBBS-DW, andGBBS-FD) is 7.08% (resp. 6.48% and 7.43%) greater than that detected by Dupin-DG (respectively Dupin-DW, and Dupin-FD) on average. Spade shares similar densities with GBBS. Due to Spade’s tendency to accumulate er- rors over time, its density can become inflated. We will explain its limitations in practical applications in more detail in the case study. kCLIST vs PBBS vs Dupin. a) For 𝑘CLiDS, PBBS-𝑘CLiDS could not complete on most of our test datasets, so we did not compare their densities. kCLIST-𝑘CLiDS could not complete on the sk dataset. On the other datasets, Dupin-𝑘CLiDS’s density was only 6.97% smaller than kCLIST-𝑘CLiDS. On some datasets, such as bio and kron,Dupin𝑘CLiDS even detected subgraphs with greater density. b) For TDS, PBBS-TDS could detect subgraphs on smaller graphs but failed on billion-scale graphs such as rv, sk, and la. Dupin-TDS’s density was 2.03% greater than kCLIST-TDS. Therefore, we conclude that Dupin is comparable to existing parallel methods in terms of the density of the detected dense subgraphs. Ablation Study of Dupin. This experiment delves into the effectiveness of our optimizations, DupinGPO and DupinLPO. Our analysis primarily focuses on comparing the densities of dense subgraphs detected by both DupinGPO and DupinLPO in various settings. DupinGPO achieves the same density as Dupin across five den- sity metrics because it trims the long tail, enhancing efficiency. For DupinLPO-DG (resp. DupinLPO-DW, DupinLPO-FD, DupinLPOTDS, and DupinGPO-𝑘CLiDS), we observe that the detected sub- graphs exhibit densities 11.09% (resp. 7.32%, 8.00%, 1.01%, and 26.26%) greater than those detected by Dupin-DG, Dupin-DW, Dupin-FD, Dupin-TDS, andDupin-𝑘CLiDS, respectively, demonstrating notable outcomes. This is because DupinLPO trims the graph in each iteration, resulting in denser subgraphs compared to Dupin. Impact of the Approximation Ratio 𝜖. The approximation ratio 𝜖 serves to control accuracy. In our experiment, we vary 𝜖 from 0.1 to 1 to investigate its influence on the accuracy of Dupin. We observe that as 𝜖 increases from 0.1 to 1, the density of subgraphs detected by Dupin, DupinGPO, and DupinLPO generally decreases. ForDG, the densities ofDupin-DG,DupinGPO-DG, andDupinLPODG decrease by 22.71%, 22.71%, and 11.99%, respectively. The trends for DW and DG are similar, with the worst performance observed at 𝜖 = 0.6 for both density metrics. The trends for FD, TDS, and 𝑘CLiDS are also similar. DupinLPO detects denser subgraphs across most density metrics and is less affected by 𝜖 , due to its iterative trimming. For example, for FD, DupinLPO’s density decreases by only 6.23%, while DupinGPO and Dupin’s densities decrease by 17.37%.',\n",
       " '6.3 Case study': 'We also compare the performance in real-world scenarios, focusing on three key aspects: accuracy, latency, and prevention ratio. Accuracy. As discussed in Section 1, although Spade can quickly respond to new transactions, it tends to accumulate errors over time. Our analysis utilizes transaction data from Anon, with the transaction network comprising |𝐸 |= 2 billion edges. Under the incremental computation framework of Spade, the assumption is made that the weights of existing edges remain constant, thereby neglecting the impact of newly inserted edges on these weights, leading to error accumulation. For instance, using FD density metric, we observe that errors accumulate up to 24.4% within a day and increase to 30.3% over a week, as illustrated in Figure 9. Although Dupin sacrifices some accuracy to achieve parallel computational efficiency, our experiments show that Dupin’s errors remain relatively stable between 3% and 5%. Hence, the detection precision of Spade declines from 87.9% to 68.3%, while Dupin’s precision remains above 86%. Latency and Prevention Ratio. If a transaction is identified as fraudulent, subsequent related transactions are blocked to mitigate potential losses. We define the ratio of successfully prevented sus- picious transactions to the total number of suspicious transactions as R. As shown in Figure 6, the prevention ratio R continues to decrease as latency increases on Anon’s datasets. Using the default density metric, FD, in Anon, our results show thatDupin-FD can prevent 94.5% of fraudulent activities, GBBS-FD can prevent 3.0%, and Spade-FD can prevent 45.3%. The reason for the lower performance of GBBS-FD is its low parallelism in the peeling process, which results in long peeling times and thus an average latency of 6,014 seconds, delaying the prevention of many fraudulent activities. Although Spade-FD performs well in reordering techniques on smaller datasets, in industrial scenarios with billion-scale graphs, latency significantly worsens due to the extensive reordering range required in the peeling sequences as normal users transition to fraudsters, increasing the cost of incremental computations. Additional tests were conducted with other density metrics; for DG, Dupin-DG can prevent 78.8%, while other methods prevent varying amounts. Similarly, for DW, Dupin-DW can prevent 86.1%, while other methods demonstrate different prevention capabilities.',\n",
       " '7 CONCLUSION AND FUTUREWORKS': 'In this paper, we presented Dupin, a novel parallel fraud detection framework tailored for massive graphs. Our experiments demonstrated that Dupin’s advanced peeling algorithms and long-tail pruning techniques, DupinGPO and DupinLPO, significantly enhance the detection efficiency of dense subgraphs without sacrificing accuracy. Comparative studies with GBBS, PBBS, kCLIST, Spade, and Dupin highlighted Dupin’s superior performance in terms of com- putational efficiency and accuracy. Through case studies, we also validated that Dupin achieves lower latency and higher detection ac- curacy compared to existing fraud detection systems on billion-scale graphs. Overall, Dupinproves to be a powerful tool for large-scale fraud detection, providing a scalable and efficient solution for real-time applications. Future work will explore the integration of additional suspiciousness functions and further optimization to handle even larger datasets and more complex fraud scenarios.',\n",
       " 'REFERENCES': '[1] Distil networks: The 2019 bad bot report. https://www.bluecubesecurity.com/wp- content/uploads/bad-bot-report-2019LR.pdf. [2] B. Bahmani, R. Kumar, and S. Vassilvitskii. Densest subgraph in streaming and mapreduce. Proceedings of the VLDB Endowment, 5(5), 2012. [3] Y. Ban, X. Liu, T. Zhang, L. Huang, Y. Duan, X. Liu, and W. Xu. Badlink: Combining graph and information-theoretical features for online fraud group detection. arXiv preprint arXiv:1805.10053, 2018. [4] A. Beutel, W. Xu, V. Guruswami, C. Palow, and C. Faloutsos. Copycatch: stopping group attacks by spotting lockstep behavior in social networks. In Proceedings of the 22nd international conference on World Wide Web, pages 119–130, 2013. [5] D. Boob, Y. Gao, R. Peng, S. Sawlani, C. Tsourakakis, D.Wang, and J.Wang. Flowless: Extracting densest subgraphs without flow computations. In Proceedings of The Web Conference 2020, pages 573–583, 2020. [6] M. Charikar. Greedy approximation algorithms for finding dense components in a graph. In International Workshop on Approximation Algorithms for Combinatorial Optimization, pages 84–95. Springer, 2000. [7] M. Charikar. Greedy approximation algorithms for finding dense components in a graph. In Approximation Algorithms for Combinatorial Optimization, Third International Workshop, APPROX 2000, Saarbrücken, Germany, September 5-8, 2000, Proceedings, volume 1913 of Lecture Notes in Computer Science, pages 84–95. Springer, 2000. [8] C. Chekuri, K. Quanrud, and M. R. Torres. Densest subgraph: Supermodularity, iterative peeling, and flow. In Proceedings of the 2022 Annual ACM-SIAM Symposium on Discrete Algorithms (SODA), pages 1531–1555. SIAM, 2022. [9] J. Chen and Y. Saad. Dense subgraph extraction with application to community detection. IEEE Transactions on knowledge and data engineering, 24(7):1216–1230, 2010. [10] Y. Chen, J. Jiang, S. Sun, B. He, and M. Chen. Rush: Real-time burst subgraph detection in dynamic graphs. Proceedings of the VLDB Endowment, 17(11):3657– 3665, 2024. [11] N. Chiba and T. Nishizeki. Arboricity and subgraph listing algorithms. SIAM Journal on computing, 14(1):210–223, 1985. [12] L. Chu, Y. Zhang, Y. Yang, L. Wang, and J. Pei. Online density bursting subgraph detection from temporal graphs. Proceedings of the VLDB Endowment, 12(13):2353– 2365, 2019. [13] L. Dagum and R. Menon. Openmp: an industry standard api for shared-memory programming. IEEE computational science and engineering, 5(1):46–55, 1998. [14] M. Danisch, O. Balalau, and M. Sozio. Listing k-cliques in sparse real-world graphs. In Proceedings of the 2018 World Wide Web Conference, pages 589–598, 2018. [15] M. Danisch, T.-H. H. Chan, and M. Sozio. Large scale density-friendly graph decomposition via convex programming. In Proceedings of the 26th International Conference on World Wide Web, pages 233–242, 2017. [16] L. Dhulipala, J. Shi, T. Tseng, G. E. Blelloch, and J. Shun. The graph based benchmark suite (gbbs). In Proceedings of the 3rd Joint International Workshop on Graph Data Management Experiences & Systems (GRADES) and Network Data Analytics (NDA), pages 1–8, 2020. [17] Y. Dourisboure, F. Geraci, and M. Pellegrini. Extraction and classification of dense communities in the web. In Proceedings of the 16th international conference on World Wide Web, pages 461–470, 2007. [18] A. Epasto, S. Lattanzi, and M. Sozio. Efficient densest subgraph computation in evolving graphs. In Proceedings of the 24th international conference on world wide web, pages 300–310, 2015. [19] D. Gibson, R. Kumar, and A. Tomkins. Discovering large dense subgraphs in massive graphs. In Proceedings of the 31st international conference on Very large data bases, pages 721–732. Citeseer, 2005. [20] A. Gionis and C. E. Tsourakakis. Dense subgraph discovery: Kdd 2015 tutorial. In Proceedings of the 21th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, pages 2313–2314, 2015. [21] A. V. Goldberg. Finding a maximum density subgraph. 1984. [22] A. V. Goldberg and R. E. Tarjan. A new approach to the maximum-flow problem. Journal of the ACM (JACM), pages 921–940, 1988. [23] N. V. Gudapati, E. Malaguti, and M. Monaci. In search of dense subgraphs: How good is greedy peeling? Networks, 77(4):572–586, 2021. [24] B. Hooi, H. A. Song, A. Beutel, N. Shah, K. Shin, and C. Faloutsos. Fraudar: Bounding graph fraud in the face of camouflage. In Proceedings of the 22nd ACM SIGKDD international conference on knowledge discovery and data mining, pages 895–904, 2016. [25] M. Jaggi. Revisiting frank-wolfe: Projection-free sparse convex optimization. In International conference on machine learning, pages 427–435. PMLR, 2013. [26] J. Jiang, Y. Chen, B. He, M. Chen, and J. Chen. Spade+: A generic real-time fraud detection framework on dynamic graphs. IEEE Transactions on Knowledge and Data Engineering, 2024. [27] M. Jiang, P. Cui, A. Beutel, C. Faloutsos, and S. Yang. Inferring strange behavior from connectivity pattern in social networks. In Pacific-Asia conference on knowledge discovery and data mining, pages 126–138. Springer, 2014. [28] J. Jiaxin, L. Yuan, H. Bingsheng, H. Bryan, C. Jia, and J. K. Z. Kang. A real-time fraud detection framework on evolving graphs. PVLDB, 2023. [29] S. Kumar, W. L. Hamilton, J. Leskovec, and D. Jurafsky. Community interaction and conflict on the web. In Proceedings of the 2018 world wide web conference, pages 933–943, 2018. [30] V. E. Lee, N. Ruan, R. Jin, and C. Aggarwal. A survey of algorithms for dense subgraph discovery. In Managing and mining graph data, pages 303–336. Springer, 2010. [31] J. Leskovec and A. Krevl. SNAP Datasets: Stanford large network dataset collection. http://snap.stanford.edu/data, June 2014. [32] W. Luo, Z. Tang, Y. Fang, C. Ma, and X. Zhou. Scalable algorithms for densest subgraph discovery. In 2023 IEEE 39th International Conference on Data Engineering (ICDE), pages 287–300. IEEE, 2023. [33] C. Ma, Y. Fang, R. Cheng, L. V. Lakshmanan, and X. Han. A convex-programming approach for efficient directed densest subgraph discovery. In Proceedings of the 2022 International Conference on Management of Data, pages 845–859, 2022. [34] M. Mitzenmacher, J. Pachocki, R. Peng, C. Tsourakakis, and S. C. Xu. Scalable large near-clique detection in large-scale networks via sampling. In Proceedings of the 21th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, pages 815–824, 2015. [35] V. Podpečan, Ž. Ramšak, K. Gruden, H. Toivonen, and N. Lavrač. Interactive explo- ration of heterogeneous biological networks with biomine explorer. Bioinformatics, 06 2019. [36] H. Qin, R.-H. Li, Y. Yuan, G. Wang, L. Qin, and Z. Zhang. Mining bursting core in large temporal graphs. Proceedings of the VLDB Endowment, 2022. [37] Y. Ren, H. Zhu, J. Zhang, P. Dai, and L. Bo. Ensemfdet: An ensemble approach to fraud detection based on bipartite graph. In 2021 IEEE 37th International Conference on Data Engineering (ICDE), pages 2039–2044. IEEE, 2021. [38] R. A. Rossi and N. K. Ahmed. The network data repository with interactive graph analytics and visualization. In AAAI, 2015. [39] A. E. Sarıyüce andA. Pinar. Peeling bipartite networks for dense subgraph discovery. In Proceedings of the Eleventh ACM International Conference on Web Search and Data Mining, pages 504–512, 2018. [40] S. B. Seidman. Network structure and minimum degree. Social networks, 5(3):269– 287, 1983. [41] J. Shi, L. Dhulipala, and J. Shun. Parallel clique counting and peeling algorithms. In SIAM Conference on Applied and Computational Discrete Algorithms (ACDA21), pages 135–146. SIAM, 2021. [42] K. Shin, T. Eliassi-Rad, and C. Faloutsos. Corescope: Graph mining using k-core analysis—patterns, anomalies and algorithms. In 2016 IEEE 16th international conference on data mining (ICDM), pages 469–478. IEEE, 2016. [43] K. Shin, B. Hooi, J. Kim, and C. Faloutsos. Densealert: Incremental dense-subtensor detection in tensor streams. In Proceedings of the 23rd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, pages 1057–1066, 2017. [44] P. Sukprasert, Q. C. Liu, L. Dhulipala, and J. Shun. Practical parallel algorithms for near-optimal densest subgraphs on massive graphs. In 2024 Proceedings of the Symposium on Algorithm Engineering and Experiments (ALENEX), pages 59–73. SIAM, 2024. [45] B. Sun, M. Danisch, T. H. Chan, and M. Sozio. Kclist++: A simple algorithm for finding k-clique densest subgraphs in large graphs. Proceedings of the VLDB Endowment (PVLDB), 2020. [46] C. Tsourakakis. The k-clique densest subgraph problem. In Proceedings of the 24th international conference on world wide web, pages 1122–1132, 2015. [47] C. Ye, Y. Li, B. He, Z. Li, and J. Sun. Gpu-accelerated graph label propagation for real-time fraud detection. In Proceedings of the 2021 International Conference on Management of Data, pages 2348–2356, 2021.'}"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "parse_document(full_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def parse_document(text):\n",
    "    # Initialize variables\n",
    "    sections = {}\n",
    "    current_main_section = None\n",
    "    current_content = []\n",
    "\n",
    "    # Split text into lines\n",
    "    lines = text.split('\\n')\n",
    "\n",
    "    # Define heading patterns\n",
    "    main_heading_pattern = re.compile(r'^(\\d+)\\s+(.*)$')  # Matches headings like '1 INTRODUCTION'\n",
    "    sub_heading_pattern = re.compile(r'^(\\d+\\.\\d+)\\s+(.*)$')  # Matches headings like '2.1 Preliminary'\n",
    "    # Known headings (case-insensitive)\n",
    "    known_headings = {'Abstract', 'Acknowledgements', 'References', 'Conclusion', 'Introduction', 'Background', 'Related Work', 'Methodology', 'Results', 'Discussion', 'Appendix'}\n",
    "    known_headings_lower = [h.lower() for h in known_headings]\n",
    "\n",
    "    for idx, line in enumerate(lines):\n",
    "        line = line.strip()\n",
    "        if not line:\n",
    "            continue  # Skip empty lines\n",
    "\n",
    "        # Check for main numbered heading\n",
    "        m_main = main_heading_pattern.match(line)\n",
    "        if m_main and '.' not in m_main.group(1):\n",
    "            # Save current section\n",
    "            if current_main_section is not None:\n",
    "                content = ' '.join(current_content).strip()\n",
    "                sections[current_main_section] = content\n",
    "            # Start new main section\n",
    "            section_number = m_main.group(1)\n",
    "            section_title = m_main.group(2).strip()\n",
    "            current_main_section = f\"{section_number} {section_title}\"\n",
    "            current_content = []\n",
    "            continue\n",
    "\n",
    "        # Check for unnumbered heading (known headings)\n",
    "        if line.lower() in known_headings_lower:\n",
    "            # Save current section\n",
    "            if current_main_section is not None:\n",
    "                content = ' '.join(current_content).strip()\n",
    "                sections[current_main_section] = content\n",
    "            # Start new main section\n",
    "            current_main_section = line.strip()\n",
    "            current_content = []\n",
    "            continue\n",
    "\n",
    "        # For subheadings and other content\n",
    "        # We include subheadings in the content of the current main section\n",
    "        # So we can check for subheadings and append them to content\n",
    "        m_sub = sub_heading_pattern.match(line)\n",
    "        if m_sub:\n",
    "            # It's a subheading\n",
    "            sub_section_number = m_sub.group(1)\n",
    "            sub_section_title = m_sub.group(2).strip()\n",
    "            current_content.append(f\"{sub_section_number} {sub_section_title}\")\n",
    "            continue\n",
    "\n",
    "        # Append line to current content\n",
    "        if current_main_section is None:\n",
    "            # If no current section, start with 'Abstract'\n",
    "            current_main_section = 'Abstract'\n",
    "        current_content.append(line)\n",
    "\n",
    "    # Save the last section\n",
    "    if current_main_section is not None:\n",
    "        content = ' '.join(current_content).strip()\n",
    "        sections[current_main_section] = content\n",
    "    return sections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_temp_text = re.sub(r'\\n+', ' ', full_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Abstract': 'ABSTRACT Detecting fraudulent activities in financial and e-commerce transac- tion networks is crucial, and one effective method for this is Densest Subgraph Discovery (DSD). However, deploying DSD methods in production systems faces substantial scalability challenges due to the predominantly sequential nature of existing methods, which impedes their ability to handle large-scale transaction networks and results in significant detection delays. In this paper, we introduce Dupin, a novel parallel processing framework designed for efficient DSD processing in billion-scale graphs. Dupin is powered by a processing engine that exploits the unique properties of the peeling process, with theoretical guarantees on detection quality and efficiency.Dupin provides user-friendly APIs for flexible customization of DSD objectives and ensures robust adaptability to diverse fraud detection scenarios. Empirical evaluations indicate that Dupin outperforms several existing DSD methods, with performance improvements observed in specific scenarios reaching up to two orders of magnitude. On billion-scale graphs, Dupin demonstrates the potential to enhance the prevention of fraudulent transactions by approximately 49.5 basis points and reduce density error from 30% to below 5%, as supported by our experimental results. ACM Reference Format: Anonymous Author(s). 2024. Dupin: A Parallel Densest Subgraph Discovery Framework on Massive Graphs. In Proceedings of ACM Conference (Conference’17). ACM, New York, NY, USA, 14 pages. https://doi.org/10.1145/ nnnnnnn.nnnnnnn',\n",
       " '1 INTRODUCTION': 'Graph structures pervade numerous contemporary applications, ranging from transaction and communication networks to expan- sive social media platforms. The study of densest subgraph discovery (DSD), first explored by [21], has since proven instrumental in various domains: link spam identification [4, 19], community de- tection [9, 17], and notably, fraud detection [8, 24, 42]. Central to this domain is the peeling process [2, 5, 8, 24, 46], which operates by iteratively removing vertices based on density metrics, such as vertex degree or the cumulative weight of adjacent edges. Their widespread adoption can be attributed to their inherent efficiency, robustness, and proven worst-case performance guarantees for DSD. In practice, business requirements demand the detection of fraud- ulent communities on billion-scale graphs within seconds. Recent Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for components of this work owned by others than ACM must be honored. Abstracting with credit is permitted. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. Request permissions from permissions@acm.org. Conference’17, July 2017, Washington, DC, USA © 2024 Association for Computing Machinery. ACM ISBN 978-x-xxxx-xxxx-x/YY/MM. . . $15.00 https://doi.org/10.1145/nnnnnnn.nnnnnnn studies, including those by [1] and [47], have highlighted the perva- sive issue of malicious bot activity in e-commerce, accounting for a substantial 21.4% of all traffic in 2018. These fraudulent activities often happen quickly, making them hard to detect. The inability to efficiently process and analyze such massive graphs not only ham- pers fraud detection efforts but also exposes businesses to substan- tial financial risks and reputational damage. Therefore, addressing these challenges is not merely an academic exercise; it is a press- ing necessity for enhancing the integrity of digital transactions and safeguarding consumer trust. Despite extensive research onDSD for static graphs [20–22, 30, 33, 34], dynamic graphs [2, 18, 26, 28, 43], and parallel approaches [14, 16, 41, 42], existing methods face significant challenges for practical de- ployment in real-world fraud detection systems. For instance, many existing algorithms primarily rely on static score functions for DSD, lacking the flexibility required for dynamic and evolving fraud detec- tion scenarios. Methods such as [4] and [27] are designed with fixed detection criteria, which can become less effective as fraudulent be- haviors adapt over time. Additionally, incremental algorithms remain inefficient at handling billion-scale graphs with high transaction volumes, leading to increased latency. Parallel DSD frameworks often lack efficient pruning techniques, resulting in excessive iterations that degrade performance on large-scale graphs. This paper addresses the following research question: How can we enhance the efficiency and scalability of DSD methods to effectively detect fraud in massive graphs? Challenges. We evaluated existing methods with our industry partner, Anon (anonymized for review). The following issues surfaced: Firstly, existing algorithms focus on static score functions for DSD and lack flexible customization. While this may be adequate for short-term detection, it becomes less effective over time. For example, as shown in Figure 1a, graph density can accumulate sig- nificant errors within a single day, highlighting the need for more adaptive and customizable detection methods. Secondly, existing incremental algorithms are still inefficient at handling billion-scale graphs with high transaction volumes, leading to increased latency. This limitation is evident in Figure 1b, which illustrates the system load distribution for a specific day’s transactions, underscoring the inability of current methods to efficiently manage high transaction volumes without significant delays. Thirdly, existing parallel densest subgraph discovery frameworks lack efficient pruning techniques, re- sulting in a large number of iterations. This inefficiency is especially problematic on billion-scale graphs, where performance degrades substantially due to the excessive computational overhead caused by numerous iterations. Adapting to Evolving Research Needs. Recognizing the dynamic nature of fraud detection and the rapid evolution of fraudulent tactics, we have adapted our research focus to address these challenges. Our approach incorporates flexible customization options for DSD methods, allowing for adjustments based on the changing landscape of fraudulent activities. This adaptability is crucial for maintaining the effectiveness of fraud detection systems in an environment where new threats continuously emerge. Contributions. In this work, we propose a DSD framework for fraud detection. Our key contributions are as follows: • A DSD Parallelization Framework. We propose Dupin, a novel parallel processing framework for densest subgraph discovery (DSD) that breaks the sequential dependencies inherent in tra- ditional density metrics, significantly enhancing scalability and efficiency on both weighted and unweighted graphs. Unlike existing methods that focus on specific density metrics, Dupin allows developers to define custom fraud detection semantics through flexible suspiciousness functions for edges and vertices. It supports a wide range of DSD semantics, including DG [6], DW [23], FD [24], TDS [46], and 𝑘CLiDS [14]. Importantly, Dupin not only leverages parallelism but also provides theoretical guarantees on approximation ratios for the supported density metrics. To the best of our knowledge, this is the first generic parallel processing framework that accommodates a broad class of density metrics in DSD, offering both flexibility and theoretical assurance. • Exploiting Long-Tail Properties.We identify and address the long-tail problem in the peeling process of DSD algorithms, where late iterations remove only a small fraction of vertices and con- tribute little to the quality of the densest subgraph, leading to inefficiency. To tackle this, we introduce two novel optimizations: Global Pruning Optimization, which proactively removes low-contribution vertices within each peeling iteration to reduce computational overhead, and Local Pruning Optimization, which eliminates disparate structures formed after vertex removal to enhance the density and quality of the detected subgraph. These optimizations significantly reduce the number of iterations re- quired, improving both performance and result quality. • Empirical Validation and Real-World Impact: We conduct comprehensive experiments on large-scale industry datasets to validate the effectiveness of Dupin. Our results show that Dupin achieves up to 100 times faster fraud detection compared to the state-of-the-art incremental method Spade [28] and parallel methods like PBBS [41]. Moreover,Dupin is capable of preventing up to 94.5% of potential fraud, demonstrating significant practical bene- fits and reinforcing system integrity in real-world applications. Organization. The remainder of this paper is structured as follows: Section 2 presents the background and the literature review. Section 3 introduces the architecture of Dupin. In Section 4, we propose the theoretical foundations for parallel peeling-based algorithms for DSD. Section 5 introduces long-tail pruning techniques to improve efficiency and effectiveness. The experimental evaluation of Dupin is presented in Section 6. Finally, Section 7 concludes the paper and outlines avenues for future research.',\n",
       " '2 BACKGROUND AND RELATEDWORK': '2.1 Preliminary This subsection presents the preliminary. Some frequently used no- tations are summarized in Table 1. Graph 𝐺 . In this work, we operate on a weighted graph denoted as 𝐺 = (𝑉 , 𝐸), where 𝑉 represents the set of vertices and 𝐸 (⊆ 𝑉 ×𝑉 ) represents the set of edges. Each edge (𝑢𝑖 , 𝑢 𝑗 ) ∈ 𝐸 is assigned a nonnegative weight, expressed as 𝑐𝑖 𝑗 . Additionally, 𝑁 (𝑢) denotes the set of neighboring vertices connected to vertex 𝑢. Induced Subgraph. For a given subset 𝑆 of vertices in 𝑉 , we define the induced subgraph as 𝐺[𝑆] = (𝑆, 𝐸[𝑆]), where 𝐸[𝑆] = {(𝑢, 𝑣) | (𝑢, 𝑣) ∈ 𝐸 ∧ 𝑢, 𝑣 ∈ 𝑆}. The size of the subset 𝑆 is denoted by |𝑆 |. Density Metrics 𝑔 and Weight Function 𝑓 . We utilize the class of metrics𝑔 as defined in prior works [6, 24, 28], where for a given subset of vertices 𝑆 ⊆ 𝑉 , the density metric is computed as: 𝑔(𝑆) = 𝑓 (𝑆) |𝑆 | . Here, 𝑓 (𝑆) represents the total weight of the induced subgraph 𝐺[𝑆], which is defined as the sum of the weights of vertices in 𝑆 and the weights of edges in 𝐸[𝑆]. More formally: 𝑓 (𝑆) = ∑︁ 𝑢𝑖 ∈𝑆 𝑎𝑖 + ∑︁ (𝑢𝑖 ,𝑢 𝑗 )∈𝐸[𝑆] 𝑐𝑖 𝑗 (1) The weight of a vertex 𝑢𝑖 , denoted as 𝑎𝑖 (𝑎𝑖 ≥ 0), quantifies the suspiciousness of user 𝑢𝑖 . The weight of an edge (𝑢𝑖 , 𝑢 𝑗 ), represented by 𝑐𝑖 𝑗 (𝑐𝑖 𝑗 > 0), reflects the suspiciousness of the transaction between users 𝑢𝑖 and 𝑢 𝑗 . Intuitively, the density metric 𝑔(𝑆) encapsulates the overall suspiciousness density of the induced subgraph𝐺[𝑆]. A larger value of 𝑔(𝑆) indicates a higher density of suspiciousness within𝐺[𝑆]. The vertex set that maximizes the density metric 𝑔 is denoted by 𝑆∗. We next introduce five representative density metrics extensively employed in many fraud detection applications. The density metrics are mainly differed by how the weight function is defined. Dense Subgraphs (DG) [6]. The DG metric is employed to quan- tify the connectivity of substructures within a graph. It has found extensive applications in various domains, such as detecting fake comments in online platforms [29] and identifying fraudulent activities in social networks [3]. Given a subset of vertices 𝑆 ⊆ 𝑉 , the weight function of DG is defined as 𝑓 (𝑆) = |𝐸[𝑆]|, where |𝐸[𝑆]| denotes the number of edges in the induced subgraph 𝐺[𝑆]. Dense Subgraph on Weighted Graphs (DW) [23]. In transac- tion graphs, it is common to have weights assigned to the edges, representing quantities such as the transaction amount. The DW metric extends the concept of dense subgraphs to accommodate these weighted relationships. Given a subset of vertices 𝑆 ⊆ 𝑉 , the weight function of DW is defined as 𝑓 (𝑆) = ∑ (𝑢𝑖 ,𝑢 𝑗 )∈𝐸[𝑆] 𝑐𝑖 𝑗 . Fraudar (FD) [24]. In order to mitigate the effects of fraudulent actors deliberately obfuscating their activities, Hooi et al. [24] introduced the FD algorithm. This approach innovatively incorporates weights for edges and assigns prior suspiciousness scores to vertices, potentially utilizing side information to enhance detection accuracy. Given a subset of vertices 𝑆 ⊆ 𝑉 , the weight function of FD is defined as Equation 1 where 𝑎𝑖 denotes the suspicious score of vertex 𝑢𝑖 and 𝑐𝑖 𝑗 = 1 log(𝑥+𝑐) . Here 𝑥 is the degree of the object vertex between 𝑢𝑖 and 𝑢 𝑗 , and 𝑐 is a positive constant [24]. Triangle Densest Subgraph (TDS) [46]. Triangles serve as a crucial structural motif in graphs, providing valuable insights into the connectivity and cohesiveness of subgraphs. The TDS metric, in par- ticular, leverages the prevalence of triangles to quantify the density of a subgraph. Given a subset of vertices 𝑆 ⊆ 𝑉 , the weight function of TDS is defined as 𝑓 (𝑆) = 𝑡 (𝑆) where 𝑡 (𝑆) represents the total number of triangles within the induced subgraph 𝐺[𝑆]. 𝑘-Clique Densest Subgraph (𝑘CLiDS) [14]. Extending the concept of triangle densest subgraphs, 𝑘CLiDS focuses on identifying dense subgraphs based on the presence of 𝑘-Cliques. A 𝑘-Clique is a complete subgraph of 𝑘 vertices, representing a tightly-knit group of vertices. For a given subset of vertices 𝑆 ⊆ 𝑉 , the weight function of 𝑘CLiDS is defined as 𝑓 (𝑆) = 𝑘(𝑆), where 𝑘(𝑆) denotes the number of 𝑘-Cliques within the induced subgraph 𝐺[𝑆]. 2.2 Sequential Peeling Algorithms Peeling algorithms have gained popularity for their efficiency, ro- bustness, and proven worst-case performance in various applica- tions [6, 24, 46]. Next, we provide a concise overview of the typical execution flow of these algorithms. Peeling Weight.We use𝑤𝑢𝑖 (𝑆) to indicate the decrease in the value of the weight function 𝑓 when the vertex𝑢𝑖 is removed from a vertex set 𝑆 , i.e., the peeling weight. Formally, 𝑤𝑢𝑖 (𝑆) = 𝑎𝑖 + ∑︁ (𝑢 𝑗 ∈𝑆) ∧ ((𝑢𝑖 ,𝑢 𝑗 )∈𝐸) 𝑐𝑖 𝑗 + ∑︁ (𝑢 𝑗 ∈𝑆) ∧ ((𝑢 𝑗 ,𝑢𝑖 )∈𝐸) 𝑐 𝑗𝑖 (2) Algorithm 1: Sequential Peeling Algorithm Input: A graph𝐺 = (𝑉 , 𝐸) and a density metric 𝑔(𝑆) Output: A subset of vertices 𝑆 that maximizes the density metric 𝑔(𝑆) 1 𝑆0 ← 𝑉 /* Initialize the subset with all vertices */ 2 for 𝑖 ← 1 to |𝑉 | do 3 𝑢 ← arg max𝑣∈𝑆𝑖−1 𝑔(𝑆𝑖−1 \\\\ {𝑣 }) /* The vertex to be peeled */ 4 𝑆𝑖 ← 𝑆𝑖−1 \\\\ {𝑢 } /* The selected vertex */ 5 return arg max𝑆𝑖 𝑔(𝑆𝑖 ) /* The subset that maximizes 𝑔(𝑆) */ Sequential PeelingAlgorithm (Algorithm1).The peeling process starts with the full vertex set 𝑆0 = 𝑉 (Line 1) and iteratively removes vertices to maximize the density metric 𝑔. In each step 𝑖 , a vertex𝑢𝑖 is removed from 𝑆𝑖−1 to form 𝑆𝑖 , such that 𝑔(𝑆𝑖−1 \\\\ {𝑢𝑖 }) is maximized (Lines 3∼4). This process is repeated until 𝑆𝑖 becomes empty, resulting in a sequence of nested sets 𝑆0, 𝑆1, . . . , 𝑆 |𝑉 | . The algorithm ultimately returns the subset 𝑆𝑖 ∈ {𝑆0, . . . , 𝑆 |𝑉 |} that maximizes 𝑔(𝑆𝑖 ). Example 2.1 (Seqential Peeling Algorithm Process). Consider the graph 𝐺 as shown in Figure 2. The process begins with an initial graph density of 2.33. In the first iteration, vertex 𝑢1 is peeled due to its smallest peeling weight among all vertices. Subsequently, vertices are peeled in the following order: 𝑢2, 𝑢3, 𝑢4, 𝑢5, and 𝑢6 based on the updated criteria after each peeling operation. Notably, after peeling 𝑢2, the graph density increases, which is a characteristic observation in the peeling process. The sequence of peeling continues until the last vertex, 𝑢6, is peeled, resulting in a final graph density of 0. The peeling sequence, therefore, is 𝑂 = [𝑢1, 𝑢2, 𝑢3, 𝑢4, 𝑢5, 𝑢6], completely disassembling the graph. The induced subgraph𝐺[𝑆] of 𝑆 = [𝑢3, 𝑢4, 𝑢5, 𝑢6] has the greatest density of 2.75, thus 𝐺[𝑆] is returned. Approximation. An algorithm 𝑄 is defined as an 𝛼-approximation for the Densest Subgraph Discovery (DSD), where 𝛼 ≥ 1, if for any given graph it returns a subset 𝑆 satisfying the condition: 𝑔(𝑆) ≥ 𝑔(𝑆∗) 𝛼 , where 𝑆∗ is the optimal subset that maximizes the density metric 𝑔. Theorem 2.1 ([24]). For the vertex set 𝑆𝑝 returned by Algorithm 1 and the optimal vertex set 𝑆∗, it holds that 𝑔(𝑆𝑝 ) ≥ 𝑔(𝑆∗) 2 for DG, DW and FD as the density metrics. We extend the analysis to TDS and 𝑘CLiDS. The proof of the following theorem is omitted due to space limitation. Theorem 2.2. For the vertex set 𝑆𝑝 returned by Algorithm 1 and the optimal vertex set 𝑆∗, it holds that 𝑔(𝑆𝑝 ) ≥ 𝑔(𝑆∗) 𝑘 for TDS and 𝑘CLiDS as the density metrics where 𝑘 = 3 for TDS. 2.3 Related work Densest Subgraph Discovery (DSD). Densest subgraph discov- ery represents a core area of research in graph theory, extensively explored in literature [7, 8, 20, 30, 39, 40]. The foundational max-flow- based algorithm by Goldberg et al. [21] set a benchmark for exact densest subgraph detection, with subsequent studies introducing more scalable exact solutions [33, 34]. While speed enhancements have been achieved through various greedy approaches [6, 8, 24, 33], these methods are inherently sequential and difficult to parallelize. For example, they often peel one vertex at a time, which results in too many peeling iterations. This sequential nature limits their scalability and efficiency on massive graphs. In graph-based fraud de- tection, techniques like COPYCATCH [4] and GETTHESCOOP [27] leverage local search heuristics to identify dense subgraphs in bi- partite networks, frequently applied in fraud, spam, and community detection across social and review platforms [24, 37, 42]. Despite their widespread use, these methods suffer from efficiency issues in very large graphs, often involving numerous iterations and lack- ing effective pruning techniques, which exacerbates the long-tail problem. Moreover, they are limited to specific definitions of dense subgraphs. Contrarily, Dupin offers a versatile, parallel approach to densest subgraph detection. It facilitates the simultaneous peeling of multiple vertices, drastically reducing the computational overhead. Additionally, Dupin provides APIs that allow users to customize defi- nitions of dense subgraphs, ensuring that the resultant graph density adheres to theoretical bounds. DSD on Dynamic Graphs. To tackle real-time fraud detection, sev- eral variants have been designed for dynamic graphs [10, 18, 26, 28, 43]. [28] addresses real-time fraud community detection on evolving graphs (edge insertions) using an incremental peeling sequence re- ordering method, while [26] extends this approach to fully dynamic graphs (edge insertions and deletions). [43] proposes an incremental algorithm that maintains and updates a dense subtensor in a ten- sor stream. [12] and [36] utilize sliding windows to detect dense subgraphs and bursting cores within specific time windows. These methods have three limitations. First, response time is an issue. Al- though they respond quickly to benign communities, the results can change drastically for newly formed fraudulent communities, caus- ing significant delays in incremental calculations for fraud detection. Second, efficiency is problematic. While incremental processing is faster than recomputation for a single transaction on billion-scale graphs, frequent incremental evaluations can be slower than a full recomputation. Third, in terms of flexibility, most methods support only a single semantic. In contrast, Dupin introduces a unified parallel framework for peeling-based dense subgraph detection.Dupin can respond within seconds on billion-scale graphs and defines multiple density metrics that can be used on the Dupin platform. Parallel DSD. Several parallel methods for DSD have been pro- posed [2, 14, 16, 39, 41, 42]. Bahmani et al. [2] proposed a parallel algorithm for DG using the MapReduce framework. FWA [15] proposed a parallel algorithm for DW based on Frank-Wolfe algo- rithm [25] using OpenMP [13]. Some works propose parallel 𝑘-core algorithms [32, 41, 44]. Others introduce parallel algorithms for 𝑘- Clique detection, with approaches offering a (1 + 𝜖)-approximation ratio and parallel implementations for the 𝑘CLiDS problem [16, 45]. There are also algorithms for 𝑘-Clique densest subgraph detection using OpenMP [14], and methods for dense subtensor detection [42]. However, these systems have two main limitations. First, they typi- cally support only a single-densitymetric and do not handle weighted graphs. In real-world applications, transactions between users can have varying degrees of importance based on amounts, frequen- cies [28], and other factors. Extending these systems to support other dense metrics with theoretical guarantees is non-trivial. Second, they often lack efficient pruning techniques, requiring numerous itera- tions and suffering from the long-tail issue, which hinders scalability on massive graphs. In contrast, Dupin defines the characteristics of a set of supported dense metrics and accommodates weighted sub- graphs, allowing for more nuanced analysis reflective of real-world data. Our method introduces efficient pruning strategies by peeling multiple vertices in parallel based on a threshold mechanism, signifi- cantly reducing the number of iterations and mitigating the long-tail problem. This makes Dupin more effective and scalable in diverse real-world scenarios compared to previous approaches. 3 THE Dupin FRAMEWORK 3.1 Motivations A notable example of advanced fraud detection is seen in Anon, a prominent technology company in Southeast Asia known for its digital payment and food delivery services. As delineated in [28], the detection process begins with forming a transaction graph,𝐺 , derived from the platform’s transactions. This graph is incrementally updated, represented as 𝐺 = 𝐺 ⊕ ∆𝐺 . Subsequent stages utilize densest subgraph discovery algorithms to uncover fraudulent communities. A typical fraud scenario on the platform involves customer-merchant collusion, where fake accounts are created to exploit promotional incentives. These fraudulent activities manifest as densest subgraphs within the transaction graph. Efficiency. Our empirical findings reveal that the incremental execution of DSD algorithms, such as Spade [28], on a graph with 2 billion edges requires approximately 198 seconds per batch (default setting: one batch of 1K edge insertions). However, in practical business sce- narios, the number of incoming edges can peak at 400K per minute. Clearly, using incremental evaluation on billion-scale graphs does not provide sufficient throughput to meet business requirements. Latency and Prevention Ratio. Existing DSD systems face sig- nificant challenges in terms of latency and prevention ratio. Upon identification of fraudulent activities, actions such as account bans or freezes are implemented to prevent further losses. However, the aver- age latency for these systems to detect fraud and take action is often high, typically around 200 to 300 seconds for incremental algorithms and more than 1000 seconds for existing parallel algorithms, which is insufficient for real-time requirements (operational demand requires detection within seconds). Moreover, when the graph size increases from million-scale to billion-scale, the prevention ratio decreases from 92.47% to 45%. These issues underscore the necessity for more efficient and scalable algorithms that can maintain low latency and high prevention ratios, even under heavy transaction loads. Parallelization offers a viable solution to these challenges by dis- tributing the computational load across multiple processors. Mod- ern CPUs are equipped with numerous cores that are cost-effective, enabling parallel processing. If the peeling process of DSD is paral- lelized, different processors can simultaneously evaluate and peel different nodes, thereby increasing scalability and efficiency. For example, if we can improve processing speed by one to two orders of magnitude, the system would be able to meet industry requirements for real-time fraud detection, completing detection within seconds. Problem Statement. Given a graph 𝐺 = (𝑉 , 𝐸) and a community detection semantic with density metric 𝑔(𝑆), our goal is to find a vertex subset 𝑆∗ ⊆ 𝑉 that maximizes 𝑔(𝑆∗). 3.2 System Architecture To enable users to design complex DSD algorithms to detect fraudsters based on our framework, Dupin, we have introduced the archi- tecture, APIs, and supported density metrics in this section. Architecture of Dupin. Figure 3 presents the architecture of Dupin with user-defined functions for detecting suspicious activities.Dupin enables users to define their fraud detection semantics through a set of provided APIs. Within Dupin’s computational engine, we sup- port both sequential and parallel peeling algorithms. Additionally, algorithms for long-tail pruning techniques are implemented to ac- celerate convergence and enhance the density of subgraphs in each iteration. Dupin identifies fraudulent communities, which are then flagged for further investigation by moderators. APIs of Dupin (Figure 3). Dupin simplifies the process of defining detection semantics by allowing users to specify a few simple functions. Additionally, Dupin offers APIs that enable users to balance detection speed and accuracy effectively. The key APIs encompass several components designed to facilitate the customization and op- timization of fraud detection processes. These components provide a robust framework for users to implement their fraud detection strate- gies, ensuring both high performance and precision in identifying fraudulent activities. Details are listed below: • pWeight and updateNgh. These functions are designed for the dynamic adjustment of weights within the network. They allow the user to define the peeling weight of a vertex and specify the method for updating the peeling weight of neighboring vertices after a vertex has been peeled. • VSusp and ESusp. Given a vertex/edge, these components are re- sponsible for deciding the suspiciousness of the endpoint of the edge or the edge with a user-defined strategy. • isBenign. This component is used to decide whether a vertex is benign during the whole peeling process (Section 5). If the vertex is benign, it is peeled within the current iteration. • setEpsilon. This function is utilized to control the precision of fraud detection. During periods of high system load, moderators can increase 𝜖 to achieve higher throughput. Conversely, when the system load is low, 𝜖 can be decreased to enhance accuracy. • setK. This function specifies the 𝑘-Clique parameter, enabling the definition of the 𝑘-Clique for analysis. 1 double vsusp(const Vertex& v, const Graph& g) { 2 return g.weight[v]; // Side information on vertex 3 } 4 double esusp(const Edge& e, const Graph& g) { 5 return 1.0 / log(g.deg[e.src] + 5.0); 6 } 7 int main() { 8 Dupin dupin; 9 dupin.VSusp(vsusp); // Plug in vsusp 10 dupin.ESusp(esusp); // Plug in esusp 11 dupin.setEpsilon (0.1); 12 dupin.LoadGraph(\"graph_sample_path\"); 13 vector <Vertex > fraudsters = dupin.ParDetect (); 14 return 0; 15 } Listing 1: Implementation of FD on Dupin Example (Listing 1). To implement FD [24] on Dupin, users simply need to plug in the suspiciousness function vsusp for the vertices by calling VSusp and the suspiciousness function esusp for the edges by calling ESusp. Specifically, 1) vsusp is a constant function, i.e., given a vertex 𝑢, vsusp(𝑢) = 𝑎𝑖 and 2) esusp is a logarithmic function such that given an edge (𝑢𝑖 , 𝑢 𝑗 ), esusp(𝑢𝑖 , 𝑢 𝑗 ) = 1 log(𝑥+𝑐) , where 𝑥 is the degree of the object vertex between 𝑢𝑖 and 𝑢 𝑗 , and 𝑐 is a positive constant [24]. Developers can easily implement customized peeling algorithms with Dupin, significantly reducing the engineering effort. Density Metrics Support in Dupin. To illustrate the versatility of Dupin in handling various density metrics, we establish comprehensive sufficient conditions. These conditions ensure Dupin’s adaptability in integrating a wide range of metrics to assess graph density effectively. This foundational approach enablesDupin to sup- port diverse density calculations, crucial for nuanced fraud detection and network analysis tasks. Property 3.1. Dupin supports a density metric 𝑔(𝑆) if the following conditions are met: (1) 𝑔(𝑆) = 𝑓 (𝑆) |𝑆 | , where 𝑓 is a monotone increasing function. (2) 𝑎𝑖 ≥ 0, ensuring non-negative vertex weight function. (3) 𝑐𝑖 𝑗 ≥ 0, ensuring non-negative edge weight function.',\n",
       " '4 PARALLELABLE PEELING ALGORITHMS': 'For sequential peeling algorithms, only one vertex can be removed at a time, with each vertex’s removal depending on the removal of previous vertices. This dependency prevents the algorithm from executing multiple vertex removals simultaneously. In this section, we introduce a parallel peeling paradigm to break this dependency chain. This new approach allowsmultiple vertices to be peeled in parallel without affecting the programming abstractions that Dupin provides to end users. We then demonstrate how this paradigm offers fine-grained trade-offs between parallelism and ap- proximation guarantees for all density metrics studied in this work. 4.1 Parallel Peeling Paradigm Instead of peeling the vertex with the maximum peeling weight, we can peel all vertices that lead to a significant decrease in the density scores of the remaining graph. This decrease is controlled by a tunable parameter 𝜖 . A larger 𝜖 allows more vertices to be peeled in parallel, while a smaller 𝜖 provides a better approximation ratio. Algorithm 2: Dupin: Parallel Peeling Input:𝐺 = (𝑉 , 𝐸), a density metric 𝑔(𝑆), and 𝜖 ≥ 0 Output: A subset of vertices 𝑆 that maximizes the density metric 𝑔(𝑆) 1 𝑆0 ← 𝑉 /* Initialize the subset with all vertices */ 2 𝑖 ← 1 /* Initialize the index */ 3 while 𝑆𝑖−1 ̸= ∅ do 4 𝜏 = 𝑘(1 + 𝜖) · 𝑔(𝑆𝑖−1) /* The threshold for peeling */ 5 𝑈 ← {𝑢 | 𝑤𝑢 (𝑆𝑖−1) ≤ 𝜏 } /* Vertices to be peeled */ 6 𝑆𝑖 ← 𝑆𝑖−1 \\\\𝑈 /* Update the subset */ 7 𝑖 ← 𝑖 + 1 /* Increment the index */ 8 return arg max𝑆𝑖 𝑔(𝑆𝑖 ) /* The subset that maximizes 𝑔(𝑆) */ Parallel Peeling (Algorithm 2). Let 𝑆𝑖 denote the vertex set after the 𝑖-th peeling iteration. Initially, 𝑆0 = 𝑉 (Line 1). In each iteration, it removes a subset of vertices𝑈 from 𝑆𝑖−1, where ∀𝑢 ∈ 𝑈 ,𝑤𝑢 (𝑆𝑖−1) ≤ 𝑘(1 + 𝜖)𝑔(𝑆𝑖−1). Parameter 𝑘 is determined by the metric used. 𝑘 = 2 works for DG, DW and FD. 𝑘 = 3 works for TDS and 𝑘 is the size of the cliques for 𝑘CLiDS. This process is repeated recursively until no vertices are left, resulting in a series of vertex sets 𝑆0, . . . , 𝑆𝑅 , where 𝑅 is the number of iterations. The algorithm then returns the set 𝑆𝑖 (𝑖 ∈ [0, 𝑅]) that maximizes the density metric 𝑔(𝑆𝑖 ), denoted as 𝑆𝑝 . In what follows, we also show a case where 𝑘CLiDS is the density metric for DSD. 3 3 6 6 5 4 21 0 00 3 3 6 6 3 3 0 0 Density: 0.91 Density: 1.33 Density: 0 Iteration i Iteration i+1 Iteration i+2 Iteration i+3 Density: 0 τ = 3× 0.91 = 2.73 τ = 3× 1.33 = 3.99 τ = 0 τ = 0 4.2 Theoretical Analysis In this section, we analyze the theoretical properties of the parallel peeling process. We begin by examining the number of iterations required for parallel peeling. Built upon the result, we then determine the time complexity and the approximation ratio of the parallel peeling process. Lemma 4.1. Given any 𝜖 > 0, 𝑅 < log 1+𝜖 |𝑉 |. Proof. We prove that for any 𝜖 ≥ 0, the size of the remaining vertex set is bounded by |𝑆𝑖 |< 1 1+𝜖 |𝑆𝑖−1 | for all 𝑖 ∈ [1, 𝑅]. We first examine the peeling weights of the vertices in 𝑆𝑖−1 for the density metrics DG. DW and FD where 𝑘 = 2. As each edge weight in the peeling weight calculation contributes twice (once for each endpoint), and the vertex weights contribute once, we have∑︁ 𝑢∈𝑆𝑖−1 𝑤𝑢 (𝑆𝑖−1) ≤ 2𝑓 (𝑆𝑖−1), (3) The peeling weights of the vertices in 𝑆𝑖−1 are calculated as follows:∑︁ 𝑢∈𝑆𝑖−1 𝑤𝑢 (𝑆𝑖−1) = ∑︁ 𝑢∈𝑆𝑖−1\\\\𝑆𝑖 𝑤𝑢 (𝑆𝑖−1) + ∑︁ 𝑢∈𝑆𝑖 𝑤𝑢 (𝑆𝑖−1) ≥ ∑︁ 𝑢∈𝑆𝑖 𝑤𝑢 (𝑆𝑖−1) > 2(1 + 𝜖)|𝑆𝑖 |𝑔(𝑆𝑖−1) (4) Combining Equation 3 and Equation 4, we have: 2(1 + 𝜖)|𝑆𝑖 |𝑔(𝑆𝑖−1) < 2𝑓 (𝑆𝑖−1) = 2|𝑆𝑖−1 |𝑔(𝑆𝑖−1) (5) For all 𝑖 ∈ [1, 𝑅], |𝑆𝑖 |< 1 1 + 𝜖 |𝑆𝑖−1 | (6) Then we have: |𝑉 |= |𝑆0 |> 1 1 + 𝜖 |𝑆1 |> . . . > ( 1 1 + 𝜖 ) 𝑅 |𝑆𝑅 | (7) Therefore, we have 𝑅 < log 1+𝜖 |𝑉 |. For the density metrics TDS and 𝑘CLiDS, we use the fact that each edge contributes to the peeling weight calculation exactly 𝑘 times due to the counting of each edge within every 𝑘-Clique in the graph (𝑘 = 3 for TDS). This multiplicity of edge contributions leads to the relationship: ∑︁ 𝑢∈𝑆𝑖−1 𝑤𝑢 (𝑆𝑖−1) ≤ 𝑘 𝑓 (𝑆𝑖−1), (8) We then adapt Equations 4-7 and the proof follows. □ Time Complexity. Algorithm 2 operates for at most 𝑅 = log 1+𝜖 |𝑉 | iterations, with each iteration scanning all vertices, taking 𝑂(|𝑉 |) time. As vertices are peeled, updates are necessitated for the peeling weights of their neighboring vertices. Over the course of 𝑅 iterations, there are no more than |𝐸 | such updates for DG, DW and FD. Consequently, Algorithm 3 incurs 𝑂((|𝐸 |+|𝑉 |) log 1+𝜖 |𝑉 |) node/edge weight updates. For DG, DW and FD, the update costs are 𝑂(1). For TDS and 𝑘CLiDS, we follow previous studies [11, 14] to compute the peeling weights, and the complexity is 𝑂(𝑘 |𝐸 |𝛼(𝐺) 𝑘−2 ), where 𝛼(𝐺) is the arboricity of the graph. Hence, the overall complexity is 𝑂(𝑘 |𝐸 |𝛼(𝐺) 𝑘−2 (|𝐸 |+|𝑉 |) log 1+𝜖 |𝑉 |). The cost of peeling weight up- dates is orthogonal to the Dupin framework. Approximation Ratio. Next, we are ready to show that Dupin provides a theoretical guarantee with a 𝑘(1 + 𝜖)-approximation for all density metrics studied in this work. We denote 𝑢𝑖 ∈ 𝑆𝑝 as the first vertex in 𝑆∗ peeled by Dupin, and 𝑆 ′ as the peeling subsequence starting from 𝑢𝑖 . Theorem 4.2. For the vertex set 𝑆𝑝 returned by Dupin and the optimal vertex set 𝑆∗, the relationship 𝑔(𝑆𝑝 ) ≥ 𝑔(𝑆∗) 𝑘(1+𝜖) holds. Proof. We first prove that ∀𝑢𝑖 ∈ 𝑆∗, 𝑤𝑢𝑖 (𝑆 ∗ ) ≥ 𝑔(𝑆∗). We prove it in contradiction. We assume that 𝑤𝑢𝑖 (𝑆 ∗ ) < 𝑔(𝑆∗). By peeling 𝑢𝑖 from 𝑆∗, we have the following. 𝑔(𝑆∗ \\\\ {𝑢𝑖 }) = 𝑓 (𝑆∗) −𝑤𝑢𝑖 (𝑆 ∗ ) |𝑆∗ |−1 > 𝑓 (𝑆∗) − 𝑔(𝑆𝑖 ) |𝑆∗ |−1 = 𝑓 (𝑆∗) − 𝑔(𝑆∗) |𝑆∗ |−1 = 𝑓 (𝑆∗) − 𝑓 (𝑆∗) |𝑆∗ | |𝑆∗ |−1 = 𝑔(𝑆∗) (9) Therefore, a better solution can be obtained by removing 𝑢𝑖 from 𝑆∗, which contradicts the fact that 𝑆∗ is the optimal solution. We can conclude that𝑤𝑢𝑖 (𝑆 ∗ ) ≥ 𝑔(𝑆∗). Next, we assume that 𝑢𝑖 ∈ 𝑆𝑝 is the first vertex in 𝑆∗ peeled by the peeling algorithm. Since 𝑆∗ is the optimal vertex set, we have 𝑔(𝑆∗) ≤ 𝑤𝑢𝑖 (𝑆 ∗ ), (10) because the optimal value 𝑔(𝑆∗) must be less than or equal to the weight contribution of any single vertex in 𝑆∗, in particular 𝑢𝑖 . Since 𝑆∗ ⊆ 𝑆 ′, it is clear that 𝑤𝑢𝑖 (𝑆 ∗ ) ≤ 𝑤𝑢𝑖 (𝑆 ′ ) (11) By the peeling condition, we have 𝑤𝑢𝑖 (𝑆 ′ ) ≤ 𝑘(1 + 𝜖)𝑔(𝑆 ′) (12) Therefore, we have 𝑔(𝑆∗) ≤ 𝑤𝑢𝑖 (𝑆 ∗ ) ≤ 𝑤𝑢𝑖 (𝑆 ′ ) ≤ 𝑘(1 + 𝜖)𝑔(𝑆 ′) ≤ 𝑘(1 + 𝜖)𝑔(𝑆𝑝 ) (13) Hence, we can conclude that 𝑔(𝑆𝑝 ) ≥ 𝑔(𝑆∗) 𝑘(1+𝜖) . □ Summary. Dupin can easily balance efficiency and the worst-case approximation ratio with a single parameter, 𝜖 . A larger 𝜖 allows more vertices to be peeled in parallel, reducing the number of peeling iterations. Meanwhile, the worst-case approximation ratio of parallel peeling is only affected by a factor of (1 + 𝜖) compared to the ratio for sequential peeling w.r.t. all density metrics studied in this work.',\n",
       " '5 LONG-TAIL PRUNING': 'We observe that the parallel peeling algorithm encounters two sig- nificant issues. First, there is a long-tail problem where subsequent iterations fail to produce denser subgraphs. Instead, each iteration only peels off a small fraction of vertices, which significantly impacts the overall runtime efficiency. Second, during batch peeling, each peeling iteration often results in disparate structures. These are pri- marily caused by the peeling of certain vertices, which leaves their neighboring structure disconnected and fragmented. This phenome- non affects the continuity and coherence of the subgraphs. As shown in Table 2, on the la dataset (the largest social network dataset tested in our experiments), DG, DW, and FD experience 24.67%, 46.84%, and 3.01% of rounds as ineffective long-tail iterations, severely limiting response time. Additionally, during the peeling process, if the batch size is large (e.g., 𝜖 = 0.5), approximately 25.34%, 29.46%, and 7.16% of nodes become disconnected and sparse, signifi- cantly impacting the quality of the detected subgraphs. To address the challenges of high iteration counts in peeling algorithms, Dupin introduces two long-tail pruning techniques: Global Peeling Optimization (GPO) to maintain a global peeling threshold, and Local Peeling Optimization (LPO) to improve the density of the current peeling iteration’s subgraph, thereby increasing the peeling threshold. We tested three algorithms, DG, DW, and FD, and found that they required 17,637, 150,223, and 112,074 iterations respectively, which is significantly high. By strategically pruning long-tail ver- tices and sparse vertices, the number of iterations can be reduced by up to 46.84%. Additionally, LPO can further reduce the number of iterations by up to 92.79%. These optimizations significantly enhance the efficiency of the peeling process, as discussed in this section. 5.1 Global Peeling Optimization Initially, we define what constitutes a long-tail vertex during the peeling process. Peeling these long-tail vertices results in ineffective peeling iterations. To address this, we maintain a global maximum peeling threshold to determine which vertices can be peeled directly. When the global peeling threshold is greater than the current itera- tion’s threshold, we use the global peeling threshold for peeling. Algorithm 3: DupinGPO: Global Peeling Optimization Input:𝐺 = (𝑉 , 𝐸), a density metric 𝑔(𝑆), and 𝜖 ≥ 0 Output: A subset of vertices 𝑆𝑝 that maximizes the density metric 𝑔(𝑆) 1 𝑆0 ← 𝑉 /* Initialize the subset with all vertices */ 2 𝑖 ← 1 3 𝜏max ← 𝑔(𝑆 0 ) 𝑘(1+𝜖) /* Initialize the threshold */ 4 while 𝑆𝑖−1 ̸= ∅ do 5 𝜏 ← max{𝜏max, 𝑘(1 + 𝜖) · 𝑔(𝑆𝑖−1)} /* Peeling threshold */ 6 𝑈 ← {𝑢 | 𝑤𝑢 (𝑆𝑖−1) ≤ 𝜏 } /* Vertices to be peeled */ 7 𝑆𝑖 ← 𝑆𝑖−1 \\\\𝑈 /* Update the subset */ 8 𝜏max = max{𝜏max, 𝑔(𝑆𝑖 ) 𝑘(1+𝜖) } /* Refine the threshold */ 9 𝑖 ← 𝑖 + 1 10 return arg max𝑆𝑖 𝑔(𝑆𝑖 ) /* The subset that maximizes 𝑔(𝑆) */ Definition 5.1 (Long-Tail vertex). Given an 𝛼-𝑎𝑝𝑝𝑟𝑜𝑥𝑖𝑚𝑎𝑡𝑖𝑜𝑛 DSD algorithm and a set of vertices 𝑆 ⊆ 𝑉 , if𝑤𝑢 (𝑆) < 𝑔(𝑆∗) 𝛼 and 𝑆∗ is the optimal vertex set, then 𝑢 is a long-tail vertex. Lemma 5.1. If ∃𝑢 ∈ 𝑆𝑖 is a long-tail vertex, then 𝑆𝑖 ̸= 𝑆𝑃 . Proof. We prove it in contradiction by assuming that 𝑆𝑖 = 𝑆𝑃 . Since 𝑢 is a long-tail vertex in 𝑆𝑖 ,𝑤𝑢 (𝑆𝑖 ) < 𝑔(𝑆∗) 𝛼 < 𝑔(𝑆𝑃 ). By peeling 𝑢 from 𝑆𝑃 , we have the following. 𝑔(𝑆𝑃 \\\\ {𝑢}) = 𝑓 (𝑆𝑃 ) −𝑤𝑢 (𝑆𝑃 ) |𝑆𝑃 |−1 > 𝑓 (𝑆𝑃 ) − 𝑔(𝑆𝑃 ) |𝑆𝑃 |−1 = 𝑓 (𝑆𝑃 ) − 𝑓 (𝑆𝑃 ) |𝑆𝑃 | |𝑆𝑃 |−1 = 𝑔(𝑆𝑃 ) (14) A denser subgraph can be obtained by peeling 𝑢 from 𝑆𝑃 . This contradicts that 𝑆𝑃 is the optimal solution. Hence, 𝑆𝑖 ̸= 𝑆𝑃 . □ Identifying Long-Tail Vertices. Based on Lemma 5.2, we can also establish that a vertex𝑢 in set 𝑆 can be classified as a long-tail vertex if 𝑤𝑢 (𝑆) < 𝑔(𝑆∗) 𝑘(1+𝜖) . This insight allows us to implement a global peeling threshold, denoted as 𝜏max. During each iteration, we compare 𝑔(𝑆𝑖 ) 𝑘(1+𝜖) with 𝜏max. If the former exceeds the latter, Dupin updates 𝜏max to 𝑔(𝑆𝑖 ) 𝑘(1+𝜖) , thereby enhancing the efficiency of the peeling. Peeling with Global Threshold (Algorithm 3). Using FD as an example, we can set 𝑘 to 2. In Lemma 5.2, we recognize that long-tail vertices in set 𝑆𝑖 can be peeled directly during iteration 𝑖 for FD. To facilitate this, Dupin initiates with a global peeling threshold 𝜏max (Line 3). After identifying a denser subgraph post-peeling, 𝜏max is updated to 𝑔(𝑆𝑖 ) 2(1+𝜖) , refining the peeling process (Line 8). Example 5.1. In the initial depiction provided in the leftmost panel of Figure 6 (the 𝑖-th iteration), the density of the graph is 4.28. Traditionally, this scenario would necessitate two additional iterations to complete the peeling process, which could notably increase computational time. As illustrated, vertex 𝑢 cannot be peeled in the (𝑖 + 1)-th iteration and would typically require an extra iteration, the (𝑖 + 2)-th, for its removal. However, Dupin can promptly peel vertices that fall below this threshold by employing our algorithm’s global peeling threshold, which is calculated as half of the current density (𝜏max = 4.28/2 = 2.14). This approach enables the simultaneous peeling of three nodes in the (𝑖 + 1)-th iteration. Without this pruning strategy, an additional iteration would be essential to peel vertex 𝑢, underscoring our method’s enhanced efficiency in the peeling process. This optimization obviates the necessity for a prolonged iterative sequence. The example herein substantiates the efficacy of our tai- lored optimization for long-tail distribution, substantially enhancing the algorithm’s time efficiency. 5.2 Local Peeling Optimization Algorithm 4: DupinLPO: Local Peeling Optimization Input: A graph𝐺 = (𝑉 , 𝐸), a density metric 𝑔(𝑆), and 𝜖 ≥ 0 Output: A subset of vertices 𝑆𝑝 representing the densest subgraph 1 𝑆0 ← 𝑉 /* Initialize the subset with all vertices */ 2 𝜏max ← 𝑔(𝑆 0 ) 𝑘(1+𝜖) /* Initialize the threshold */ 3 while 𝑆𝑖−1 ̸= ∅ do 4 𝜏1 ← max{𝜏max, 𝑘(1 + 𝜖) · 𝑔(𝑆𝑖−1)} /* Peeling threshold */ 5 𝑈1 ← {𝑢 ∈ 𝑆 | 𝑤𝑢 (𝑆) ≤ 𝜏1 } 6 𝑆𝑖 ← 𝑆𝑖−1 \\\\𝑈1 7 while ∃𝑢 ∈ 𝑆𝑖 , 𝑤𝑢 (𝑆𝑖 ) < 𝑔(𝑆𝑖 ) do 8 𝜏2 ← max{𝜏max, 𝑔(𝑆𝑖 )} /* Update the threshold */ 9 𝑈2 ← {𝑢 | 𝑤𝑢 (𝑆𝑖 ) < 𝜏2 } 10 𝑆𝑖 ← 𝑆𝑖 \\\\𝑈2 11 𝜏max = max{𝜏max, 𝑔(𝑆𝑖 ) 𝑘(1+𝜖) } /* Update the threshold */ 12 return arg max𝑆𝑖 𝑔(𝑆𝑖 ) /* The subset that maximizes 𝑔(𝑆) */ It is important to note that each peeling iteration often results in disparate structures. This is primarily caused by the peeling of cer- tain vertices, which leaves their neighboring structures disconnected and fragmented. This not only affects the density of the detected sub- graph but also lowers the peeling threshold. Lemma 5.2 establishes a foundational property of vertices within a dense graph. Lemma 5.2. ∀𝑢 ∈ 𝑆 , if𝑤𝑢 (𝑆) < 𝑔(𝑆), 𝑔(𝑆 \\\\ {𝑢}) > 𝑔(𝑆). Proof. By peeling 𝑢 from 𝑆 , we have the following. 𝑔(𝑆 \\\\ {𝑢}) = 𝑓 (𝑆) −𝑤𝑢 (𝑆) |𝑆 |−1 > 𝑓 (𝑆) − 𝑔(𝑆𝑖 ) |𝑆 |−1 = 𝑓 (𝑆) − 𝑔(𝑆) |𝑆∗ |−1 = 𝑓 (𝑆) − 𝑓 (𝑆) |𝑆 | |𝑆 |−1 = 𝑔(𝑆) (15) Therefore, a denser graph can be obtained by peeling 𝑢 from 𝑆 . □ According to Lemma 5.2, we introduce local peeling optimization within the iteration. After each peeling iteration, if the peeling weight of a vertex is smaller than the current density, Dupin will trim it. We implement a local peeling optimization (Lines 7-10, Algorithm 4) before the next iteration. After a vertex 𝑢 is peeled, Dupin invokes updateNgh to update the peeling weights of 𝑢’s neighboring nodes. If any of these neighbors have a peeling weight less than the current density, they are also peeled, thereby increasing the density of the current iteration. Consequently, when an iteration yields globally optimal results, a denser structure is obtained. The benefits of this approach extend beyond density improvements; performance is also enhanced. By trimming these vertices, we reduce the number of vertices to traverse in subsequent iterations, and a higher density enables more efficient iterations due to the possibility of using larger peeling thresholds. Lemma 5.3. If 𝑢𝑖 ∈ 𝑆𝑝 is the first vertex in 𝑆∗ removed by Dupin, 𝑢𝑖 will not be removed during the local peeling optimization process. Proof. Assume, for the sake of contradiction, that 𝑢𝑖 is trimmed during the local peeling optimization process. Let 𝑆 ′ be the set of vertices left before 𝑢𝑖 is trimmed. By definition, we have: 𝑤𝑢𝑖 (𝑆 ′ ) < 𝑔(𝑆∗). Since 𝑢𝑖 is the first vertex in 𝑆∗ to be removed, 𝑆∗ ⊆ 𝑆 ′, therefore: 𝑤𝑢𝑖 (𝑆 ∗ ) ≤ 𝑤𝑢𝑖 (𝑆 ′ ) < 𝑔(𝑆∗). This contradicts Lemma 5.2, which states that ∀𝑢𝑖 ∈ 𝑆∗: 𝑤𝑢𝑖 (𝑆 ∗ ) ≥ 𝑔(𝑆∗). Otherwise, a better solution can be obtained by removing 𝑢 from 𝑆∗, which contradicts the fact that 𝑆∗ is the optimal solution. Thus, 𝑢𝑖 cannot be trimmed. Hence,𝑢𝑖 will not be removed during the local peeling optimization process. □ Due to Lemma 5.3, 𝑢𝑖 will only be removed during the peeling process. Hence, the approximation ratios (Theorem 4.2) for different density metrics are preserved. 6 EXPERIMENTAL STUDY Our experiments are run on a machine that has an X5650 CPU, 512 GB RAM. The implementation is made memory-resident and implemented in C++. All codes are compiled by GCC-9.3.0 with -𝑂3. Datasets. We report the datasets used in our experiment in Table 3. One industrial dataset is from Anon (gfg). Datasets links-anon(la) and Twitter-rv (rv)were obtained from Stanford NetworkDataset Collection [31]. Datasets kron-logn21(kron), soc-twitter(soc), Web-uk-2005(uk) and Web-sk-2005(sk) were obtained from Network Data Repository [38]. Dataset bio was from the BIOMINE [35]. Baseline Methods. In this section, we introduce five commonly used density metrics pivotal in analyzing network structures:DG [6], DW [23], FD [24], TDS [46], and𝑘CLiDS [14].We implemented these metrics in our framework, Dupin, and compared their performance with frameworks including Spade [28], kCLIST [14], GBBS [16], PBBS [41], PKMC [32], FWA [15] and ALENEX [44]. kCLIST [14] and PBBS [41] are parallel 𝑘CLiDS algorithms, while GBBS [16] supports the DG density metric. Although GBBS does not support weighted graphs, we precompute the peeling weights offline and then import them into GBBS. The time spent on this offline computation is not included in our reported runtime. FWA, ALENEX and PKMC support the DG density metric. For clarity, we denote the incremental peeling process, which involves the iterative removal of graph elements based on density scores, as Spade-X, where X corresponds to each metric, under the same experimental condi- tions described in [28], where the batch size is set to 1K, consistent with the default setting in [28]. We report the average runtime per batch for Spade. DupinGPO-X and DupinLPO-X represent our par- allel peeling algorithms that incorporate global and local peeling optimization. Default Settings. In our experiments, the parameter 𝜖 is set to 0.1 by default, and the number of threads 𝑡 is configured to 128. These settings are used consistently unless otherwise specified. 6.1 Efficiency of Dupin Spade vs Dupin. Our initial comparison focuses on the performance disparity between the incremental and parallel peeling algorithms. Specifically, Dupin-DG (resp. Dupin-DW, Dupin-FD, Dupin-TDS, andDupin-𝑘CLiDS) demonstrates a speedup factor of 6.97 (resp. 7.71, 7.71, 7.65, and 8.46) over Spade-DG (resp. Spade-DW, Spade-FD, Spade-TDS, and Spade-𝑘CLiDS). Notably, Spade-TDS and Spade𝑘CLiDS only complete computations within 7,200 seconds on gfg. The bottleneck for Spade arises in larger graphs where initial calculations of triangle and 𝑘-Clique counting are unable to finish. Dupin outpaces Spade primarily because Spade suffers from frequent re- ordering of peeling sequences when there are many increments, which proves to be slower than recalculating. GBBS vs Dupin. Previous works have not provided simultaneous support for all density metrics. To bridge this gap, we implemented DG, DW, and FD within the parallel GBBS framework. On average, Dupin-DG, Dupin-DW, and Dupin-FD demonstrated speedup factors of 6.33, 12.51, and 17.07 times, respectively, over GBBS-DG, GBBS-DW, andGBBS-FD. This performance advantage is attributed to the fundamental difference in the peeling process between the two frameworks. In GBBS, the basic unit of each peel is a bucket, where all nodes within the same bucket have an identical peeling weight. This design limits the parallelism in weighted graphs like those using DW and FD, where many buckets may contain only a single node. In contrast, Dupin peels batches of nodes at each step, achieving higher parallelism and thus consistently outperforming GBBS, particularly in weighted graph scenarios where the speedup is even more pronounced. kCLIST vs PBBS vs Dupin. Both kCLIST and PBBS support the density metrics TDS and 𝑘CLiDS. In our comparisons, Dupin demonstrates notable performance advantages. Specifically, Dupin-TDS is faster than kCLIST-TDS and PBBS-TDS by 39.85 and 62.71 times, respectively. Furthermore, Dupin-𝑘CLiDS also outperforms kCLIST𝑘CLiDS by 4.16 times. A significant finding is that PBBS-𝑘CLiDS fails to complete computations within 7200 seconds on most datasets, highlighting the efficiency of Dupin in handling more complex met- rics under stringent time constraints. Impact of DupinGPO. Next, we evaluate the acceleration effect of DupinGPO, using Dupin as a baseline for comparison. Notably, DupinGPO-DG is found to be 1.14 times faster, DupinGPO-DW is 1.11 times faster,DupinGPO-TDS is 1.10 times faster, andDupinGPO𝑘CLiDS is 1.35 times faster than their respective Dupin implementations. The acceleration is particularly significant for 𝑘CLiDS on larger datasets, such as soc, rv, kron, and sk. DupinGPO-𝑘CLiDS is 1.40, 1.54, 1.51, and 1.61 times faster than Dupin-𝑘CLiDS on soc, rv, kron, and sk, respectively. This is attributed to the greater number of iterations required due to the larger dataset sizes and the higher count of 𝑘-Cliques, necessitating more peeling iterations. In such scenarios,DupinGPO effectively prunes the tail iterations early, thereby reducing the number of rounds needed. Impact of DupinLPO. We next compared the efficiency of Dupin andDupinLPO. On average, thoughDupinLPO-DGwas 10.09% slower than Dupin-DG, the speed is still comparable. For example, on the soc dataset, DupinLPO-DG was 5.94% faster than Dupin-DG. This is because DupinLPO requires some lock operations to update the peeling weights of neighboring vertices during local peeling optimization. However, the advantage is that DupinLPO can detect up to 26.26% denser subgraphs, as detailed in Section 6.2. On larger datasets, such as soc, rv, and la, the vertices being trimmed have relatively low correlations with each other, so the impact of locks is minimal. For example, on the soc (resp. rv), DupinLPO-DG was 5.94% (resp. 9.94%) faster than Dupin-DG. In contrast, on smaller datasets, such as bio and kron, the extra cost of locks is higher. Impact of the Number of Threads 𝑡 . All methods are influenced by the concurrency level, i.e., the number of threads. Generally, the greater the number of threads, the faster the execution speed. In our experiments, using the la dataset as a benchmark, we varied the number of threads 𝑡 from 2 to 128 to observe the impact on runtime performance. For GBBS-DG, runtime stabilizes and accelerates by 3.33 times as thread count increases from 2 to 8. For GBBS-DW, performance deteriorates when the thread count exceeds 32, but from 2 to 32 threads, it accelerates by 3.50 times. Similarly, GBBS-FD shows no significant change in runtime beyond 32 threads, with an acceleration of 3.09 times from 2 to 32 threads. GBBS-TDS and GBBS-𝑘CLiDS fail to complete computations on the la dataset. In contrast, all five methods tested onDupin exhibit good scalability. As the number of threads increases from 2 to 128,Dupin-DG accelerates by 9.91 times, Dupin-DW by 12.69 times, Dupin-FD by 13.03 times, and Dupin-TDS by 28.06 times. Although Dupin-𝑘CLiDS fails to produce results when the thread count is below 16, it accelerates by 1.32 times from 32 to 128 threads. These results demonstrate Dupin’s strong scalability across varied threading levels. 6.2 Effectiveness of Dupin In addition to performance speedup, we evaluate the density of the subgraphs detected by different frameworks. Detailed density comparisons are presented in Table 5. GBBS vs Spade vs Dupin. Firstly, we compare the densities of the subgraphs identified by GBBS-DG (respectively GBBS-DW, and GBBS-FD) to those detected by Dupin in parallel execution. Our experimental results indicate that Dupin maintains comparable subgraph densities with GBBS, signifying that the increase in com- putational efficiency does not compromise the accuracy of the de- tected subgraphs. Specifically, the density of the subgraph detected byGBBS-DG (respectivelyGBBS-DW, andGBBS-FD) is 7.08% (resp. 6.48% and 7.43%) greater than that detected by Dupin-DG (respectively Dupin-DW, and Dupin-FD) on average. Spade shares similar densities with GBBS. Due to Spade’s tendency to accumulate er- rors over time, its density can become inflated. We will explain its limitations in practical applications in more detail in the case study. kCLIST vs PBBS vs Dupin. a) For 𝑘CLiDS, PBBS-𝑘CLiDS could not complete on most of our test datasets, so we did not compare their densities. kCLIST-𝑘CLiDS could not complete on the sk dataset. On the other datasets, Dupin-𝑘CLiDS’s density was only 6.97% smaller than kCLIST-𝑘CLiDS. On some datasets, such as bio and kron,Dupin𝑘CLiDS even detected subgraphs with greater density. b) For TDS, PBBS-TDS could detect subgraphs on smaller graphs but failed on billion-scale graphs such as rv, sk, and la. Dupin-TDS’s density was 2.03% greater than kCLIST-TDS. Therefore, we conclude that Dupin is comparable to existing parallel methods in terms of the density of the detected dense subgraphs. Ablation Study of Dupin. This experiment delves into the effectiveness of our optimizations, DupinGPO and DupinLPO. Our analysis primarily focuses on comparing the densities of dense subgraphs detected by both DupinGPO and DupinLPO in various settings. DupinGPO achieves the same density as Dupin across five den- sity metrics because it trims the long tail, enhancing efficiency. For DupinLPO-DG (resp. DupinLPO-DW, DupinLPO-FD, DupinLPOTDS, and DupinGPO-𝑘CLiDS), we observe that the detected sub- graphs exhibit densities 11.09% (resp. 7.32%, 8.00%, 1.01%, and 26.26%) greater than those detected by Dupin-DG, Dupin-DW, Dupin-FD, Dupin-TDS, andDupin-𝑘CLiDS, respectively, demonstrating notable outcomes. This is because DupinLPO trims the graph in each iteration, resulting in denser subgraphs compared to Dupin. Impact of the Approximation Ratio 𝜖. The approximation ratio 𝜖 serves to control accuracy. In our experiment, we vary 𝜖 from 0.1 to 1 to investigate its influence on the accuracy of Dupin. We observe that as 𝜖 increases from 0.1 to 1, the density of subgraphs detected by Dupin, DupinGPO, and DupinLPO generally decreases. ForDG, the densities ofDupin-DG,DupinGPO-DG, andDupinLPODG decrease by 22.71%, 22.71%, and 11.99%, respectively. The trends for DW and DG are similar, with the worst performance observed at 𝜖 = 0.6 for both density metrics. The trends for FD, TDS, and 𝑘CLiDS are also similar. DupinLPO detects denser subgraphs across most density metrics and is less affected by 𝜖 , due to its iterative trimming. For example, for FD, DupinLPO’s density decreases by only 6.23%, while DupinGPO and Dupin’s densities decrease by 17.37%. 6.3 Case study We also compare the performance in real-world scenarios, focusing on three key aspects: accuracy, latency, and prevention ratio. Accuracy. As discussed in Section 1, although Spade can quickly respond to new transactions, it tends to accumulate errors over time. Our analysis utilizes transaction data from Anon, with the transaction network comprising |𝐸 |= 2 billion edges. Under the incremental computation framework of Spade, the assumption is made that the weights of existing edges remain constant, thereby neglecting the impact of newly inserted edges on these weights, leading to error accumulation. For instance, using FD density metric, we observe that errors accumulate up to 24.4% within a day and increase to 30.3% over a week, as illustrated in Figure 9. Although Dupin sacrifices some accuracy to achieve parallel computational efficiency, our experiments show that Dupin’s errors remain relatively stable between 3% and 5%. Hence, the detection precision of Spade declines from 87.9% to 68.3%, while Dupin’s precision remains above 86%. Latency and Prevention Ratio. If a transaction is identified as fraudulent, subsequent related transactions are blocked to mitigate potential losses. We define the ratio of successfully prevented sus- picious transactions to the total number of suspicious transactions as R. As shown in Figure 6, the prevention ratio R continues to decrease as latency increases on Anon’s datasets. Using the default density metric, FD, in Anon, our results show thatDupin-FD can prevent 94.5% of fraudulent activities, GBBS-FD can prevent 3.0%, and Spade-FD can prevent 45.3%. The reason for the lower performance of GBBS-FD is its low parallelism in the peeling process, which results in long peeling times and thus an average latency of 6,014 seconds, delaying the prevention of many fraudulent activities. Although Spade-FD performs well in reordering techniques on smaller datasets, in industrial scenarios with billion-scale graphs, latency significantly worsens due to the extensive reordering range required in the peeling sequences as normal users transition to fraudsters, increasing the cost of incremental computations. Additional tests were conducted with other density metrics; for DG, Dupin-DG can prevent 78.8%, while other methods prevent varying amounts. Similarly, for DW, Dupin-DW can prevent 86.1%, while other methods demonstrate different prevention capabilities.',\n",
       " '7 CONCLUSION AND FUTUREWORKS': 'In this paper, we presented Dupin, a novel parallel fraud detection framework tailored for massive graphs. Our experiments demonstrated that Dupin’s advanced peeling algorithms and long-tail pruning techniques, DupinGPO and DupinLPO, significantly enhance the detection efficiency of dense subgraphs without sacrificing accuracy. Comparative studies with GBBS, PBBS, kCLIST, Spade, and Dupin highlighted Dupin’s superior performance in terms of com- putational efficiency and accuracy. Through case studies, we also validated that Dupin achieves lower latency and higher detection ac- curacy compared to existing fraud detection systems on billion-scale graphs. Overall, Dupinproves to be a powerful tool for large-scale fraud detection, providing a scalable and efficient solution for real-time applications. Future work will explore the integration of additional suspiciousness functions and further optimization to handle even larger datasets and more complex fraud scenarios.',\n",
       " 'REFERENCES': '[1] Distil networks: The 2019 bad bot report. https://www.bluecubesecurity.com/wp- content/uploads/bad-bot-report-2019LR.pdf. [2] B. Bahmani, R. Kumar, and S. Vassilvitskii. Densest subgraph in streaming and mapreduce. Proceedings of the VLDB Endowment, 5(5), 2012. [3] Y. Ban, X. Liu, T. Zhang, L. Huang, Y. Duan, X. Liu, and W. Xu. Badlink: Combining graph and information-theoretical features for online fraud group detection. arXiv preprint arXiv:1805.10053, 2018. [4] A. Beutel, W. Xu, V. Guruswami, C. Palow, and C. Faloutsos. Copycatch: stopping group attacks by spotting lockstep behavior in social networks. In Proceedings of the 22nd international conference on World Wide Web, pages 119–130, 2013. [5] D. Boob, Y. Gao, R. Peng, S. Sawlani, C. Tsourakakis, D.Wang, and J.Wang. Flowless: Extracting densest subgraphs without flow computations. In Proceedings of The Web Conference 2020, pages 573–583, 2020. [6] M. Charikar. Greedy approximation algorithms for finding dense components in a graph. In International Workshop on Approximation Algorithms for Combinatorial Optimization, pages 84–95. Springer, 2000. [7] M. Charikar. Greedy approximation algorithms for finding dense components in a graph. In Approximation Algorithms for Combinatorial Optimization, Third International Workshop, APPROX 2000, Saarbrücken, Germany, September 5-8, 2000, Proceedings, volume 1913 of Lecture Notes in Computer Science, pages 84–95. Springer, 2000. [8] C. Chekuri, K. Quanrud, and M. R. Torres. Densest subgraph: Supermodularity, iterative peeling, and flow. In Proceedings of the 2022 Annual ACM-SIAM Symposium on Discrete Algorithms (SODA), pages 1531–1555. SIAM, 2022. [9] J. Chen and Y. Saad. Dense subgraph extraction with application to community detection. IEEE Transactions on knowledge and data engineering, 24(7):1216–1230, 2010. [10] Y. Chen, J. Jiang, S. Sun, B. He, and M. Chen. Rush: Real-time burst subgraph detection in dynamic graphs. Proceedings of the VLDB Endowment, 17(11):3657– 3665, 2024. [11] N. Chiba and T. Nishizeki. Arboricity and subgraph listing algorithms. SIAM Journal on computing, 14(1):210–223, 1985. [12] L. Chu, Y. Zhang, Y. Yang, L. Wang, and J. Pei. Online density bursting subgraph detection from temporal graphs. Proceedings of the VLDB Endowment, 12(13):2353– 2365, 2019. [13] L. Dagum and R. Menon. Openmp: an industry standard api for shared-memory programming. IEEE computational science and engineering, 5(1):46–55, 1998. [14] M. Danisch, O. Balalau, and M. Sozio. Listing k-cliques in sparse real-world graphs. In Proceedings of the 2018 World Wide Web Conference, pages 589–598, 2018. [15] M. Danisch, T.-H. H. Chan, and M. Sozio. Large scale density-friendly graph decomposition via convex programming. In Proceedings of the 26th International Conference on World Wide Web, pages 233–242, 2017. [16] L. Dhulipala, J. Shi, T. Tseng, G. E. Blelloch, and J. Shun. The graph based benchmark suite (gbbs). In Proceedings of the 3rd Joint International Workshop on Graph Data Management Experiences & Systems (GRADES) and Network Data Analytics (NDA), pages 1–8, 2020. [17] Y. Dourisboure, F. Geraci, and M. Pellegrini. Extraction and classification of dense communities in the web. In Proceedings of the 16th international conference on World Wide Web, pages 461–470, 2007. [18] A. Epasto, S. Lattanzi, and M. Sozio. Efficient densest subgraph computation in evolving graphs. In Proceedings of the 24th international conference on world wide web, pages 300–310, 2015. [19] D. Gibson, R. Kumar, and A. Tomkins. Discovering large dense subgraphs in massive graphs. In Proceedings of the 31st international conference on Very large data bases, pages 721–732. Citeseer, 2005. [20] A. Gionis and C. E. Tsourakakis. Dense subgraph discovery: Kdd 2015 tutorial. In Proceedings of the 21th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, pages 2313–2314, 2015. [21] A. V. Goldberg. Finding a maximum density subgraph. 1984. [22] A. V. Goldberg and R. E. Tarjan. A new approach to the maximum-flow problem. Journal of the ACM (JACM), pages 921–940, 1988. [23] N. V. Gudapati, E. Malaguti, and M. Monaci. In search of dense subgraphs: How good is greedy peeling? Networks, 77(4):572–586, 2021. [24] B. Hooi, H. A. Song, A. Beutel, N. Shah, K. Shin, and C. Faloutsos. Fraudar: Bounding graph fraud in the face of camouflage. In Proceedings of the 22nd ACM SIGKDD international conference on knowledge discovery and data mining, pages 895–904, 2016. [25] M. Jaggi. Revisiting frank-wolfe: Projection-free sparse convex optimization. In International conference on machine learning, pages 427–435. PMLR, 2013. [26] J. Jiang, Y. Chen, B. He, M. Chen, and J. Chen. Spade+: A generic real-time fraud detection framework on dynamic graphs. IEEE Transactions on Knowledge and Data Engineering, 2024. [27] M. Jiang, P. Cui, A. Beutel, C. Faloutsos, and S. Yang. Inferring strange behavior from connectivity pattern in social networks. In Pacific-Asia conference on knowledge discovery and data mining, pages 126–138. Springer, 2014. [28] J. Jiaxin, L. Yuan, H. Bingsheng, H. Bryan, C. Jia, and J. K. Z. Kang. A real-time fraud detection framework on evolving graphs. PVLDB, 2023. [29] S. Kumar, W. L. Hamilton, J. Leskovec, and D. Jurafsky. Community interaction and conflict on the web. In Proceedings of the 2018 world wide web conference, pages 933–943, 2018. [30] V. E. Lee, N. Ruan, R. Jin, and C. Aggarwal. A survey of algorithms for dense subgraph discovery. In Managing and mining graph data, pages 303–336. Springer, 2010. [31] J. Leskovec and A. Krevl. SNAP Datasets: Stanford large network dataset collection. http://snap.stanford.edu/data, June 2014. [32] W. Luo, Z. Tang, Y. Fang, C. Ma, and X. Zhou. Scalable algorithms for densest subgraph discovery. In 2023 IEEE 39th International Conference on Data Engineering (ICDE), pages 287–300. IEEE, 2023. [33] C. Ma, Y. Fang, R. Cheng, L. V. Lakshmanan, and X. Han. A convex-programming approach for efficient directed densest subgraph discovery. In Proceedings of the 2022 International Conference on Management of Data, pages 845–859, 2022. [34] M. Mitzenmacher, J. Pachocki, R. Peng, C. Tsourakakis, and S. C. Xu. Scalable large near-clique detection in large-scale networks via sampling. In Proceedings of the 21th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, pages 815–824, 2015. [35] V. Podpečan, Ž. Ramšak, K. Gruden, H. Toivonen, and N. Lavrač. Interactive explo- ration of heterogeneous biological networks with biomine explorer. Bioinformatics, 06 2019. [36] H. Qin, R.-H. Li, Y. Yuan, G. Wang, L. Qin, and Z. Zhang. Mining bursting core in large temporal graphs. Proceedings of the VLDB Endowment, 2022. [37] Y. Ren, H. Zhu, J. Zhang, P. Dai, and L. Bo. Ensemfdet: An ensemble approach to fraud detection based on bipartite graph. In 2021 IEEE 37th International Conference on Data Engineering (ICDE), pages 2039–2044. IEEE, 2021. [38] R. A. Rossi and N. K. Ahmed. The network data repository with interactive graph analytics and visualization. In AAAI, 2015. [39] A. E. Sarıyüce andA. Pinar. Peeling bipartite networks for dense subgraph discovery. In Proceedings of the Eleventh ACM International Conference on Web Search and Data Mining, pages 504–512, 2018. [40] S. B. Seidman. Network structure and minimum degree. Social networks, 5(3):269– 287, 1983. [41] J. Shi, L. Dhulipala, and J. Shun. Parallel clique counting and peeling algorithms. In SIAM Conference on Applied and Computational Discrete Algorithms (ACDA21), pages 135–146. SIAM, 2021. [42] K. Shin, T. Eliassi-Rad, and C. Faloutsos. Corescope: Graph mining using k-core analysis—patterns, anomalies and algorithms. In 2016 IEEE 16th international conference on data mining (ICDM), pages 469–478. IEEE, 2016. [43] K. Shin, B. Hooi, J. Kim, and C. Faloutsos. Densealert: Incremental dense-subtensor detection in tensor streams. In Proceedings of the 23rd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, pages 1057–1066, 2017. [44] P. Sukprasert, Q. C. Liu, L. Dhulipala, and J. Shun. Practical parallel algorithms for near-optimal densest subgraphs on massive graphs. In 2024 Proceedings of the Symposium on Algorithm Engineering and Experiments (ALENEX), pages 59–73. SIAM, 2024. [45] B. Sun, M. Danisch, T. H. Chan, and M. Sozio. Kclist++: A simple algorithm for finding k-clique densest subgraphs in large graphs. Proceedings of the VLDB Endowment (PVLDB), 2020. [46] C. Tsourakakis. The k-clique densest subgraph problem. In Proceedings of the 24th international conference on world wide web, pages 1122–1132, 2015. [47] C. Ye, Y. Li, B. He, Z. Li, and J. Sun. Gpu-accelerated graph label propagation for real-time fraud detection. In Proceedings of the 2021 International Conference on Management of Data, pages 2348–2356, 2021.'}"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "parse_document(full_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Abstract': 'ABSTRACT Detecting fraudulent activities in financial and e-commerce transac- tion networks is crucial, and one effective method for this is Densest Subgraph Discovery (DSD). However, deploying DSD methods in production systems faces substantial scalability challenges due to the predominantly sequential nature of existing methods, which impedes their ability to handle large-scale transaction networks and results in significant detection delays. In this paper, we introduce Dupin, a novel parallel processing framework designed for efficient DSD processing in billion-scale graphs. Dupin is powered by a processing engine that exploits the unique properties of the peeling process, with theoretical guarantees on detection quality and efficiency.Dupin provides user-friendly APIs for flexible customization of DSD objectives and ensures robust adaptability to diverse fraud detection scenarios. Empirical evaluations indicate that Dupin outperforms several existing DSD methods, with performance improvements observed in specific scenarios reaching up to two orders of magnitude. On billion-scale graphs, Dupin demonstrates the potential to enhance the prevention of fraudulent transactions by approximately 49.5 basis points and reduce density error from 30% to below 5%, as supported by our experimental results. ACM Reference Format: Anonymous Author(s). 2024. Dupin: A Parallel Densest Subgraph Discovery Framework on Massive Graphs. In Proceedings of ACM Conference (Conference’17). ACM, New York, NY, USA, 14 pages. https://doi.org/10.1145/ nnnnnnn.nnnnnnn',\n",
       " '1 INTRODUCTION': 'Graph structures pervade numerous contemporary applications, ranging from transaction and communication networks to expan- sive social media platforms. The study of densest subgraph discovery (DSD), first explored by [21], has since proven instrumental in various domains: link spam identification [4, 19], community de- tection [9, 17], and notably, fraud detection [8, 24, 42]. Central to this domain is the peeling process [2, 5, 8, 24, 46], which operates by iteratively removing vertices based on density metrics, such as vertex degree or the cumulative weight of adjacent edges. Their widespread adoption can be attributed to their inherent efficiency, robustness, and proven worst-case performance guarantees for DSD. In practice, business requirements demand the detection of fraud- ulent communities on billion-scale graphs within seconds. Recent Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for components of this work owned by others than ACM must be honored. Abstracting with credit is permitted. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. Request permissions from permissions@acm.org. Conference’17, July 2017, Washington, DC, USA © 2024 Association for Computing Machinery. ACM ISBN 978-x-xxxx-xxxx-x/YY/MM. . . $15.00 https://doi.org/10.1145/nnnnnnn.nnnnnnn studies, including those by [1] and [47], have highlighted the perva- sive issue of malicious bot activity in e-commerce, accounting for a substantial 21.4% of all traffic in 2018. These fraudulent activities often happen quickly, making them hard to detect. The inability to efficiently process and analyze such massive graphs not only ham- pers fraud detection efforts but also exposes businesses to substan- tial financial risks and reputational damage. Therefore, addressing these challenges is not merely an academic exercise; it is a press- ing necessity for enhancing the integrity of digital transactions and safeguarding consumer trust. Despite extensive research onDSD for static graphs [20–22, 30, 33, 34], dynamic graphs [2, 18, 26, 28, 43], and parallel approaches [14, 16, 41, 42], existing methods face significant challenges for practical de- ployment in real-world fraud detection systems. For instance, many existing algorithms primarily rely on static score functions for DSD, lacking the flexibility required for dynamic and evolving fraud detec- tion scenarios. Methods such as [4] and [27] are designed with fixed detection criteria, which can become less effective as fraudulent be- haviors adapt over time. Additionally, incremental algorithms remain inefficient at handling billion-scale graphs with high transaction volumes, leading to increased latency. Parallel DSD frameworks often lack efficient pruning techniques, resulting in excessive iterations that degrade performance on large-scale graphs. This paper addresses the following research question: How can we enhance the efficiency and scalability of DSD methods to effectively detect fraud in massive graphs? Challenges. We evaluated existing methods with our industry partner, Anon (anonymized for review). The following issues surfaced: Firstly, existing algorithms focus on static score functions for DSD and lack flexible customization. While this may be adequate for short-term detection, it becomes less effective over time. For example, as shown in Figure 1a, graph density can accumulate sig- nificant errors within a single day, highlighting the need for more adaptive and customizable detection methods. Secondly, existing incremental algorithms are still inefficient at handling billion-scale graphs with high transaction volumes, leading to increased latency. This limitation is evident in Figure 1b, which illustrates the system load distribution for a specific day’s transactions, underscoring the inability of current methods to efficiently manage high transaction volumes without significant delays. Thirdly, existing parallel densest subgraph discovery frameworks lack efficient pruning techniques, re- sulting in a large number of iterations. This inefficiency is especially problematic on billion-scale graphs, where performance degrades substantially due to the excessive computational overhead caused by numerous iterations. Adapting to Evolving Research Needs. Recognizing the dynamic nature of fraud detection and the rapid evolution of fraudulent tactics, we have adapted our research focus to address these challenges. Our approach incorporates flexible customization options for DSD methods, allowing for adjustments based on the changing landscape of fraudulent activities. This adaptability is crucial for maintaining the effectiveness of fraud detection systems in an environment where new threats continuously emerge. Contributions. In this work, we propose a DSD framework for fraud detection. Our key contributions are as follows: • A DSD Parallelization Framework. We propose Dupin, a novel parallel processing framework for densest subgraph discovery (DSD) that breaks the sequential dependencies inherent in tra- ditional density metrics, significantly enhancing scalability and efficiency on both weighted and unweighted graphs. Unlike existing methods that focus on specific density metrics, Dupin allows developers to define custom fraud detection semantics through flexible suspiciousness functions for edges and vertices. It supports a wide range of DSD semantics, including DG [6], DW [23], FD [24], TDS [46], and 𝑘CLiDS [14]. Importantly, Dupin not only leverages parallelism but also provides theoretical guarantees on approximation ratios for the supported density metrics. To the best of our knowledge, this is the first generic parallel processing framework that accommodates a broad class of density metrics in DSD, offering both flexibility and theoretical assurance. • Exploiting Long-Tail Properties.We identify and address the long-tail problem in the peeling process of DSD algorithms, where late iterations remove only a small fraction of vertices and con- tribute little to the quality of the densest subgraph, leading to inefficiency. To tackle this, we introduce two novel optimizations: Global Pruning Optimization, which proactively removes low-contribution vertices within each peeling iteration to reduce computational overhead, and Local Pruning Optimization, which eliminates disparate structures formed after vertex removal to enhance the density and quality of the detected subgraph. These optimizations significantly reduce the number of iterations re- quired, improving both performance and result quality. • Empirical Validation and Real-World Impact: We conduct comprehensive experiments on large-scale industry datasets to validate the effectiveness of Dupin. Our results show that Dupin achieves up to 100 times faster fraud detection compared to the state-of-the-art incremental method Spade [28] and parallel methods like PBBS [41]. Moreover,Dupin is capable of preventing up to 94.5% of potential fraud, demonstrating significant practical bene- fits and reinforcing system integrity in real-world applications. Organization. The remainder of this paper is structured as follows: Section 2 presents the background and the literature review. Section 3 introduces the architecture of Dupin. In Section 4, we propose the theoretical foundations for parallel peeling-based algorithms for DSD. Section 5 introduces long-tail pruning techniques to improve efficiency and effectiveness. The experimental evaluation of Dupin is presented in Section 6. Finally, Section 7 concludes the paper and outlines avenues for future research.',\n",
       " '2 BACKGROUND AND RELATEDWORK': '2.1 Preliminary This subsection presents the preliminary. Some frequently used no- tations are summarized in Table 1. Graph 𝐺 . In this work, we operate on a weighted graph denoted as 𝐺 = (𝑉 , 𝐸), where 𝑉 represents the set of vertices and 𝐸 (⊆ 𝑉 ×𝑉 ) represents the set of edges. Each edge (𝑢𝑖 , 𝑢 𝑗 ) ∈ 𝐸 is assigned a nonnegative weight, expressed as 𝑐𝑖 𝑗 . Additionally, 𝑁 (𝑢) denotes the set of neighboring vertices connected to vertex 𝑢. Induced Subgraph. For a given subset 𝑆 of vertices in 𝑉 , we define the induced subgraph as 𝐺[𝑆] = (𝑆, 𝐸[𝑆]), where 𝐸[𝑆] = {(𝑢, 𝑣) | (𝑢, 𝑣) ∈ 𝐸 ∧ 𝑢, 𝑣 ∈ 𝑆}. The size of the subset 𝑆 is denoted by |𝑆 |. Density Metrics 𝑔 and Weight Function 𝑓 . We utilize the class of metrics𝑔 as defined in prior works [6, 24, 28], where for a given subset of vertices 𝑆 ⊆ 𝑉 , the density metric is computed as: 𝑔(𝑆) = 𝑓 (𝑆) |𝑆 | . Here, 𝑓 (𝑆) represents the total weight of the induced subgraph 𝐺[𝑆], which is defined as the sum of the weights of vertices in 𝑆 and the weights of edges in 𝐸[𝑆]. More formally: 𝑓 (𝑆) = ∑︁ 𝑢𝑖 ∈𝑆 𝑎𝑖 + ∑︁ (𝑢𝑖 ,𝑢 𝑗 )∈𝐸[𝑆] 𝑐𝑖 𝑗 (1) The weight of a vertex 𝑢𝑖 , denoted as 𝑎𝑖 (𝑎𝑖 ≥ 0), quantifies the suspiciousness of user 𝑢𝑖 . The weight of an edge (𝑢𝑖 , 𝑢 𝑗 ), represented by 𝑐𝑖 𝑗 (𝑐𝑖 𝑗 > 0), reflects the suspiciousness of the transaction between users 𝑢𝑖 and 𝑢 𝑗 . Intuitively, the density metric 𝑔(𝑆) encapsulates the overall suspiciousness density of the induced subgraph𝐺[𝑆]. A larger value of 𝑔(𝑆) indicates a higher density of suspiciousness within𝐺[𝑆]. The vertex set that maximizes the density metric 𝑔 is denoted by 𝑆∗. We next introduce five representative density metrics extensively employed in many fraud detection applications. The density metrics are mainly differed by how the weight function is defined. Dense Subgraphs (DG) [6]. The DG metric is employed to quan- tify the connectivity of substructures within a graph. It has found extensive applications in various domains, such as detecting fake comments in online platforms [29] and identifying fraudulent activities in social networks [3]. Given a subset of vertices 𝑆 ⊆ 𝑉 , the weight function of DG is defined as 𝑓 (𝑆) = |𝐸[𝑆]|, where |𝐸[𝑆]| denotes the number of edges in the induced subgraph 𝐺[𝑆]. Dense Subgraph on Weighted Graphs (DW) [23]. In transac- tion graphs, it is common to have weights assigned to the edges, representing quantities such as the transaction amount. The DW metric extends the concept of dense subgraphs to accommodate these weighted relationships. Given a subset of vertices 𝑆 ⊆ 𝑉 , the weight function of DW is defined as 𝑓 (𝑆) = ∑ (𝑢𝑖 ,𝑢 𝑗 )∈𝐸[𝑆] 𝑐𝑖 𝑗 . Fraudar (FD) [24]. In order to mitigate the effects of fraudulent actors deliberately obfuscating their activities, Hooi et al. [24] introduced the FD algorithm. This approach innovatively incorporates weights for edges and assigns prior suspiciousness scores to vertices, potentially utilizing side information to enhance detection accuracy. Given a subset of vertices 𝑆 ⊆ 𝑉 , the weight function of FD is defined as Equation 1 where 𝑎𝑖 denotes the suspicious score of vertex 𝑢𝑖 and 𝑐𝑖 𝑗 = 1 log(𝑥+𝑐) . Here 𝑥 is the degree of the object vertex between 𝑢𝑖 and 𝑢 𝑗 , and 𝑐 is a positive constant [24]. Triangle Densest Subgraph (TDS) [46]. Triangles serve as a crucial structural motif in graphs, providing valuable insights into the connectivity and cohesiveness of subgraphs. The TDS metric, in par- ticular, leverages the prevalence of triangles to quantify the density of a subgraph. Given a subset of vertices 𝑆 ⊆ 𝑉 , the weight function of TDS is defined as 𝑓 (𝑆) = 𝑡 (𝑆) where 𝑡 (𝑆) represents the total number of triangles within the induced subgraph 𝐺[𝑆]. 𝑘-Clique Densest Subgraph (𝑘CLiDS) [14]. Extending the concept of triangle densest subgraphs, 𝑘CLiDS focuses on identifying dense subgraphs based on the presence of 𝑘-Cliques. A 𝑘-Clique is a complete subgraph of 𝑘 vertices, representing a tightly-knit group of vertices. For a given subset of vertices 𝑆 ⊆ 𝑉 , the weight function of 𝑘CLiDS is defined as 𝑓 (𝑆) = 𝑘(𝑆), where 𝑘(𝑆) denotes the number of 𝑘-Cliques within the induced subgraph 𝐺[𝑆]. 2.2 Sequential Peeling Algorithms Peeling algorithms have gained popularity for their efficiency, ro- bustness, and proven worst-case performance in various applica- tions [6, 24, 46]. Next, we provide a concise overview of the typical execution flow of these algorithms. Peeling Weight.We use𝑤𝑢𝑖 (𝑆) to indicate the decrease in the value of the weight function 𝑓 when the vertex𝑢𝑖 is removed from a vertex set 𝑆 , i.e., the peeling weight. Formally, 𝑤𝑢𝑖 (𝑆) = 𝑎𝑖 + ∑︁ (𝑢 𝑗 ∈𝑆) ∧ ((𝑢𝑖 ,𝑢 𝑗 )∈𝐸) 𝑐𝑖 𝑗 + ∑︁ (𝑢 𝑗 ∈𝑆) ∧ ((𝑢 𝑗 ,𝑢𝑖 )∈𝐸) 𝑐 𝑗𝑖 (2) Algorithm 1: Sequential Peeling Algorithm Input: A graph𝐺 = (𝑉 , 𝐸) and a density metric 𝑔(𝑆) Output: A subset of vertices 𝑆 that maximizes the density metric 𝑔(𝑆) 1 𝑆0 ← 𝑉 /* Initialize the subset with all vertices */ 2 for 𝑖 ← 1 to |𝑉 | do 3 𝑢 ← arg max𝑣∈𝑆𝑖−1 𝑔(𝑆𝑖−1 \\\\ {𝑣 }) /* The vertex to be peeled */ 4 𝑆𝑖 ← 𝑆𝑖−1 \\\\ {𝑢 } /* The selected vertex */ 5 return arg max𝑆𝑖 𝑔(𝑆𝑖 ) /* The subset that maximizes 𝑔(𝑆) */ Sequential PeelingAlgorithm (Algorithm1).The peeling process starts with the full vertex set 𝑆0 = 𝑉 (Line 1) and iteratively removes vertices to maximize the density metric 𝑔. In each step 𝑖 , a vertex𝑢𝑖 is removed from 𝑆𝑖−1 to form 𝑆𝑖 , such that 𝑔(𝑆𝑖−1 \\\\ {𝑢𝑖 }) is maximized (Lines 3∼4). This process is repeated until 𝑆𝑖 becomes empty, resulting in a sequence of nested sets 𝑆0, 𝑆1, . . . , 𝑆 |𝑉 | . The algorithm ultimately returns the subset 𝑆𝑖 ∈ {𝑆0, . . . , 𝑆 |𝑉 |} that maximizes 𝑔(𝑆𝑖 ). Example 2.1 (Seqential Peeling Algorithm Process). Consider the graph 𝐺 as shown in Figure 2. The process begins with an initial graph density of 2.33. In the first iteration, vertex 𝑢1 is peeled due to its smallest peeling weight among all vertices. Subsequently, vertices are peeled in the following order: 𝑢2, 𝑢3, 𝑢4, 𝑢5, and 𝑢6 based on the updated criteria after each peeling operation. Notably, after peeling 𝑢2, the graph density increases, which is a characteristic observation in the peeling process. The sequence of peeling continues until the last vertex, 𝑢6, is peeled, resulting in a final graph density of 0. The peeling sequence, therefore, is 𝑂 = [𝑢1, 𝑢2, 𝑢3, 𝑢4, 𝑢5, 𝑢6], completely disassembling the graph. The induced subgraph𝐺[𝑆] of 𝑆 = [𝑢3, 𝑢4, 𝑢5, 𝑢6] has the greatest density of 2.75, thus 𝐺[𝑆] is returned. Approximation. An algorithm 𝑄 is defined as an 𝛼-approximation for the Densest Subgraph Discovery (DSD), where 𝛼 ≥ 1, if for any given graph it returns a subset 𝑆 satisfying the condition: 𝑔(𝑆) ≥ 𝑔(𝑆∗) 𝛼 , where 𝑆∗ is the optimal subset that maximizes the density metric 𝑔. Theorem 2.1 ([24]). For the vertex set 𝑆𝑝 returned by Algorithm 1 and the optimal vertex set 𝑆∗, it holds that 𝑔(𝑆𝑝 ) ≥ 𝑔(𝑆∗) 2 for DG, DW and FD as the density metrics. We extend the analysis to TDS and 𝑘CLiDS. The proof of the following theorem is omitted due to space limitation. Theorem 2.2. For the vertex set 𝑆𝑝 returned by Algorithm 1 and the optimal vertex set 𝑆∗, it holds that 𝑔(𝑆𝑝 ) ≥ 𝑔(𝑆∗) 𝑘 for TDS and 𝑘CLiDS as the density metrics where 𝑘 = 3 for TDS. 2.3 Related work Densest Subgraph Discovery (DSD). Densest subgraph discov- ery represents a core area of research in graph theory, extensively explored in literature [7, 8, 20, 30, 39, 40]. The foundational max-flow- based algorithm by Goldberg et al. [21] set a benchmark for exact densest subgraph detection, with subsequent studies introducing more scalable exact solutions [33, 34]. While speed enhancements have been achieved through various greedy approaches [6, 8, 24, 33], these methods are inherently sequential and difficult to parallelize. For example, they often peel one vertex at a time, which results in too many peeling iterations. This sequential nature limits their scalability and efficiency on massive graphs. In graph-based fraud de- tection, techniques like COPYCATCH [4] and GETTHESCOOP [27] leverage local search heuristics to identify dense subgraphs in bi- partite networks, frequently applied in fraud, spam, and community detection across social and review platforms [24, 37, 42]. Despite their widespread use, these methods suffer from efficiency issues in very large graphs, often involving numerous iterations and lack- ing effective pruning techniques, which exacerbates the long-tail problem. Moreover, they are limited to specific definitions of dense subgraphs. Contrarily, Dupin offers a versatile, parallel approach to densest subgraph detection. It facilitates the simultaneous peeling of multiple vertices, drastically reducing the computational overhead. Additionally, Dupin provides APIs that allow users to customize defi- nitions of dense subgraphs, ensuring that the resultant graph density adheres to theoretical bounds. DSD on Dynamic Graphs. To tackle real-time fraud detection, sev- eral variants have been designed for dynamic graphs [10, 18, 26, 28, 43]. [28] addresses real-time fraud community detection on evolving graphs (edge insertions) using an incremental peeling sequence re- ordering method, while [26] extends this approach to fully dynamic graphs (edge insertions and deletions). [43] proposes an incremental algorithm that maintains and updates a dense subtensor in a ten- sor stream. [12] and [36] utilize sliding windows to detect dense subgraphs and bursting cores within specific time windows. These methods have three limitations. First, response time is an issue. Al- though they respond quickly to benign communities, the results can change drastically for newly formed fraudulent communities, caus- ing significant delays in incremental calculations for fraud detection. Second, efficiency is problematic. While incremental processing is faster than recomputation for a single transaction on billion-scale graphs, frequent incremental evaluations can be slower than a full recomputation. Third, in terms of flexibility, most methods support only a single semantic. In contrast, Dupin introduces a unified parallel framework for peeling-based dense subgraph detection.Dupin can respond within seconds on billion-scale graphs and defines multiple density metrics that can be used on the Dupin platform. Parallel DSD. Several parallel methods for DSD have been pro- posed [2, 14, 16, 39, 41, 42]. Bahmani et al. [2] proposed a parallel algorithm for DG using the MapReduce framework. FWA [15] proposed a parallel algorithm for DW based on Frank-Wolfe algo- rithm [25] using OpenMP [13]. Some works propose parallel 𝑘-core algorithms [32, 41, 44]. Others introduce parallel algorithms for 𝑘- Clique detection, with approaches offering a (1 + 𝜖)-approximation ratio and parallel implementations for the 𝑘CLiDS problem [16, 45]. There are also algorithms for 𝑘-Clique densest subgraph detection using OpenMP [14], and methods for dense subtensor detection [42]. However, these systems have two main limitations. First, they typi- cally support only a single-densitymetric and do not handle weighted graphs. In real-world applications, transactions between users can have varying degrees of importance based on amounts, frequen- cies [28], and other factors. Extending these systems to support other dense metrics with theoretical guarantees is non-trivial. Second, they often lack efficient pruning techniques, requiring numerous itera- tions and suffering from the long-tail issue, which hinders scalability on massive graphs. In contrast, Dupin defines the characteristics of a set of supported dense metrics and accommodates weighted sub- graphs, allowing for more nuanced analysis reflective of real-world data. Our method introduces efficient pruning strategies by peeling multiple vertices in parallel based on a threshold mechanism, signifi- cantly reducing the number of iterations and mitigating the long-tail problem. This makes Dupin more effective and scalable in diverse real-world scenarios compared to previous approaches. 3 THE Dupin FRAMEWORK 3.1 Motivations A notable example of advanced fraud detection is seen in Anon, a prominent technology company in Southeast Asia known for its digital payment and food delivery services. As delineated in [28], the detection process begins with forming a transaction graph,𝐺 , derived from the platform’s transactions. This graph is incrementally updated, represented as 𝐺 = 𝐺 ⊕ ∆𝐺 . Subsequent stages utilize densest subgraph discovery algorithms to uncover fraudulent communities. A typical fraud scenario on the platform involves customer-merchant collusion, where fake accounts are created to exploit promotional incentives. These fraudulent activities manifest as densest subgraphs within the transaction graph. Efficiency. Our empirical findings reveal that the incremental execution of DSD algorithms, such as Spade [28], on a graph with 2 billion edges requires approximately 198 seconds per batch (default setting: one batch of 1K edge insertions). However, in practical business sce- narios, the number of incoming edges can peak at 400K per minute. Clearly, using incremental evaluation on billion-scale graphs does not provide sufficient throughput to meet business requirements. Latency and Prevention Ratio. Existing DSD systems face sig- nificant challenges in terms of latency and prevention ratio. Upon identification of fraudulent activities, actions such as account bans or freezes are implemented to prevent further losses. However, the aver- age latency for these systems to detect fraud and take action is often high, typically around 200 to 300 seconds for incremental algorithms and more than 1000 seconds for existing parallel algorithms, which is insufficient for real-time requirements (operational demand requires detection within seconds). Moreover, when the graph size increases from million-scale to billion-scale, the prevention ratio decreases from 92.47% to 45%. These issues underscore the necessity for more efficient and scalable algorithms that can maintain low latency and high prevention ratios, even under heavy transaction loads. Parallelization offers a viable solution to these challenges by dis- tributing the computational load across multiple processors. Mod- ern CPUs are equipped with numerous cores that are cost-effective, enabling parallel processing. If the peeling process of DSD is paral- lelized, different processors can simultaneously evaluate and peel different nodes, thereby increasing scalability and efficiency. For example, if we can improve processing speed by one to two orders of magnitude, the system would be able to meet industry requirements for real-time fraud detection, completing detection within seconds. Problem Statement. Given a graph 𝐺 = (𝑉 , 𝐸) and a community detection semantic with density metric 𝑔(𝑆), our goal is to find a vertex subset 𝑆∗ ⊆ 𝑉 that maximizes 𝑔(𝑆∗). 3.2 System Architecture To enable users to design complex DSD algorithms to detect fraudsters based on our framework, Dupin, we have introduced the archi- tecture, APIs, and supported density metrics in this section. Architecture of Dupin. Figure 3 presents the architecture of Dupin with user-defined functions for detecting suspicious activities.Dupin enables users to define their fraud detection semantics through a set of provided APIs. Within Dupin’s computational engine, we sup- port both sequential and parallel peeling algorithms. Additionally, algorithms for long-tail pruning techniques are implemented to ac- celerate convergence and enhance the density of subgraphs in each iteration. Dupin identifies fraudulent communities, which are then flagged for further investigation by moderators. APIs of Dupin (Figure 3). Dupin simplifies the process of defining detection semantics by allowing users to specify a few simple functions. Additionally, Dupin offers APIs that enable users to balance detection speed and accuracy effectively. The key APIs encompass several components designed to facilitate the customization and op- timization of fraud detection processes. These components provide a robust framework for users to implement their fraud detection strate- gies, ensuring both high performance and precision in identifying fraudulent activities. Details are listed below: • pWeight and updateNgh. These functions are designed for the dynamic adjustment of weights within the network. They allow the user to define the peeling weight of a vertex and specify the method for updating the peeling weight of neighboring vertices after a vertex has been peeled. • VSusp and ESusp. Given a vertex/edge, these components are re- sponsible for deciding the suspiciousness of the endpoint of the edge or the edge with a user-defined strategy. • isBenign. This component is used to decide whether a vertex is benign during the whole peeling process (Section 5). If the vertex is benign, it is peeled within the current iteration. • setEpsilon. This function is utilized to control the precision of fraud detection. During periods of high system load, moderators can increase 𝜖 to achieve higher throughput. Conversely, when the system load is low, 𝜖 can be decreased to enhance accuracy. • setK. This function specifies the 𝑘-Clique parameter, enabling the definition of the 𝑘-Clique for analysis. 1 double vsusp(const Vertex& v, const Graph& g) { 2 return g.weight[v]; // Side information on vertex 3 } 4 double esusp(const Edge& e, const Graph& g) { 5 return 1.0 / log(g.deg[e.src] + 5.0); 6 } 7 int main() { 8 Dupin dupin; 9 dupin.VSusp(vsusp); // Plug in vsusp 10 dupin.ESusp(esusp); // Plug in esusp 11 dupin.setEpsilon (0.1); 12 dupin.LoadGraph(\"graph_sample_path\"); 13 vector <Vertex > fraudsters = dupin.ParDetect (); 14 return 0; 15 } Listing 1: Implementation of FD on Dupin Example (Listing 1). To implement FD [24] on Dupin, users simply need to plug in the suspiciousness function vsusp for the vertices by calling VSusp and the suspiciousness function esusp for the edges by calling ESusp. Specifically, 1) vsusp is a constant function, i.e., given a vertex 𝑢, vsusp(𝑢) = 𝑎𝑖 and 2) esusp is a logarithmic function such that given an edge (𝑢𝑖 , 𝑢 𝑗 ), esusp(𝑢𝑖 , 𝑢 𝑗 ) = 1 log(𝑥+𝑐) , where 𝑥 is the degree of the object vertex between 𝑢𝑖 and 𝑢 𝑗 , and 𝑐 is a positive constant [24]. Developers can easily implement customized peeling algorithms with Dupin, significantly reducing the engineering effort. Density Metrics Support in Dupin. To illustrate the versatility of Dupin in handling various density metrics, we establish comprehensive sufficient conditions. These conditions ensure Dupin’s adaptability in integrating a wide range of metrics to assess graph density effectively. This foundational approach enablesDupin to sup- port diverse density calculations, crucial for nuanced fraud detection and network analysis tasks. Property 3.1. Dupin supports a density metric 𝑔(𝑆) if the following conditions are met: (1) 𝑔(𝑆) = 𝑓 (𝑆) |𝑆 | , where 𝑓 is a monotone increasing function. (2) 𝑎𝑖 ≥ 0, ensuring non-negative vertex weight function. (3) 𝑐𝑖 𝑗 ≥ 0, ensuring non-negative edge weight function.',\n",
       " '4 PARALLELABLE PEELING ALGORITHMS': 'For sequential peeling algorithms, only one vertex can be removed at a time, with each vertex’s removal depending on the removal of previous vertices. This dependency prevents the algorithm from executing multiple vertex removals simultaneously. In this section, we introduce a parallel peeling paradigm to break this dependency chain. This new approach allowsmultiple vertices to be peeled in parallel without affecting the programming abstractions that Dupin provides to end users. We then demonstrate how this paradigm offers fine-grained trade-offs between parallelism and ap- proximation guarantees for all density metrics studied in this work. 4.1 Parallel Peeling Paradigm Instead of peeling the vertex with the maximum peeling weight, we can peel all vertices that lead to a significant decrease in the density scores of the remaining graph. This decrease is controlled by a tunable parameter 𝜖 . A larger 𝜖 allows more vertices to be peeled in parallel, while a smaller 𝜖 provides a better approximation ratio. Algorithm 2: Dupin: Parallel Peeling Input:𝐺 = (𝑉 , 𝐸), a density metric 𝑔(𝑆), and 𝜖 ≥ 0 Output: A subset of vertices 𝑆 that maximizes the density metric 𝑔(𝑆) 1 𝑆0 ← 𝑉 /* Initialize the subset with all vertices */ 2 𝑖 ← 1 /* Initialize the index */ 3 while 𝑆𝑖−1 ̸= ∅ do 4 𝜏 = 𝑘(1 + 𝜖) · 𝑔(𝑆𝑖−1) /* The threshold for peeling */ 5 𝑈 ← {𝑢 | 𝑤𝑢 (𝑆𝑖−1) ≤ 𝜏 } /* Vertices to be peeled */ 6 𝑆𝑖 ← 𝑆𝑖−1 \\\\𝑈 /* Update the subset */ 7 𝑖 ← 𝑖 + 1 /* Increment the index */ 8 return arg max𝑆𝑖 𝑔(𝑆𝑖 ) /* The subset that maximizes 𝑔(𝑆) */ Parallel Peeling (Algorithm 2). Let 𝑆𝑖 denote the vertex set after the 𝑖-th peeling iteration. Initially, 𝑆0 = 𝑉 (Line 1). In each iteration, it removes a subset of vertices𝑈 from 𝑆𝑖−1, where ∀𝑢 ∈ 𝑈 ,𝑤𝑢 (𝑆𝑖−1) ≤ 𝑘(1 + 𝜖)𝑔(𝑆𝑖−1). Parameter 𝑘 is determined by the metric used. 𝑘 = 2 works for DG, DW and FD. 𝑘 = 3 works for TDS and 𝑘 is the size of the cliques for 𝑘CLiDS. This process is repeated recursively until no vertices are left, resulting in a series of vertex sets 𝑆0, . . . , 𝑆𝑅 , where 𝑅 is the number of iterations. The algorithm then returns the set 𝑆𝑖 (𝑖 ∈ [0, 𝑅]) that maximizes the density metric 𝑔(𝑆𝑖 ), denoted as 𝑆𝑝 . In what follows, we also show a case where 𝑘CLiDS is the density metric for DSD. 3 3 6 6 5 4 21 0 00 3 3 6 6 3 3 0 0 Density: 0.91 Density: 1.33 Density: 0 Iteration i Iteration i+1 Iteration i+2 Iteration i+3 Density: 0 τ = 3× 0.91 = 2.73 τ = 3× 1.33 = 3.99 τ = 0 τ = 0 4.2 Theoretical Analysis In this section, we analyze the theoretical properties of the parallel peeling process. We begin by examining the number of iterations required for parallel peeling. Built upon the result, we then determine the time complexity and the approximation ratio of the parallel peeling process. Lemma 4.1. Given any 𝜖 > 0, 𝑅 < log 1+𝜖 |𝑉 |. Proof. We prove that for any 𝜖 ≥ 0, the size of the remaining vertex set is bounded by |𝑆𝑖 |< 1 1+𝜖 |𝑆𝑖−1 | for all 𝑖 ∈ [1, 𝑅]. We first examine the peeling weights of the vertices in 𝑆𝑖−1 for the density metrics DG. DW and FD where 𝑘 = 2. As each edge weight in the peeling weight calculation contributes twice (once for each endpoint), and the vertex weights contribute once, we have∑︁ 𝑢∈𝑆𝑖−1 𝑤𝑢 (𝑆𝑖−1) ≤ 2𝑓 (𝑆𝑖−1), (3) The peeling weights of the vertices in 𝑆𝑖−1 are calculated as follows:∑︁ 𝑢∈𝑆𝑖−1 𝑤𝑢 (𝑆𝑖−1) = ∑︁ 𝑢∈𝑆𝑖−1\\\\𝑆𝑖 𝑤𝑢 (𝑆𝑖−1) + ∑︁ 𝑢∈𝑆𝑖 𝑤𝑢 (𝑆𝑖−1) ≥ ∑︁ 𝑢∈𝑆𝑖 𝑤𝑢 (𝑆𝑖−1) > 2(1 + 𝜖)|𝑆𝑖 |𝑔(𝑆𝑖−1) (4) Combining Equation 3 and Equation 4, we have: 2(1 + 𝜖)|𝑆𝑖 |𝑔(𝑆𝑖−1) < 2𝑓 (𝑆𝑖−1) = 2|𝑆𝑖−1 |𝑔(𝑆𝑖−1) (5) For all 𝑖 ∈ [1, 𝑅], |𝑆𝑖 |< 1 1 + 𝜖 |𝑆𝑖−1 | (6) Then we have: |𝑉 |= |𝑆0 |> 1 1 + 𝜖 |𝑆1 |> . . . > ( 1 1 + 𝜖 ) 𝑅 |𝑆𝑅 | (7) Therefore, we have 𝑅 < log 1+𝜖 |𝑉 |. For the density metrics TDS and 𝑘CLiDS, we use the fact that each edge contributes to the peeling weight calculation exactly 𝑘 times due to the counting of each edge within every 𝑘-Clique in the graph (𝑘 = 3 for TDS). This multiplicity of edge contributions leads to the relationship: ∑︁ 𝑢∈𝑆𝑖−1 𝑤𝑢 (𝑆𝑖−1) ≤ 𝑘 𝑓 (𝑆𝑖−1), (8) We then adapt Equations 4-7 and the proof follows. □ Time Complexity. Algorithm 2 operates for at most 𝑅 = log 1+𝜖 |𝑉 | iterations, with each iteration scanning all vertices, taking 𝑂(|𝑉 |) time. As vertices are peeled, updates are necessitated for the peeling weights of their neighboring vertices. Over the course of 𝑅 iterations, there are no more than |𝐸 | such updates for DG, DW and FD. Consequently, Algorithm 3 incurs 𝑂((|𝐸 |+|𝑉 |) log 1+𝜖 |𝑉 |) node/edge weight updates. For DG, DW and FD, the update costs are 𝑂(1). For TDS and 𝑘CLiDS, we follow previous studies [11, 14] to compute the peeling weights, and the complexity is 𝑂(𝑘 |𝐸 |𝛼(𝐺) 𝑘−2 ), where 𝛼(𝐺) is the arboricity of the graph. Hence, the overall complexity is 𝑂(𝑘 |𝐸 |𝛼(𝐺) 𝑘−2 (|𝐸 |+|𝑉 |) log 1+𝜖 |𝑉 |). The cost of peeling weight up- dates is orthogonal to the Dupin framework. Approximation Ratio. Next, we are ready to show that Dupin provides a theoretical guarantee with a 𝑘(1 + 𝜖)-approximation for all density metrics studied in this work. We denote 𝑢𝑖 ∈ 𝑆𝑝 as the first vertex in 𝑆∗ peeled by Dupin, and 𝑆 ′ as the peeling subsequence starting from 𝑢𝑖 . Theorem 4.2. For the vertex set 𝑆𝑝 returned by Dupin and the optimal vertex set 𝑆∗, the relationship 𝑔(𝑆𝑝 ) ≥ 𝑔(𝑆∗) 𝑘(1+𝜖) holds. Proof. We first prove that ∀𝑢𝑖 ∈ 𝑆∗, 𝑤𝑢𝑖 (𝑆 ∗ ) ≥ 𝑔(𝑆∗). We prove it in contradiction. We assume that 𝑤𝑢𝑖 (𝑆 ∗ ) < 𝑔(𝑆∗). By peeling 𝑢𝑖 from 𝑆∗, we have the following. 𝑔(𝑆∗ \\\\ {𝑢𝑖 }) = 𝑓 (𝑆∗) −𝑤𝑢𝑖 (𝑆 ∗ ) |𝑆∗ |−1 > 𝑓 (𝑆∗) − 𝑔(𝑆𝑖 ) |𝑆∗ |−1 = 𝑓 (𝑆∗) − 𝑔(𝑆∗) |𝑆∗ |−1 = 𝑓 (𝑆∗) − 𝑓 (𝑆∗) |𝑆∗ | |𝑆∗ |−1 = 𝑔(𝑆∗) (9) Therefore, a better solution can be obtained by removing 𝑢𝑖 from 𝑆∗, which contradicts the fact that 𝑆∗ is the optimal solution. We can conclude that𝑤𝑢𝑖 (𝑆 ∗ ) ≥ 𝑔(𝑆∗). Next, we assume that 𝑢𝑖 ∈ 𝑆𝑝 is the first vertex in 𝑆∗ peeled by the peeling algorithm. Since 𝑆∗ is the optimal vertex set, we have 𝑔(𝑆∗) ≤ 𝑤𝑢𝑖 (𝑆 ∗ ), (10) because the optimal value 𝑔(𝑆∗) must be less than or equal to the weight contribution of any single vertex in 𝑆∗, in particular 𝑢𝑖 . Since 𝑆∗ ⊆ 𝑆 ′, it is clear that 𝑤𝑢𝑖 (𝑆 ∗ ) ≤ 𝑤𝑢𝑖 (𝑆 ′ ) (11) By the peeling condition, we have 𝑤𝑢𝑖 (𝑆 ′ ) ≤ 𝑘(1 + 𝜖)𝑔(𝑆 ′) (12) Therefore, we have 𝑔(𝑆∗) ≤ 𝑤𝑢𝑖 (𝑆 ∗ ) ≤ 𝑤𝑢𝑖 (𝑆 ′ ) ≤ 𝑘(1 + 𝜖)𝑔(𝑆 ′) ≤ 𝑘(1 + 𝜖)𝑔(𝑆𝑝 ) (13) Hence, we can conclude that 𝑔(𝑆𝑝 ) ≥ 𝑔(𝑆∗) 𝑘(1+𝜖) . □ Summary. Dupin can easily balance efficiency and the worst-case approximation ratio with a single parameter, 𝜖 . A larger 𝜖 allows more vertices to be peeled in parallel, reducing the number of peeling iterations. Meanwhile, the worst-case approximation ratio of parallel peeling is only affected by a factor of (1 + 𝜖) compared to the ratio for sequential peeling w.r.t. all density metrics studied in this work.',\n",
       " '5 LONG-TAIL PRUNING': 'We observe that the parallel peeling algorithm encounters two sig- nificant issues. First, there is a long-tail problem where subsequent iterations fail to produce denser subgraphs. Instead, each iteration only peels off a small fraction of vertices, which significantly impacts the overall runtime efficiency. Second, during batch peeling, each peeling iteration often results in disparate structures. These are pri- marily caused by the peeling of certain vertices, which leaves their neighboring structure disconnected and fragmented. This phenome- non affects the continuity and coherence of the subgraphs. As shown in Table 2, on the la dataset (the largest social network dataset tested in our experiments), DG, DW, and FD experience 24.67%, 46.84%, and 3.01% of rounds as ineffective long-tail iterations, severely limiting response time. Additionally, during the peeling process, if the batch size is large (e.g., 𝜖 = 0.5), approximately 25.34%, 29.46%, and 7.16% of nodes become disconnected and sparse, signifi- cantly impacting the quality of the detected subgraphs. To address the challenges of high iteration counts in peeling algorithms, Dupin introduces two long-tail pruning techniques: Global Peeling Optimization (GPO) to maintain a global peeling threshold, and Local Peeling Optimization (LPO) to improve the density of the current peeling iteration’s subgraph, thereby increasing the peeling threshold. We tested three algorithms, DG, DW, and FD, and found that they required 17,637, 150,223, and 112,074 iterations respectively, which is significantly high. By strategically pruning long-tail ver- tices and sparse vertices, the number of iterations can be reduced by up to 46.84%. Additionally, LPO can further reduce the number of iterations by up to 92.79%. These optimizations significantly enhance the efficiency of the peeling process, as discussed in this section. 5.1 Global Peeling Optimization Initially, we define what constitutes a long-tail vertex during the peeling process. Peeling these long-tail vertices results in ineffective peeling iterations. To address this, we maintain a global maximum peeling threshold to determine which vertices can be peeled directly. When the global peeling threshold is greater than the current itera- tion’s threshold, we use the global peeling threshold for peeling. Algorithm 3: DupinGPO: Global Peeling Optimization Input:𝐺 = (𝑉 , 𝐸), a density metric 𝑔(𝑆), and 𝜖 ≥ 0 Output: A subset of vertices 𝑆𝑝 that maximizes the density metric 𝑔(𝑆) 1 𝑆0 ← 𝑉 /* Initialize the subset with all vertices */ 2 𝑖 ← 1 3 𝜏max ← 𝑔(𝑆 0 ) 𝑘(1+𝜖) /* Initialize the threshold */ 4 while 𝑆𝑖−1 ̸= ∅ do 5 𝜏 ← max{𝜏max, 𝑘(1 + 𝜖) · 𝑔(𝑆𝑖−1)} /* Peeling threshold */ 6 𝑈 ← {𝑢 | 𝑤𝑢 (𝑆𝑖−1) ≤ 𝜏 } /* Vertices to be peeled */ 7 𝑆𝑖 ← 𝑆𝑖−1 \\\\𝑈 /* Update the subset */ 8 𝜏max = max{𝜏max, 𝑔(𝑆𝑖 ) 𝑘(1+𝜖) } /* Refine the threshold */ 9 𝑖 ← 𝑖 + 1 10 return arg max𝑆𝑖 𝑔(𝑆𝑖 ) /* The subset that maximizes 𝑔(𝑆) */ Definition 5.1 (Long-Tail vertex). Given an 𝛼-𝑎𝑝𝑝𝑟𝑜𝑥𝑖𝑚𝑎𝑡𝑖𝑜𝑛 DSD algorithm and a set of vertices 𝑆 ⊆ 𝑉 , if𝑤𝑢 (𝑆) < 𝑔(𝑆∗) 𝛼 and 𝑆∗ is the optimal vertex set, then 𝑢 is a long-tail vertex. Lemma 5.1. If ∃𝑢 ∈ 𝑆𝑖 is a long-tail vertex, then 𝑆𝑖 ̸= 𝑆𝑃 . Proof. We prove it in contradiction by assuming that 𝑆𝑖 = 𝑆𝑃 . Since 𝑢 is a long-tail vertex in 𝑆𝑖 ,𝑤𝑢 (𝑆𝑖 ) < 𝑔(𝑆∗) 𝛼 < 𝑔(𝑆𝑃 ). By peeling 𝑢 from 𝑆𝑃 , we have the following. 𝑔(𝑆𝑃 \\\\ {𝑢}) = 𝑓 (𝑆𝑃 ) −𝑤𝑢 (𝑆𝑃 ) |𝑆𝑃 |−1 > 𝑓 (𝑆𝑃 ) − 𝑔(𝑆𝑃 ) |𝑆𝑃 |−1 = 𝑓 (𝑆𝑃 ) − 𝑓 (𝑆𝑃 ) |𝑆𝑃 | |𝑆𝑃 |−1 = 𝑔(𝑆𝑃 ) (14) A denser subgraph can be obtained by peeling 𝑢 from 𝑆𝑃 . This contradicts that 𝑆𝑃 is the optimal solution. Hence, 𝑆𝑖 ̸= 𝑆𝑃 . □ Identifying Long-Tail Vertices. Based on Lemma 5.2, we can also establish that a vertex𝑢 in set 𝑆 can be classified as a long-tail vertex if 𝑤𝑢 (𝑆) < 𝑔(𝑆∗) 𝑘(1+𝜖) . This insight allows us to implement a global peeling threshold, denoted as 𝜏max. During each iteration, we compare 𝑔(𝑆𝑖 ) 𝑘(1+𝜖) with 𝜏max. If the former exceeds the latter, Dupin updates 𝜏max to 𝑔(𝑆𝑖 ) 𝑘(1+𝜖) , thereby enhancing the efficiency of the peeling. Peeling with Global Threshold (Algorithm 3). Using FD as an example, we can set 𝑘 to 2. In Lemma 5.2, we recognize that long-tail vertices in set 𝑆𝑖 can be peeled directly during iteration 𝑖 for FD. To facilitate this, Dupin initiates with a global peeling threshold 𝜏max (Line 3). After identifying a denser subgraph post-peeling, 𝜏max is updated to 𝑔(𝑆𝑖 ) 2(1+𝜖) , refining the peeling process (Line 8). Example 5.1. In the initial depiction provided in the leftmost panel of Figure 6 (the 𝑖-th iteration), the density of the graph is 4.28. Traditionally, this scenario would necessitate two additional iterations to complete the peeling process, which could notably increase computational time. As illustrated, vertex 𝑢 cannot be peeled in the (𝑖 + 1)-th iteration and would typically require an extra iteration, the (𝑖 + 2)-th, for its removal. However, Dupin can promptly peel vertices that fall below this threshold by employing our algorithm’s global peeling threshold, which is calculated as half of the current density (𝜏max = 4.28/2 = 2.14). This approach enables the simultaneous peeling of three nodes in the (𝑖 + 1)-th iteration. Without this pruning strategy, an additional iteration would be essential to peel vertex 𝑢, underscoring our method’s enhanced efficiency in the peeling process. This optimization obviates the necessity for a prolonged iterative sequence. The example herein substantiates the efficacy of our tai- lored optimization for long-tail distribution, substantially enhancing the algorithm’s time efficiency. 5.2 Local Peeling Optimization Algorithm 4: DupinLPO: Local Peeling Optimization Input: A graph𝐺 = (𝑉 , 𝐸), a density metric 𝑔(𝑆), and 𝜖 ≥ 0 Output: A subset of vertices 𝑆𝑝 representing the densest subgraph 1 𝑆0 ← 𝑉 /* Initialize the subset with all vertices */ 2 𝜏max ← 𝑔(𝑆 0 ) 𝑘(1+𝜖) /* Initialize the threshold */ 3 while 𝑆𝑖−1 ̸= ∅ do 4 𝜏1 ← max{𝜏max, 𝑘(1 + 𝜖) · 𝑔(𝑆𝑖−1)} /* Peeling threshold */ 5 𝑈1 ← {𝑢 ∈ 𝑆 | 𝑤𝑢 (𝑆) ≤ 𝜏1 } 6 𝑆𝑖 ← 𝑆𝑖−1 \\\\𝑈1 7 while ∃𝑢 ∈ 𝑆𝑖 , 𝑤𝑢 (𝑆𝑖 ) < 𝑔(𝑆𝑖 ) do 8 𝜏2 ← max{𝜏max, 𝑔(𝑆𝑖 )} /* Update the threshold */ 9 𝑈2 ← {𝑢 | 𝑤𝑢 (𝑆𝑖 ) < 𝜏2 } 10 𝑆𝑖 ← 𝑆𝑖 \\\\𝑈2 11 𝜏max = max{𝜏max, 𝑔(𝑆𝑖 ) 𝑘(1+𝜖) } /* Update the threshold */ 12 return arg max𝑆𝑖 𝑔(𝑆𝑖 ) /* The subset that maximizes 𝑔(𝑆) */ It is important to note that each peeling iteration often results in disparate structures. This is primarily caused by the peeling of cer- tain vertices, which leaves their neighboring structures disconnected and fragmented. This not only affects the density of the detected sub- graph but also lowers the peeling threshold. Lemma 5.2 establishes a foundational property of vertices within a dense graph. Lemma 5.2. ∀𝑢 ∈ 𝑆 , if𝑤𝑢 (𝑆) < 𝑔(𝑆), 𝑔(𝑆 \\\\ {𝑢}) > 𝑔(𝑆). Proof. By peeling 𝑢 from 𝑆 , we have the following. 𝑔(𝑆 \\\\ {𝑢}) = 𝑓 (𝑆) −𝑤𝑢 (𝑆) |𝑆 |−1 > 𝑓 (𝑆) − 𝑔(𝑆𝑖 ) |𝑆 |−1 = 𝑓 (𝑆) − 𝑔(𝑆) |𝑆∗ |−1 = 𝑓 (𝑆) − 𝑓 (𝑆) |𝑆 | |𝑆 |−1 = 𝑔(𝑆) (15) Therefore, a denser graph can be obtained by peeling 𝑢 from 𝑆 . □ According to Lemma 5.2, we introduce local peeling optimization within the iteration. After each peeling iteration, if the peeling weight of a vertex is smaller than the current density, Dupin will trim it. We implement a local peeling optimization (Lines 7-10, Algorithm 4) before the next iteration. After a vertex 𝑢 is peeled, Dupin invokes updateNgh to update the peeling weights of 𝑢’s neighboring nodes. If any of these neighbors have a peeling weight less than the current density, they are also peeled, thereby increasing the density of the current iteration. Consequently, when an iteration yields globally optimal results, a denser structure is obtained. The benefits of this approach extend beyond density improvements; performance is also enhanced. By trimming these vertices, we reduce the number of vertices to traverse in subsequent iterations, and a higher density enables more efficient iterations due to the possibility of using larger peeling thresholds. Lemma 5.3. If 𝑢𝑖 ∈ 𝑆𝑝 is the first vertex in 𝑆∗ removed by Dupin, 𝑢𝑖 will not be removed during the local peeling optimization process. Proof. Assume, for the sake of contradiction, that 𝑢𝑖 is trimmed during the local peeling optimization process. Let 𝑆 ′ be the set of vertices left before 𝑢𝑖 is trimmed. By definition, we have: 𝑤𝑢𝑖 (𝑆 ′ ) < 𝑔(𝑆∗). Since 𝑢𝑖 is the first vertex in 𝑆∗ to be removed, 𝑆∗ ⊆ 𝑆 ′, therefore: 𝑤𝑢𝑖 (𝑆 ∗ ) ≤ 𝑤𝑢𝑖 (𝑆 ′ ) < 𝑔(𝑆∗). This contradicts Lemma 5.2, which states that ∀𝑢𝑖 ∈ 𝑆∗: 𝑤𝑢𝑖 (𝑆 ∗ ) ≥ 𝑔(𝑆∗). Otherwise, a better solution can be obtained by removing 𝑢 from 𝑆∗, which contradicts the fact that 𝑆∗ is the optimal solution. Thus, 𝑢𝑖 cannot be trimmed. Hence,𝑢𝑖 will not be removed during the local peeling optimization process. □ Due to Lemma 5.3, 𝑢𝑖 will only be removed during the peeling process. Hence, the approximation ratios (Theorem 4.2) for different density metrics are preserved. 6 EXPERIMENTAL STUDY Our experiments are run on a machine that has an X5650 CPU, 512 GB RAM. The implementation is made memory-resident and implemented in C++. All codes are compiled by GCC-9.3.0 with -𝑂3. Datasets. We report the datasets used in our experiment in Table 3. One industrial dataset is from Anon (gfg). Datasets links-anon(la) and Twitter-rv (rv)were obtained from Stanford NetworkDataset Collection [31]. Datasets kron-logn21(kron), soc-twitter(soc), Web-uk-2005(uk) and Web-sk-2005(sk) were obtained from Network Data Repository [38]. Dataset bio was from the BIOMINE [35]. Baseline Methods. In this section, we introduce five commonly used density metrics pivotal in analyzing network structures:DG [6], DW [23], FD [24], TDS [46], and𝑘CLiDS [14].We implemented these metrics in our framework, Dupin, and compared their performance with frameworks including Spade [28], kCLIST [14], GBBS [16], PBBS [41], PKMC [32], FWA [15] and ALENEX [44]. kCLIST [14] and PBBS [41] are parallel 𝑘CLiDS algorithms, while GBBS [16] supports the DG density metric. Although GBBS does not support weighted graphs, we precompute the peeling weights offline and then import them into GBBS. The time spent on this offline computation is not included in our reported runtime. FWA, ALENEX and PKMC support the DG density metric. For clarity, we denote the incremental peeling process, which involves the iterative removal of graph elements based on density scores, as Spade-X, where X corresponds to each metric, under the same experimental condi- tions described in [28], where the batch size is set to 1K, consistent with the default setting in [28]. We report the average runtime per batch for Spade. DupinGPO-X and DupinLPO-X represent our par- allel peeling algorithms that incorporate global and local peeling optimization. Default Settings. In our experiments, the parameter 𝜖 is set to 0.1 by default, and the number of threads 𝑡 is configured to 128. These settings are used consistently unless otherwise specified. 6.1 Efficiency of Dupin Spade vs Dupin. Our initial comparison focuses on the performance disparity between the incremental and parallel peeling algorithms. Specifically, Dupin-DG (resp. Dupin-DW, Dupin-FD, Dupin-TDS, andDupin-𝑘CLiDS) demonstrates a speedup factor of 6.97 (resp. 7.71, 7.71, 7.65, and 8.46) over Spade-DG (resp. Spade-DW, Spade-FD, Spade-TDS, and Spade-𝑘CLiDS). Notably, Spade-TDS and Spade𝑘CLiDS only complete computations within 7,200 seconds on gfg. The bottleneck for Spade arises in larger graphs where initial calculations of triangle and 𝑘-Clique counting are unable to finish. Dupin outpaces Spade primarily because Spade suffers from frequent re- ordering of peeling sequences when there are many increments, which proves to be slower than recalculating. GBBS vs Dupin. Previous works have not provided simultaneous support for all density metrics. To bridge this gap, we implemented DG, DW, and FD within the parallel GBBS framework. On average, Dupin-DG, Dupin-DW, and Dupin-FD demonstrated speedup factors of 6.33, 12.51, and 17.07 times, respectively, over GBBS-DG, GBBS-DW, andGBBS-FD. This performance advantage is attributed to the fundamental difference in the peeling process between the two frameworks. In GBBS, the basic unit of each peel is a bucket, where all nodes within the same bucket have an identical peeling weight. This design limits the parallelism in weighted graphs like those using DW and FD, where many buckets may contain only a single node. In contrast, Dupin peels batches of nodes at each step, achieving higher parallelism and thus consistently outperforming GBBS, particularly in weighted graph scenarios where the speedup is even more pronounced. kCLIST vs PBBS vs Dupin. Both kCLIST and PBBS support the density metrics TDS and 𝑘CLiDS. In our comparisons, Dupin demonstrates notable performance advantages. Specifically, Dupin-TDS is faster than kCLIST-TDS and PBBS-TDS by 39.85 and 62.71 times, respectively. Furthermore, Dupin-𝑘CLiDS also outperforms kCLIST𝑘CLiDS by 4.16 times. A significant finding is that PBBS-𝑘CLiDS fails to complete computations within 7200 seconds on most datasets, highlighting the efficiency of Dupin in handling more complex met- rics under stringent time constraints. Impact of DupinGPO. Next, we evaluate the acceleration effect of DupinGPO, using Dupin as a baseline for comparison. Notably, DupinGPO-DG is found to be 1.14 times faster, DupinGPO-DW is 1.11 times faster,DupinGPO-TDS is 1.10 times faster, andDupinGPO𝑘CLiDS is 1.35 times faster than their respective Dupin implementations. The acceleration is particularly significant for 𝑘CLiDS on larger datasets, such as soc, rv, kron, and sk. DupinGPO-𝑘CLiDS is 1.40, 1.54, 1.51, and 1.61 times faster than Dupin-𝑘CLiDS on soc, rv, kron, and sk, respectively. This is attributed to the greater number of iterations required due to the larger dataset sizes and the higher count of 𝑘-Cliques, necessitating more peeling iterations. In such scenarios,DupinGPO effectively prunes the tail iterations early, thereby reducing the number of rounds needed. Impact of DupinLPO. We next compared the efficiency of Dupin andDupinLPO. On average, thoughDupinLPO-DGwas 10.09% slower than Dupin-DG, the speed is still comparable. For example, on the soc dataset, DupinLPO-DG was 5.94% faster than Dupin-DG. This is because DupinLPO requires some lock operations to update the peeling weights of neighboring vertices during local peeling optimization. However, the advantage is that DupinLPO can detect up to 26.26% denser subgraphs, as detailed in Section 6.2. On larger datasets, such as soc, rv, and la, the vertices being trimmed have relatively low correlations with each other, so the impact of locks is minimal. For example, on the soc (resp. rv), DupinLPO-DG was 5.94% (resp. 9.94%) faster than Dupin-DG. In contrast, on smaller datasets, such as bio and kron, the extra cost of locks is higher. Impact of the Number of Threads 𝑡 . All methods are influenced by the concurrency level, i.e., the number of threads. Generally, the greater the number of threads, the faster the execution speed. In our experiments, using the la dataset as a benchmark, we varied the number of threads 𝑡 from 2 to 128 to observe the impact on runtime performance. For GBBS-DG, runtime stabilizes and accelerates by 3.33 times as thread count increases from 2 to 8. For GBBS-DW, performance deteriorates when the thread count exceeds 32, but from 2 to 32 threads, it accelerates by 3.50 times. Similarly, GBBS-FD shows no significant change in runtime beyond 32 threads, with an acceleration of 3.09 times from 2 to 32 threads. GBBS-TDS and GBBS-𝑘CLiDS fail to complete computations on the la dataset. In contrast, all five methods tested onDupin exhibit good scalability. As the number of threads increases from 2 to 128,Dupin-DG accelerates by 9.91 times, Dupin-DW by 12.69 times, Dupin-FD by 13.03 times, and Dupin-TDS by 28.06 times. Although Dupin-𝑘CLiDS fails to produce results when the thread count is below 16, it accelerates by 1.32 times from 32 to 128 threads. These results demonstrate Dupin’s strong scalability across varied threading levels. 6.2 Effectiveness of Dupin In addition to performance speedup, we evaluate the density of the subgraphs detected by different frameworks. Detailed density comparisons are presented in Table 5. GBBS vs Spade vs Dupin. Firstly, we compare the densities of the subgraphs identified by GBBS-DG (respectively GBBS-DW, and GBBS-FD) to those detected by Dupin in parallel execution. Our experimental results indicate that Dupin maintains comparable subgraph densities with GBBS, signifying that the increase in com- putational efficiency does not compromise the accuracy of the de- tected subgraphs. Specifically, the density of the subgraph detected byGBBS-DG (respectivelyGBBS-DW, andGBBS-FD) is 7.08% (resp. 6.48% and 7.43%) greater than that detected by Dupin-DG (respectively Dupin-DW, and Dupin-FD) on average. Spade shares similar densities with GBBS. Due to Spade’s tendency to accumulate er- rors over time, its density can become inflated. We will explain its limitations in practical applications in more detail in the case study. kCLIST vs PBBS vs Dupin. a) For 𝑘CLiDS, PBBS-𝑘CLiDS could not complete on most of our test datasets, so we did not compare their densities. kCLIST-𝑘CLiDS could not complete on the sk dataset. On the other datasets, Dupin-𝑘CLiDS’s density was only 6.97% smaller than kCLIST-𝑘CLiDS. On some datasets, such as bio and kron,Dupin𝑘CLiDS even detected subgraphs with greater density. b) For TDS, PBBS-TDS could detect subgraphs on smaller graphs but failed on billion-scale graphs such as rv, sk, and la. Dupin-TDS’s density was 2.03% greater than kCLIST-TDS. Therefore, we conclude that Dupin is comparable to existing parallel methods in terms of the density of the detected dense subgraphs. Ablation Study of Dupin. This experiment delves into the effectiveness of our optimizations, DupinGPO and DupinLPO. Our analysis primarily focuses on comparing the densities of dense subgraphs detected by both DupinGPO and DupinLPO in various settings. DupinGPO achieves the same density as Dupin across five den- sity metrics because it trims the long tail, enhancing efficiency. For DupinLPO-DG (resp. DupinLPO-DW, DupinLPO-FD, DupinLPOTDS, and DupinGPO-𝑘CLiDS), we observe that the detected sub- graphs exhibit densities 11.09% (resp. 7.32%, 8.00%, 1.01%, and 26.26%) greater than those detected by Dupin-DG, Dupin-DW, Dupin-FD, Dupin-TDS, andDupin-𝑘CLiDS, respectively, demonstrating notable outcomes. This is because DupinLPO trims the graph in each iteration, resulting in denser subgraphs compared to Dupin. Impact of the Approximation Ratio 𝜖. The approximation ratio 𝜖 serves to control accuracy. In our experiment, we vary 𝜖 from 0.1 to 1 to investigate its influence on the accuracy of Dupin. We observe that as 𝜖 increases from 0.1 to 1, the density of subgraphs detected by Dupin, DupinGPO, and DupinLPO generally decreases. ForDG, the densities ofDupin-DG,DupinGPO-DG, andDupinLPODG decrease by 22.71%, 22.71%, and 11.99%, respectively. The trends for DW and DG are similar, with the worst performance observed at 𝜖 = 0.6 for both density metrics. The trends for FD, TDS, and 𝑘CLiDS are also similar. DupinLPO detects denser subgraphs across most density metrics and is less affected by 𝜖 , due to its iterative trimming. For example, for FD, DupinLPO’s density decreases by only 6.23%, while DupinGPO and Dupin’s densities decrease by 17.37%. 6.3 Case study We also compare the performance in real-world scenarios, focusing on three key aspects: accuracy, latency, and prevention ratio. Accuracy. As discussed in Section 1, although Spade can quickly respond to new transactions, it tends to accumulate errors over time. Our analysis utilizes transaction data from Anon, with the transaction network comprising |𝐸 |= 2 billion edges. Under the incremental computation framework of Spade, the assumption is made that the weights of existing edges remain constant, thereby neglecting the impact of newly inserted edges on these weights, leading to error accumulation. For instance, using FD density metric, we observe that errors accumulate up to 24.4% within a day and increase to 30.3% over a week, as illustrated in Figure 9. Although Dupin sacrifices some accuracy to achieve parallel computational efficiency, our experiments show that Dupin’s errors remain relatively stable between 3% and 5%. Hence, the detection precision of Spade declines from 87.9% to 68.3%, while Dupin’s precision remains above 86%. Latency and Prevention Ratio. If a transaction is identified as fraudulent, subsequent related transactions are blocked to mitigate potential losses. We define the ratio of successfully prevented sus- picious transactions to the total number of suspicious transactions as R. As shown in Figure 6, the prevention ratio R continues to decrease as latency increases on Anon’s datasets. Using the default density metric, FD, in Anon, our results show thatDupin-FD can prevent 94.5% of fraudulent activities, GBBS-FD can prevent 3.0%, and Spade-FD can prevent 45.3%. The reason for the lower performance of GBBS-FD is its low parallelism in the peeling process, which results in long peeling times and thus an average latency of 6,014 seconds, delaying the prevention of many fraudulent activities. Although Spade-FD performs well in reordering techniques on smaller datasets, in industrial scenarios with billion-scale graphs, latency significantly worsens due to the extensive reordering range required in the peeling sequences as normal users transition to fraudsters, increasing the cost of incremental computations. Additional tests were conducted with other density metrics; for DG, Dupin-DG can prevent 78.8%, while other methods prevent varying amounts. Similarly, for DW, Dupin-DW can prevent 86.1%, while other methods demonstrate different prevention capabilities.',\n",
       " '7 CONCLUSION AND FUTUREWORKS': 'In this paper, we presented Dupin, a novel parallel fraud detection framework tailored for massive graphs. Our experiments demonstrated that Dupin’s advanced peeling algorithms and long-tail pruning techniques, DupinGPO and DupinLPO, significantly enhance the detection efficiency of dense subgraphs without sacrificing accuracy. Comparative studies with GBBS, PBBS, kCLIST, Spade, and Dupin highlighted Dupin’s superior performance in terms of com- putational efficiency and accuracy. Through case studies, we also validated that Dupin achieves lower latency and higher detection ac- curacy compared to existing fraud detection systems on billion-scale graphs. Overall, Dupinproves to be a powerful tool for large-scale fraud detection, providing a scalable and efficient solution for real-time applications. Future work will explore the integration of additional suspiciousness functions and further optimization to handle even larger datasets and more complex fraud scenarios.',\n",
       " 'REFERENCES': '[1] Distil networks: The 2019 bad bot report. https://www.bluecubesecurity.com/wp- content/uploads/bad-bot-report-2019LR.pdf. [2] B. Bahmani, R. Kumar, and S. Vassilvitskii. Densest subgraph in streaming and mapreduce. Proceedings of the VLDB Endowment, 5(5), 2012. [3] Y. Ban, X. Liu, T. Zhang, L. Huang, Y. Duan, X. Liu, and W. Xu. Badlink: Combining graph and information-theoretical features for online fraud group detection. arXiv preprint arXiv:1805.10053, 2018. [4] A. Beutel, W. Xu, V. Guruswami, C. Palow, and C. Faloutsos. Copycatch: stopping group attacks by spotting lockstep behavior in social networks. In Proceedings of the 22nd international conference on World Wide Web, pages 119–130, 2013. [5] D. Boob, Y. Gao, R. Peng, S. Sawlani, C. Tsourakakis, D.Wang, and J.Wang. Flowless: Extracting densest subgraphs without flow computations. In Proceedings of The Web Conference 2020, pages 573–583, 2020. [6] M. Charikar. Greedy approximation algorithms for finding dense components in a graph. In International Workshop on Approximation Algorithms for Combinatorial Optimization, pages 84–95. Springer, 2000. [7] M. Charikar. Greedy approximation algorithms for finding dense components in a graph. In Approximation Algorithms for Combinatorial Optimization, Third International Workshop, APPROX 2000, Saarbrücken, Germany, September 5-8, 2000, Proceedings, volume 1913 of Lecture Notes in Computer Science, pages 84–95. Springer, 2000. [8] C. Chekuri, K. Quanrud, and M. R. Torres. Densest subgraph: Supermodularity, iterative peeling, and flow. In Proceedings of the 2022 Annual ACM-SIAM Symposium on Discrete Algorithms (SODA), pages 1531–1555. SIAM, 2022. [9] J. Chen and Y. Saad. Dense subgraph extraction with application to community detection. IEEE Transactions on knowledge and data engineering, 24(7):1216–1230, 2010. [10] Y. Chen, J. Jiang, S. Sun, B. He, and M. Chen. Rush: Real-time burst subgraph detection in dynamic graphs. Proceedings of the VLDB Endowment, 17(11):3657– 3665, 2024. [11] N. Chiba and T. Nishizeki. Arboricity and subgraph listing algorithms. SIAM Journal on computing, 14(1):210–223, 1985. [12] L. Chu, Y. Zhang, Y. Yang, L. Wang, and J. Pei. Online density bursting subgraph detection from temporal graphs. Proceedings of the VLDB Endowment, 12(13):2353– 2365, 2019. [13] L. Dagum and R. Menon. Openmp: an industry standard api for shared-memory programming. IEEE computational science and engineering, 5(1):46–55, 1998. [14] M. Danisch, O. Balalau, and M. Sozio. Listing k-cliques in sparse real-world graphs. In Proceedings of the 2018 World Wide Web Conference, pages 589–598, 2018. [15] M. Danisch, T.-H. H. Chan, and M. Sozio. Large scale density-friendly graph decomposition via convex programming. In Proceedings of the 26th International Conference on World Wide Web, pages 233–242, 2017. [16] L. Dhulipala, J. Shi, T. Tseng, G. E. Blelloch, and J. Shun. The graph based benchmark suite (gbbs). In Proceedings of the 3rd Joint International Workshop on Graph Data Management Experiences & Systems (GRADES) and Network Data Analytics (NDA), pages 1–8, 2020. [17] Y. Dourisboure, F. Geraci, and M. Pellegrini. Extraction and classification of dense communities in the web. In Proceedings of the 16th international conference on World Wide Web, pages 461–470, 2007. [18] A. Epasto, S. Lattanzi, and M. Sozio. Efficient densest subgraph computation in evolving graphs. In Proceedings of the 24th international conference on world wide web, pages 300–310, 2015. [19] D. Gibson, R. Kumar, and A. Tomkins. Discovering large dense subgraphs in massive graphs. In Proceedings of the 31st international conference on Very large data bases, pages 721–732. Citeseer, 2005. [20] A. Gionis and C. E. Tsourakakis. Dense subgraph discovery: Kdd 2015 tutorial. In Proceedings of the 21th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, pages 2313–2314, 2015. [21] A. V. Goldberg. Finding a maximum density subgraph. 1984. [22] A. V. Goldberg and R. E. Tarjan. A new approach to the maximum-flow problem. Journal of the ACM (JACM), pages 921–940, 1988. [23] N. V. Gudapati, E. Malaguti, and M. Monaci. In search of dense subgraphs: How good is greedy peeling? Networks, 77(4):572–586, 2021. [24] B. Hooi, H. A. Song, A. Beutel, N. Shah, K. Shin, and C. Faloutsos. Fraudar: Bounding graph fraud in the face of camouflage. In Proceedings of the 22nd ACM SIGKDD international conference on knowledge discovery and data mining, pages 895–904, 2016. [25] M. Jaggi. Revisiting frank-wolfe: Projection-free sparse convex optimization. In International conference on machine learning, pages 427–435. PMLR, 2013. [26] J. Jiang, Y. Chen, B. He, M. Chen, and J. Chen. Spade+: A generic real-time fraud detection framework on dynamic graphs. IEEE Transactions on Knowledge and Data Engineering, 2024. [27] M. Jiang, P. Cui, A. Beutel, C. Faloutsos, and S. Yang. Inferring strange behavior from connectivity pattern in social networks. In Pacific-Asia conference on knowledge discovery and data mining, pages 126–138. Springer, 2014. [28] J. Jiaxin, L. Yuan, H. Bingsheng, H. Bryan, C. Jia, and J. K. Z. Kang. A real-time fraud detection framework on evolving graphs. PVLDB, 2023. [29] S. Kumar, W. L. Hamilton, J. Leskovec, and D. Jurafsky. Community interaction and conflict on the web. In Proceedings of the 2018 world wide web conference, pages 933–943, 2018. [30] V. E. Lee, N. Ruan, R. Jin, and C. Aggarwal. A survey of algorithms for dense subgraph discovery. In Managing and mining graph data, pages 303–336. Springer, 2010. [31] J. Leskovec and A. Krevl. SNAP Datasets: Stanford large network dataset collection. http://snap.stanford.edu/data, June 2014. [32] W. Luo, Z. Tang, Y. Fang, C. Ma, and X. Zhou. Scalable algorithms for densest subgraph discovery. In 2023 IEEE 39th International Conference on Data Engineering (ICDE), pages 287–300. IEEE, 2023. [33] C. Ma, Y. Fang, R. Cheng, L. V. Lakshmanan, and X. Han. A convex-programming approach for efficient directed densest subgraph discovery. In Proceedings of the 2022 International Conference on Management of Data, pages 845–859, 2022. [34] M. Mitzenmacher, J. Pachocki, R. Peng, C. Tsourakakis, and S. C. Xu. Scalable large near-clique detection in large-scale networks via sampling. In Proceedings of the 21th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, pages 815–824, 2015. [35] V. Podpečan, Ž. Ramšak, K. Gruden, H. Toivonen, and N. Lavrač. Interactive explo- ration of heterogeneous biological networks with biomine explorer. Bioinformatics, 06 2019. [36] H. Qin, R.-H. Li, Y. Yuan, G. Wang, L. Qin, and Z. Zhang. Mining bursting core in large temporal graphs. Proceedings of the VLDB Endowment, 2022. [37] Y. Ren, H. Zhu, J. Zhang, P. Dai, and L. Bo. Ensemfdet: An ensemble approach to fraud detection based on bipartite graph. In 2021 IEEE 37th International Conference on Data Engineering (ICDE), pages 2039–2044. IEEE, 2021. [38] R. A. Rossi and N. K. Ahmed. The network data repository with interactive graph analytics and visualization. In AAAI, 2015. [39] A. E. Sarıyüce andA. Pinar. Peeling bipartite networks for dense subgraph discovery. In Proceedings of the Eleventh ACM International Conference on Web Search and Data Mining, pages 504–512, 2018. [40] S. B. Seidman. Network structure and minimum degree. Social networks, 5(3):269– 287, 1983. [41] J. Shi, L. Dhulipala, and J. Shun. Parallel clique counting and peeling algorithms. In SIAM Conference on Applied and Computational Discrete Algorithms (ACDA21), pages 135–146. SIAM, 2021. [42] K. Shin, T. Eliassi-Rad, and C. Faloutsos. Corescope: Graph mining using k-core analysis—patterns, anomalies and algorithms. In 2016 IEEE 16th international conference on data mining (ICDM), pages 469–478. IEEE, 2016. [43] K. Shin, B. Hooi, J. Kim, and C. Faloutsos. Densealert: Incremental dense-subtensor detection in tensor streams. In Proceedings of the 23rd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, pages 1057–1066, 2017. [44] P. Sukprasert, Q. C. Liu, L. Dhulipala, and J. Shun. Practical parallel algorithms for near-optimal densest subgraphs on massive graphs. In 2024 Proceedings of the Symposium on Algorithm Engineering and Experiments (ALENEX), pages 59–73. SIAM, 2024. [45] B. Sun, M. Danisch, T. H. Chan, and M. Sozio. Kclist++: A simple algorithm for finding k-clique densest subgraphs in large graphs. Proceedings of the VLDB Endowment (PVLDB), 2020. [46] C. Tsourakakis. The k-clique densest subgraph problem. In Proceedings of the 24th international conference on world wide web, pages 1122–1132, 2015. [47] C. Ye, Y. Li, B. He, Z. Li, and J. Sun. Gpu-accelerated graph label propagation for real-time fraud detection. In Proceedings of the 2021 International Conference on Management of Data, pages 2348–2356, 2021.'}"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "parse_document(full_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def parse_document(text):\n",
    "    # Initialize variables\n",
    "    sections = {}\n",
    "    current_main_section = None\n",
    "    current_content = []\n",
    "\n",
    "    # Split text into lines\n",
    "    lines = text.split('\\n')\n",
    "\n",
    "    # Combine lines to handle headings that span multiple lines\n",
    "    combined_lines = []\n",
    "    i = 0\n",
    "    while i < len(lines):\n",
    "        line = lines[i].strip()\n",
    "        if not line:\n",
    "            i += 1\n",
    "            continue  # Skip empty lines\n",
    "        # Check if this line is a number and the next line is possibly a heading\n",
    "        if re.match(r'^\\d+$', line) and i+1 < len(lines):\n",
    "            next_line = lines[i+1].strip()\n",
    "            # Assume that if the next line is not empty, it's a heading\n",
    "            if next_line:\n",
    "                combined_line = f\"{line} {next_line}\"\n",
    "                combined_lines.append(combined_line)\n",
    "                i += 2  # Skip the next line as it's part of the heading\n",
    "            else:\n",
    "                combined_lines.append(line)\n",
    "                i += 1\n",
    "        else:\n",
    "            combined_lines.append(line)\n",
    "            i += 1\n",
    "\n",
    "    # Define heading patterns\n",
    "    main_heading_pattern = re.compile(r'^(\\d+)\\s+(.*)$')  # Matches headings like '1 INTRODUCTION'\n",
    "    sub_heading_pattern = re.compile(r'^(\\d+\\.\\d+)\\s+(.*)$')  # Matches headings like '2.1 Preliminary'\n",
    "    # Known headings (case-insensitive)\n",
    "    known_headings = {'Abstract', 'Acknowledgements', 'References', 'Conclusion', 'Introduction', 'Background', 'Related Work', 'Methodology', 'Results', 'Discussion', 'Appendix'}\n",
    "    known_headings_lower = [h.lower() for h in known_headings]\n",
    "\n",
    "    for idx, line in enumerate(combined_lines):\n",
    "        line = line.strip()\n",
    "        if not line:\n",
    "            continue  # Skip empty lines\n",
    "\n",
    "        # Check for main numbered heading\n",
    "        m_main = main_heading_pattern.match(line)\n",
    "        if m_main and '.' not in m_main.group(1):\n",
    "            # Save current section\n",
    "            if current_main_section is not None:\n",
    "                content = ' '.join(current_content).strip()\n",
    "                sections[current_main_section] = content\n",
    "            # Start new main section\n",
    "            section_number = m_main.group(1)\n",
    "            section_title = m_main.group(2).strip()\n",
    "            current_main_section = f\"{section_number} {section_title}\"\n",
    "            current_content = []\n",
    "            continue\n",
    "\n",
    "        # Check for unnumbered heading (known headings)\n",
    "        if line.lower() in known_headings_lower:\n",
    "            # Save current section\n",
    "            if current_main_section is not None:\n",
    "                content = ' '.join(current_content).strip()\n",
    "                sections[current_main_section] = content\n",
    "            # Start new main section\n",
    "            current_main_section = line.strip()\n",
    "            current_content = []\n",
    "            continue\n",
    "\n",
    "        # For subheadings and other content\n",
    "        # We include subheadings in the content of the current main section\n",
    "        # So we can check for subheadings and append them to content\n",
    "        m_sub = sub_heading_pattern.match(line)\n",
    "        if m_sub:\n",
    "            # It's a subheading\n",
    "            sub_section_number = m_sub.group(1)\n",
    "            sub_section_title = m_sub.group(2).strip()\n",
    "            current_content.append(f\"{sub_section_number} {sub_section_title}\")\n",
    "            continue\n",
    "\n",
    "        # Append line to current content\n",
    "        if current_main_section is None:\n",
    "            # If no current section, start with 'Abstract'\n",
    "            current_main_section = 'Abstract'\n",
    "        current_content.append(line)\n",
    "\n",
    "    # Save the last section\n",
    "    if current_main_section is not None:\n",
    "        content = ' '.join(current_content).strip()\n",
    "        sections[current_main_section] = content\n",
    "    return sections\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Abstract': 'ABSTRACT Detecting fraudulent activities in financial and e-commerce transac- tion networks is crucial, and one effective method for this is Densest Subgraph Discovery (DSD). However, deploying DSD methods in production systems faces substantial scalability challenges due to the predominantly sequential nature of existing methods, which impedes their ability to handle large-scale transaction networks and results in significant detection delays. In this paper, we introduce Dupin, a novel parallel processing framework designed for efficient DSD processing in billion-scale graphs. Dupin is powered by a processing engine that exploits the unique properties of the peeling process, with theoretical guarantees on detection quality and efficiency.Dupin provides user-friendly APIs for flexible customization of DSD objectives and ensures robust adaptability to diverse fraud detection scenarios. Empirical evaluations indicate that Dupin outperforms several existing DSD methods, with performance improvements observed in specific scenarios reaching up to two orders of magnitude. On billion-scale graphs, Dupin demonstrates the potential to enhance the prevention of fraudulent transactions by approximately 49.5 basis points and reduce density error from 30% to below 5%, as supported by our experimental results. ACM Reference Format: Anonymous Author(s). 2024. Dupin: A Parallel Densest Subgraph Discovery Framework on Massive Graphs. In Proceedings of ACM Conference (Conference’17). ACM, New York, NY, USA, 14 pages. https://doi.org/10.1145/ nnnnnnn.nnnnnnn',\n",
       " '1 INTRODUCTION': 'Graph structures pervade numerous contemporary applications, ranging from transaction and communication networks to expan- sive social media platforms. The study of densest subgraph discovery (DSD), first explored by [21], has since proven instrumental in various domains: link spam identification [4, 19], community de- tection [9, 17], and notably, fraud detection [8, 24, 42]. Central to this domain is the peeling process [2, 5, 8, 24, 46], which operates by iteratively removing vertices based on density metrics, such as vertex degree or the cumulative weight of adjacent edges. Their widespread adoption can be attributed to their inherent efficiency, robustness, and proven worst-case performance guarantees for DSD. In practice, business requirements demand the detection of fraud- ulent communities on billion-scale graphs within seconds. Recent Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for components of this work owned by others than ACM must be honored. Abstracting with credit is permitted. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. Request permissions from permissions@acm.org. Conference’17, July 2017, Washington, DC, USA © 2024 Association for Computing Machinery. ACM ISBN 978-x-xxxx-xxxx-x/YY/MM. . . $15.00 https://doi.org/10.1145/nnnnnnn.nnnnnnn studies, including those by [1] and [47], have highlighted the perva- sive issue of malicious bot activity in e-commerce, accounting for a substantial 21.4% of all traffic in 2018. These fraudulent activities often happen quickly, making them hard to detect. The inability to efficiently process and analyze such massive graphs not only ham- pers fraud detection efforts but also exposes businesses to substan- tial financial risks and reputational damage. Therefore, addressing these challenges is not merely an academic exercise; it is a press- ing necessity for enhancing the integrity of digital transactions and safeguarding consumer trust. Despite extensive research onDSD for static graphs [20–22, 30, 33, 34], dynamic graphs [2, 18, 26, 28, 43], and parallel approaches [14, 16, 41, 42], existing methods face significant challenges for practical de- ployment in real-world fraud detection systems. For instance, many existing algorithms primarily rely on static score functions for DSD, lacking the flexibility required for dynamic and evolving fraud detec- tion scenarios. Methods such as [4] and [27] are designed with fixed detection criteria, which can become less effective as fraudulent be- haviors adapt over time. Additionally, incremental algorithms remain inefficient at handling billion-scale graphs with high transaction volumes, leading to increased latency. Parallel DSD frameworks often lack efficient pruning techniques, resulting in excessive iterations that degrade performance on large-scale graphs. This paper addresses the following research question: How can we enhance the efficiency and scalability of DSD methods to effectively detect fraud in massive graphs? Challenges. We evaluated existing methods with our industry partner, Anon (anonymized for review). The following issues surfaced: Firstly, existing algorithms focus on static score functions for DSD and lack flexible customization. While this may be adequate for short-term detection, it becomes less effective over time. For example, as shown in Figure 1a, graph density can accumulate sig- nificant errors within a single day, highlighting the need for more adaptive and customizable detection methods. Secondly, existing incremental algorithms are still inefficient at handling billion-scale graphs with high transaction volumes, leading to increased latency. This limitation is evident in Figure 1b, which illustrates the system load distribution for a specific day’s transactions, underscoring the inability of current methods to efficiently manage high transaction volumes without significant delays. Thirdly, existing parallel densest subgraph discovery frameworks lack efficient pruning techniques, re- sulting in a large number of iterations. This inefficiency is especially problematic on billion-scale graphs, where performance degrades substantially due to the excessive computational overhead caused by numerous iterations. Adapting to Evolving Research Needs. Recognizing the dynamic nature of fraud detection and the rapid evolution of fraudulent tactics, we have adapted our research focus to address these challenges. Our approach incorporates flexible customization options for DSD methods, allowing for adjustments based on the changing landscape of fraudulent activities. This adaptability is crucial for maintaining the effectiveness of fraud detection systems in an environment where new threats continuously emerge. Contributions. In this work, we propose a DSD framework for fraud detection. Our key contributions are as follows: • A DSD Parallelization Framework. We propose Dupin, a novel parallel processing framework for densest subgraph discovery (DSD) that breaks the sequential dependencies inherent in tra- ditional density metrics, significantly enhancing scalability and efficiency on both weighted and unweighted graphs. Unlike existing methods that focus on specific density metrics, Dupin allows developers to define custom fraud detection semantics through flexible suspiciousness functions for edges and vertices. It supports a wide range of DSD semantics, including DG [6], DW [23], FD [24], TDS [46], and 𝑘CLiDS [14]. Importantly, Dupin not only leverages parallelism but also provides theoretical guarantees on approximation ratios for the supported density metrics. To the best of our knowledge, this is the first generic parallel processing framework that accommodates a broad class of density metrics in DSD, offering both flexibility and theoretical assurance. • Exploiting Long-Tail Properties.We identify and address the long-tail problem in the peeling process of DSD algorithms, where late iterations remove only a small fraction of vertices and con- tribute little to the quality of the densest subgraph, leading to inefficiency. To tackle this, we introduce two novel optimizations: Global Pruning Optimization, which proactively removes low-contribution vertices within each peeling iteration to reduce computational overhead, and Local Pruning Optimization, which eliminates disparate structures formed after vertex removal to enhance the density and quality of the detected subgraph. These optimizations significantly reduce the number of iterations re- quired, improving both performance and result quality. • Empirical Validation and Real-World Impact: We conduct comprehensive experiments on large-scale industry datasets to validate the effectiveness of Dupin. Our results show that Dupin achieves up to 100 times faster fraud detection compared to the state-of-the-art incremental method Spade [28] and parallel methods like PBBS [41]. Moreover,Dupin is capable of preventing up to 94.5% of potential fraud, demonstrating significant practical bene- fits and reinforcing system integrity in real-world applications. Organization. The remainder of this paper is structured as follows: Section 2 presents the background and the literature review. Section 3 introduces the architecture of Dupin. In Section 4, we propose the theoretical foundations for parallel peeling-based algorithms for DSD. Section 5 introduces long-tail pruning techniques to improve efficiency and effectiveness. The experimental evaluation of Dupin is presented in Section 6. Finally, Section 7 concludes the paper and outlines avenues for future research.',\n",
       " '2 BACKGROUND AND RELATEDWORK': '2.1 Preliminary This subsection presents the preliminary. Some frequently used no- tations are summarized in Table 1. Graph 𝐺 . In this work, we operate on a weighted graph denoted as 𝐺 = (𝑉 , 𝐸), where 𝑉 represents the set of vertices and 𝐸 (⊆ 𝑉 ×𝑉 ) represents the set of edges. Each edge (𝑢𝑖 , 𝑢 𝑗 ) ∈ 𝐸 is assigned a nonnegative weight, expressed as 𝑐𝑖 𝑗 . Additionally, 𝑁 (𝑢) denotes the set of neighboring vertices connected to vertex 𝑢. Induced Subgraph. For a given subset 𝑆 of vertices in 𝑉 , we define the induced subgraph as 𝐺[𝑆] = (𝑆, 𝐸[𝑆]), where 𝐸[𝑆] = {(𝑢, 𝑣) | (𝑢, 𝑣) ∈ 𝐸 ∧ 𝑢, 𝑣 ∈ 𝑆}. The size of the subset 𝑆 is denoted by |𝑆 |. Density Metrics 𝑔 and Weight Function 𝑓 . We utilize the class of metrics𝑔 as defined in prior works [6, 24, 28], where for a given subset of vertices 𝑆 ⊆ 𝑉 , the density metric is computed as: 𝑔(𝑆) = 𝑓 (𝑆) |𝑆 | . Here, 𝑓 (𝑆) represents the total weight of the induced subgraph 𝐺[𝑆], which is defined as the sum of the weights of vertices in 𝑆 and the weights of edges in 𝐸[𝑆]. More formally: 𝑓 (𝑆) = ∑︁ 𝑢𝑖 ∈𝑆 𝑎𝑖 + ∑︁ (𝑢𝑖 ,𝑢 𝑗 )∈𝐸[𝑆] 𝑐𝑖 𝑗 (1) The weight of a vertex 𝑢𝑖 , denoted as 𝑎𝑖 (𝑎𝑖 ≥ 0), quantifies the suspiciousness of user 𝑢𝑖 . The weight of an edge (𝑢𝑖 , 𝑢 𝑗 ), represented by 𝑐𝑖 𝑗 (𝑐𝑖 𝑗 > 0), reflects the suspiciousness of the transaction between users 𝑢𝑖 and 𝑢 𝑗 . Intuitively, the density metric 𝑔(𝑆) encapsulates the overall suspiciousness density of the induced subgraph𝐺[𝑆]. A larger value of 𝑔(𝑆) indicates a higher density of suspiciousness within𝐺[𝑆]. The vertex set that maximizes the density metric 𝑔 is denoted by 𝑆∗. We next introduce five representative density metrics extensively employed in many fraud detection applications. The density metrics are mainly differed by how the weight function is defined. Dense Subgraphs (DG) [6]. The DG metric is employed to quan- tify the connectivity of substructures within a graph. It has found extensive applications in various domains, such as detecting fake comments in online platforms [29] and identifying fraudulent activities in social networks [3]. Given a subset of vertices 𝑆 ⊆ 𝑉 , the weight function of DG is defined as 𝑓 (𝑆) = |𝐸[𝑆]|, where |𝐸[𝑆]| denotes the number of edges in the induced subgraph 𝐺[𝑆]. Dense Subgraph on Weighted Graphs (DW) [23]. In transac- tion graphs, it is common to have weights assigned to the edges, representing quantities such as the transaction amount. The DW metric extends the concept of dense subgraphs to accommodate these weighted relationships. Given a subset of vertices 𝑆 ⊆ 𝑉 , the weight function of DW is defined as 𝑓 (𝑆) = ∑ (𝑢𝑖 ,𝑢 𝑗 )∈𝐸[𝑆] 𝑐𝑖 𝑗 . Fraudar (FD) [24]. In order to mitigate the effects of fraudulent actors deliberately obfuscating their activities, Hooi et al. [24] introduced the FD algorithm. This approach innovatively incorporates weights for edges and assigns prior suspiciousness scores to vertices, potentially utilizing side information to enhance detection accuracy. Given a subset of vertices 𝑆 ⊆ 𝑉 , the weight function of FD is defined as Equation 1 where 𝑎𝑖 denotes the suspicious score of vertex 𝑢𝑖 and 𝑐𝑖 𝑗 = 1 log(𝑥+𝑐) . Here 𝑥 is the degree of the object vertex between 𝑢𝑖 and 𝑢 𝑗 , and 𝑐 is a positive constant [24]. Triangle Densest Subgraph (TDS) [46]. Triangles serve as a crucial structural motif in graphs, providing valuable insights into the connectivity and cohesiveness of subgraphs. The TDS metric, in par- ticular, leverages the prevalence of triangles to quantify the density of a subgraph. Given a subset of vertices 𝑆 ⊆ 𝑉 , the weight function of TDS is defined as 𝑓 (𝑆) = 𝑡 (𝑆) where 𝑡 (𝑆) represents the total number of triangles within the induced subgraph 𝐺[𝑆]. 𝑘-Clique Densest Subgraph (𝑘CLiDS) [14]. Extending the concept of triangle densest subgraphs, 𝑘CLiDS focuses on identifying dense subgraphs based on the presence of 𝑘-Cliques. A 𝑘-Clique is a complete subgraph of 𝑘 vertices, representing a tightly-knit group of vertices. For a given subset of vertices 𝑆 ⊆ 𝑉 , the weight function of 𝑘CLiDS is defined as 𝑓 (𝑆) = 𝑘(𝑆), where 𝑘(𝑆) denotes the number of 𝑘-Cliques within the induced subgraph 𝐺[𝑆]. 2.2 Sequential Peeling Algorithms Peeling algorithms have gained popularity for their efficiency, ro- bustness, and proven worst-case performance in various applica- tions [6, 24, 46]. Next, we provide a concise overview of the typical execution flow of these algorithms. Peeling Weight.We use𝑤𝑢𝑖 (𝑆) to indicate the decrease in the value of the weight function 𝑓 when the vertex𝑢𝑖 is removed from a vertex set 𝑆 , i.e., the peeling weight. Formally, 𝑤𝑢𝑖 (𝑆) = 𝑎𝑖 + ∑︁ (𝑢 𝑗 ∈𝑆) ∧ ((𝑢𝑖 ,𝑢 𝑗 )∈𝐸) 𝑐𝑖 𝑗 + ∑︁ (𝑢 𝑗 ∈𝑆) ∧ ((𝑢 𝑗 ,𝑢𝑖 )∈𝐸) 𝑐 𝑗𝑖 (2) Algorithm 1: Sequential Peeling Algorithm Input: A graph𝐺 = (𝑉 , 𝐸) and a density metric 𝑔(𝑆) Output: A subset of vertices 𝑆 that maximizes the density metric 𝑔(𝑆) 1 𝑆0 ← 𝑉 /* Initialize the subset with all vertices */ 2 for 𝑖 ← 1 to |𝑉 | do 3 𝑢 ← arg max𝑣∈𝑆𝑖−1 𝑔(𝑆𝑖−1 \\\\ {𝑣 }) /* The vertex to be peeled */ 4 𝑆𝑖 ← 𝑆𝑖−1 \\\\ {𝑢 } /* The selected vertex */ 5 return arg max𝑆𝑖 𝑔(𝑆𝑖 ) /* The subset that maximizes 𝑔(𝑆) */ Sequential PeelingAlgorithm (Algorithm1).The peeling process starts with the full vertex set 𝑆0 = 𝑉 (Line 1) and iteratively removes vertices to maximize the density metric 𝑔. In each step 𝑖 , a vertex𝑢𝑖 is removed from 𝑆𝑖−1 to form 𝑆𝑖 , such that 𝑔(𝑆𝑖−1 \\\\ {𝑢𝑖 }) is maximized (Lines 3∼4). This process is repeated until 𝑆𝑖 becomes empty, resulting in a sequence of nested sets 𝑆0, 𝑆1, . . . , 𝑆 |𝑉 | . The algorithm ultimately returns the subset 𝑆𝑖 ∈ {𝑆0, . . . , 𝑆 |𝑉 |} that maximizes 𝑔(𝑆𝑖 ). Example 2.1 (Seqential Peeling Algorithm Process). Consider the graph 𝐺 as shown in Figure 2. The process begins with an initial graph density of 2.33. In the first iteration, vertex 𝑢1 is peeled due to its smallest peeling weight among all vertices. Subsequently, vertices are peeled in the following order: 𝑢2, 𝑢3, 𝑢4, 𝑢5, and 𝑢6 based on the updated criteria after each peeling operation. Notably, after peeling 𝑢2, the graph density increases, which is a characteristic observation in the peeling process. The sequence of peeling continues until the last vertex, 𝑢6, is peeled, resulting in a final graph density of 0. The peeling sequence, therefore, is 𝑂 = [𝑢1, 𝑢2, 𝑢3, 𝑢4, 𝑢5, 𝑢6], completely disassembling the graph. The induced subgraph𝐺[𝑆] of 𝑆 = [𝑢3, 𝑢4, 𝑢5, 𝑢6] has the greatest density of 2.75, thus 𝐺[𝑆] is returned. Approximation. An algorithm 𝑄 is defined as an 𝛼-approximation for the Densest Subgraph Discovery (DSD), where 𝛼 ≥ 1, if for any given graph it returns a subset 𝑆 satisfying the condition: 𝑔(𝑆) ≥ 𝑔(𝑆∗) 𝛼 , where 𝑆∗ is the optimal subset that maximizes the density metric 𝑔. Theorem 2.1 ([24]). For the vertex set 𝑆𝑝 returned by Algorithm 1 and the optimal vertex set 𝑆∗, it holds that 𝑔(𝑆𝑝 ) ≥ 𝑔(𝑆∗) 2 for DG, DW and FD as the density metrics. We extend the analysis to TDS and 𝑘CLiDS. The proof of the following theorem is omitted due to space limitation. Theorem 2.2. For the vertex set 𝑆𝑝 returned by Algorithm 1 and the optimal vertex set 𝑆∗, it holds that 𝑔(𝑆𝑝 ) ≥ 𝑔(𝑆∗) 𝑘 for TDS and 𝑘CLiDS as the density metrics where 𝑘 = 3 for TDS. 2.3 Related work Densest Subgraph Discovery (DSD). Densest subgraph discov- ery represents a core area of research in graph theory, extensively explored in literature [7, 8, 20, 30, 39, 40]. The foundational max-flow- based algorithm by Goldberg et al. [21] set a benchmark for exact densest subgraph detection, with subsequent studies introducing more scalable exact solutions [33, 34]. While speed enhancements have been achieved through various greedy approaches [6, 8, 24, 33], these methods are inherently sequential and difficult to parallelize. For example, they often peel one vertex at a time, which results in too many peeling iterations. This sequential nature limits their scalability and efficiency on massive graphs. In graph-based fraud de- tection, techniques like COPYCATCH [4] and GETTHESCOOP [27] leverage local search heuristics to identify dense subgraphs in bi- partite networks, frequently applied in fraud, spam, and community detection across social and review platforms [24, 37, 42]. Despite their widespread use, these methods suffer from efficiency issues in very large graphs, often involving numerous iterations and lack- ing effective pruning techniques, which exacerbates the long-tail problem. Moreover, they are limited to specific definitions of dense subgraphs. Contrarily, Dupin offers a versatile, parallel approach to densest subgraph detection. It facilitates the simultaneous peeling of multiple vertices, drastically reducing the computational overhead. Additionally, Dupin provides APIs that allow users to customize defi- nitions of dense subgraphs, ensuring that the resultant graph density adheres to theoretical bounds. DSD on Dynamic Graphs. To tackle real-time fraud detection, sev- eral variants have been designed for dynamic graphs [10, 18, 26, 28, 43]. [28] addresses real-time fraud community detection on evolving graphs (edge insertions) using an incremental peeling sequence re- ordering method, while [26] extends this approach to fully dynamic graphs (edge insertions and deletions). [43] proposes an incremental algorithm that maintains and updates a dense subtensor in a ten- sor stream. [12] and [36] utilize sliding windows to detect dense subgraphs and bursting cores within specific time windows. These methods have three limitations. First, response time is an issue. Al- though they respond quickly to benign communities, the results can change drastically for newly formed fraudulent communities, caus- ing significant delays in incremental calculations for fraud detection. Second, efficiency is problematic. While incremental processing is faster than recomputation for a single transaction on billion-scale graphs, frequent incremental evaluations can be slower than a full recomputation. Third, in terms of flexibility, most methods support only a single semantic. In contrast, Dupin introduces a unified parallel framework for peeling-based dense subgraph detection.Dupin can respond within seconds on billion-scale graphs and defines multiple density metrics that can be used on the Dupin platform. Parallel DSD. Several parallel methods for DSD have been pro- posed [2, 14, 16, 39, 41, 42]. Bahmani et al. [2] proposed a parallel algorithm for DG using the MapReduce framework. FWA [15] proposed a parallel algorithm for DW based on Frank-Wolfe algo- rithm [25] using OpenMP [13]. Some works propose parallel 𝑘-core algorithms [32, 41, 44]. Others introduce parallel algorithms for 𝑘- Clique detection, with approaches offering a (1 + 𝜖)-approximation ratio and parallel implementations for the 𝑘CLiDS problem [16, 45]. There are also algorithms for 𝑘-Clique densest subgraph detection using OpenMP [14], and methods for dense subtensor detection [42]. However, these systems have two main limitations. First, they typi- cally support only a single-densitymetric and do not handle weighted graphs. In real-world applications, transactions between users can have varying degrees of importance based on amounts, frequen- cies [28], and other factors. Extending these systems to support other dense metrics with theoretical guarantees is non-trivial. Second, they often lack efficient pruning techniques, requiring numerous itera- tions and suffering from the long-tail issue, which hinders scalability on massive graphs. In contrast, Dupin defines the characteristics of a set of supported dense metrics and accommodates weighted sub- graphs, allowing for more nuanced analysis reflective of real-world data. Our method introduces efficient pruning strategies by peeling multiple vertices in parallel based on a threshold mechanism, signifi- cantly reducing the number of iterations and mitigating the long-tail problem. This makes Dupin more effective and scalable in diverse real-world scenarios compared to previous approaches. 3 THE Dupin FRAMEWORK 3.1 Motivations A notable example of advanced fraud detection is seen in Anon, a prominent technology company in Southeast Asia known for its digital payment and food delivery services. As delineated in [28], the detection process begins with forming a transaction graph,𝐺 , derived from the platform’s transactions. This graph is incrementally updated, represented as 𝐺 = 𝐺 ⊕ ∆𝐺 . Subsequent stages utilize densest subgraph discovery algorithms to uncover fraudulent communities. A typical fraud scenario on the platform involves customer-merchant collusion, where fake accounts are created to exploit promotional incentives. These fraudulent activities manifest as densest subgraphs within the transaction graph. Efficiency. Our empirical findings reveal that the incremental execution of DSD algorithms, such as Spade [28], on a graph with 2 billion edges requires approximately 198 seconds per batch (default setting: one batch of 1K edge insertions). However, in practical business sce- narios, the number of incoming edges can peak at 400K per minute. Clearly, using incremental evaluation on billion-scale graphs does not provide sufficient throughput to meet business requirements. Latency and Prevention Ratio. Existing DSD systems face sig- nificant challenges in terms of latency and prevention ratio. Upon identification of fraudulent activities, actions such as account bans or freezes are implemented to prevent further losses. However, the aver- age latency for these systems to detect fraud and take action is often high, typically around 200 to 300 seconds for incremental algorithms and more than 1000 seconds for existing parallel algorithms, which is insufficient for real-time requirements (operational demand requires detection within seconds). Moreover, when the graph size increases from million-scale to billion-scale, the prevention ratio decreases from 92.47% to 45%. These issues underscore the necessity for more efficient and scalable algorithms that can maintain low latency and high prevention ratios, even under heavy transaction loads. Parallelization offers a viable solution to these challenges by dis- tributing the computational load across multiple processors. Mod- ern CPUs are equipped with numerous cores that are cost-effective, enabling parallel processing. If the peeling process of DSD is paral- lelized, different processors can simultaneously evaluate and peel different nodes, thereby increasing scalability and efficiency. For example, if we can improve processing speed by one to two orders of magnitude, the system would be able to meet industry requirements for real-time fraud detection, completing detection within seconds. Problem Statement. Given a graph 𝐺 = (𝑉 , 𝐸) and a community detection semantic with density metric 𝑔(𝑆), our goal is to find a vertex subset 𝑆∗ ⊆ 𝑉 that maximizes 𝑔(𝑆∗). 3.2 System Architecture To enable users to design complex DSD algorithms to detect fraudsters based on our framework, Dupin, we have introduced the archi- tecture, APIs, and supported density metrics in this section. Architecture of Dupin. Figure 3 presents the architecture of Dupin with user-defined functions for detecting suspicious activities.Dupin enables users to define their fraud detection semantics through a set of provided APIs. Within Dupin’s computational engine, we sup- port both sequential and parallel peeling algorithms. Additionally, algorithms for long-tail pruning techniques are implemented to ac- celerate convergence and enhance the density of subgraphs in each iteration. Dupin identifies fraudulent communities, which are then flagged for further investigation by moderators. APIs of Dupin (Figure 3). Dupin simplifies the process of defining detection semantics by allowing users to specify a few simple functions. Additionally, Dupin offers APIs that enable users to balance detection speed and accuracy effectively. The key APIs encompass several components designed to facilitate the customization and op- timization of fraud detection processes. These components provide a robust framework for users to implement their fraud detection strate- gies, ensuring both high performance and precision in identifying fraudulent activities. Details are listed below: • pWeight and updateNgh. These functions are designed for the dynamic adjustment of weights within the network. They allow the user to define the peeling weight of a vertex and specify the method for updating the peeling weight of neighboring vertices after a vertex has been peeled. • VSusp and ESusp. Given a vertex/edge, these components are re- sponsible for deciding the suspiciousness of the endpoint of the edge or the edge with a user-defined strategy. • isBenign. This component is used to decide whether a vertex is benign during the whole peeling process (Section 5). If the vertex is benign, it is peeled within the current iteration. • setEpsilon. This function is utilized to control the precision of fraud detection. During periods of high system load, moderators can increase 𝜖 to achieve higher throughput. Conversely, when the system load is low, 𝜖 can be decreased to enhance accuracy. • setK. This function specifies the 𝑘-Clique parameter, enabling the definition of the 𝑘-Clique for analysis. 1 double vsusp(const Vertex& v, const Graph& g) { 2 return g.weight[v]; // Side information on vertex 3 } 4 double esusp(const Edge& e, const Graph& g) { 5 return 1.0 / log(g.deg[e.src] + 5.0); 6 } 7 int main() { 8 Dupin dupin; 9 dupin.VSusp(vsusp); // Plug in vsusp 10 dupin.ESusp(esusp); // Plug in esusp 11 dupin.setEpsilon (0.1); 12 dupin.LoadGraph(\"graph_sample_path\"); 13 vector <Vertex > fraudsters = dupin.ParDetect (); 14 return 0; 15 } Listing 1: Implementation of FD on Dupin Example (Listing 1). To implement FD [24] on Dupin, users simply need to plug in the suspiciousness function vsusp for the vertices by calling VSusp and the suspiciousness function esusp for the edges by calling ESusp. Specifically, 1) vsusp is a constant function, i.e., given a vertex 𝑢, vsusp(𝑢) = 𝑎𝑖 and 2) esusp is a logarithmic function such that given an edge (𝑢𝑖 , 𝑢 𝑗 ), esusp(𝑢𝑖 , 𝑢 𝑗 ) = 1 log(𝑥+𝑐) , where 𝑥 is the degree of the object vertex between 𝑢𝑖 and 𝑢 𝑗 , and 𝑐 is a positive constant [24]. Developers can easily implement customized peeling algorithms with Dupin, significantly reducing the engineering effort. Density Metrics Support in Dupin. To illustrate the versatility of Dupin in handling various density metrics, we establish comprehensive sufficient conditions. These conditions ensure Dupin’s adaptability in integrating a wide range of metrics to assess graph density effectively. This foundational approach enablesDupin to sup- port diverse density calculations, crucial for nuanced fraud detection and network analysis tasks. Property 3.1. Dupin supports a density metric 𝑔(𝑆) if the following conditions are met: (1) 𝑔(𝑆) = 𝑓 (𝑆) |𝑆 | , where 𝑓 is a monotone increasing function. (2) 𝑎𝑖 ≥ 0, ensuring non-negative vertex weight function. (3) 𝑐𝑖 𝑗 ≥ 0, ensuring non-negative edge weight function.',\n",
       " '4 PARALLELABLE PEELING ALGORITHMS': 'For sequential peeling algorithms, only one vertex can be removed at a time, with each vertex’s removal depending on the removal of previous vertices. This dependency prevents the algorithm from executing multiple vertex removals simultaneously. In this section, we introduce a parallel peeling paradigm to break this dependency chain. This new approach allowsmultiple vertices to be peeled in parallel without affecting the programming abstractions that Dupin provides to end users. We then demonstrate how this paradigm offers fine-grained trade-offs between parallelism and ap- proximation guarantees for all density metrics studied in this work. 4.1 Parallel Peeling Paradigm Instead of peeling the vertex with the maximum peeling weight, we can peel all vertices that lead to a significant decrease in the density scores of the remaining graph. This decrease is controlled by a tunable parameter 𝜖 . A larger 𝜖 allows more vertices to be peeled in parallel, while a smaller 𝜖 provides a better approximation ratio. Algorithm 2: Dupin: Parallel Peeling Input:𝐺 = (𝑉 , 𝐸), a density metric 𝑔(𝑆), and 𝜖 ≥ 0 Output: A subset of vertices 𝑆 that maximizes the density metric 𝑔(𝑆) 1 𝑆0 ← 𝑉 /* Initialize the subset with all vertices */ 2 𝑖 ← 1 /* Initialize the index */ 3 while 𝑆𝑖−1 ̸= ∅ do 4 𝜏 = 𝑘(1 + 𝜖) · 𝑔(𝑆𝑖−1) /* The threshold for peeling */ 5 𝑈 ← {𝑢 | 𝑤𝑢 (𝑆𝑖−1) ≤ 𝜏 } /* Vertices to be peeled */ 6 𝑆𝑖 ← 𝑆𝑖−1 \\\\𝑈 /* Update the subset */ 7 𝑖 ← 𝑖 + 1 /* Increment the index */ 8 return arg max𝑆𝑖 𝑔(𝑆𝑖 ) /* The subset that maximizes 𝑔(𝑆) */ Parallel Peeling (Algorithm 2). Let 𝑆𝑖 denote the vertex set after the 𝑖-th peeling iteration. Initially, 𝑆0 = 𝑉 (Line 1). In each iteration, it removes a subset of vertices𝑈 from 𝑆𝑖−1, where ∀𝑢 ∈ 𝑈 ,𝑤𝑢 (𝑆𝑖−1) ≤ 𝑘(1 + 𝜖)𝑔(𝑆𝑖−1). Parameter 𝑘 is determined by the metric used. 𝑘 = 2 works for DG, DW and FD. 𝑘 = 3 works for TDS and 𝑘 is the size of the cliques for 𝑘CLiDS. This process is repeated recursively until no vertices are left, resulting in a series of vertex sets 𝑆0, . . . , 𝑆𝑅 , where 𝑅 is the number of iterations. The algorithm then returns the set 𝑆𝑖 (𝑖 ∈ [0, 𝑅]) that maximizes the density metric 𝑔(𝑆𝑖 ), denoted as 𝑆𝑝 . In what follows, we also show a case where 𝑘CLiDS is the density metric for DSD. 3 3 6 6 5 4 21 0 00 3 3 6 6 3 3 0 0 Density: 0.91 Density: 1.33 Density: 0 Iteration i Iteration i+1 Iteration i+2 Iteration i+3 Density: 0 τ = 3× 0.91 = 2.73 τ = 3× 1.33 = 3.99 τ = 0 τ = 0 4.2 Theoretical Analysis In this section, we analyze the theoretical properties of the parallel peeling process. We begin by examining the number of iterations required for parallel peeling. Built upon the result, we then determine the time complexity and the approximation ratio of the parallel peeling process. Lemma 4.1. Given any 𝜖 > 0, 𝑅 < log 1+𝜖 |𝑉 |. Proof. We prove that for any 𝜖 ≥ 0, the size of the remaining vertex set is bounded by |𝑆𝑖 |< 1 1+𝜖 |𝑆𝑖−1 | for all 𝑖 ∈ [1, 𝑅]. We first examine the peeling weights of the vertices in 𝑆𝑖−1 for the density metrics DG. DW and FD where 𝑘 = 2. As each edge weight in the peeling weight calculation contributes twice (once for each endpoint), and the vertex weights contribute once, we have∑︁ 𝑢∈𝑆𝑖−1 𝑤𝑢 (𝑆𝑖−1) ≤ 2𝑓 (𝑆𝑖−1), (3) The peeling weights of the vertices in 𝑆𝑖−1 are calculated as follows:∑︁ 𝑢∈𝑆𝑖−1 𝑤𝑢 (𝑆𝑖−1) = ∑︁ 𝑢∈𝑆𝑖−1\\\\𝑆𝑖 𝑤𝑢 (𝑆𝑖−1) + ∑︁ 𝑢∈𝑆𝑖 𝑤𝑢 (𝑆𝑖−1) ≥ ∑︁ 𝑢∈𝑆𝑖 𝑤𝑢 (𝑆𝑖−1) > 2(1 + 𝜖)|𝑆𝑖 |𝑔(𝑆𝑖−1) (4) Combining Equation 3 and Equation 4, we have: 2(1 + 𝜖)|𝑆𝑖 |𝑔(𝑆𝑖−1) < 2𝑓 (𝑆𝑖−1) = 2|𝑆𝑖−1 |𝑔(𝑆𝑖−1) (5) For all 𝑖 ∈ [1, 𝑅], |𝑆𝑖 |< 1 1 + 𝜖 |𝑆𝑖−1 | (6) Then we have: |𝑉 |= |𝑆0 |> 1 1 + 𝜖 |𝑆1 |> . . . > ( 1 1 + 𝜖 ) 𝑅 |𝑆𝑅 | (7) Therefore, we have 𝑅 < log 1+𝜖 |𝑉 |. For the density metrics TDS and 𝑘CLiDS, we use the fact that each edge contributes to the peeling weight calculation exactly 𝑘 times due to the counting of each edge within every 𝑘-Clique in the graph (𝑘 = 3 for TDS). This multiplicity of edge contributions leads to the relationship: ∑︁ 𝑢∈𝑆𝑖−1 𝑤𝑢 (𝑆𝑖−1) ≤ 𝑘 𝑓 (𝑆𝑖−1), (8) We then adapt Equations 4-7 and the proof follows. □ Time Complexity. Algorithm 2 operates for at most 𝑅 = log 1+𝜖 |𝑉 | iterations, with each iteration scanning all vertices, taking 𝑂(|𝑉 |) time. As vertices are peeled, updates are necessitated for the peeling weights of their neighboring vertices. Over the course of 𝑅 iterations, there are no more than |𝐸 | such updates for DG, DW and FD. Consequently, Algorithm 3 incurs 𝑂((|𝐸 |+|𝑉 |) log 1+𝜖 |𝑉 |) node/edge weight updates. For DG, DW and FD, the update costs are 𝑂(1). For TDS and 𝑘CLiDS, we follow previous studies [11, 14] to compute the peeling weights, and the complexity is 𝑂(𝑘 |𝐸 |𝛼(𝐺) 𝑘−2 ), where 𝛼(𝐺) is the arboricity of the graph. Hence, the overall complexity is 𝑂(𝑘 |𝐸 |𝛼(𝐺) 𝑘−2 (|𝐸 |+|𝑉 |) log 1+𝜖 |𝑉 |). The cost of peeling weight up- dates is orthogonal to the Dupin framework. Approximation Ratio. Next, we are ready to show that Dupin provides a theoretical guarantee with a 𝑘(1 + 𝜖)-approximation for all density metrics studied in this work. We denote 𝑢𝑖 ∈ 𝑆𝑝 as the first vertex in 𝑆∗ peeled by Dupin, and 𝑆 ′ as the peeling subsequence starting from 𝑢𝑖 . Theorem 4.2. For the vertex set 𝑆𝑝 returned by Dupin and the optimal vertex set 𝑆∗, the relationship 𝑔(𝑆𝑝 ) ≥ 𝑔(𝑆∗) 𝑘(1+𝜖) holds. Proof. We first prove that ∀𝑢𝑖 ∈ 𝑆∗, 𝑤𝑢𝑖 (𝑆 ∗ ) ≥ 𝑔(𝑆∗). We prove it in contradiction. We assume that 𝑤𝑢𝑖 (𝑆 ∗ ) < 𝑔(𝑆∗). By peeling 𝑢𝑖 from 𝑆∗, we have the following. 𝑔(𝑆∗ \\\\ {𝑢𝑖 }) = 𝑓 (𝑆∗) −𝑤𝑢𝑖 (𝑆 ∗ ) |𝑆∗ |−1 > 𝑓 (𝑆∗) − 𝑔(𝑆𝑖 ) |𝑆∗ |−1 = 𝑓 (𝑆∗) − 𝑔(𝑆∗) |𝑆∗ |−1 = 𝑓 (𝑆∗) − 𝑓 (𝑆∗) |𝑆∗ | |𝑆∗ |−1 = 𝑔(𝑆∗) (9) Therefore, a better solution can be obtained by removing 𝑢𝑖 from 𝑆∗, which contradicts the fact that 𝑆∗ is the optimal solution. We can conclude that𝑤𝑢𝑖 (𝑆 ∗ ) ≥ 𝑔(𝑆∗). Next, we assume that 𝑢𝑖 ∈ 𝑆𝑝 is the first vertex in 𝑆∗ peeled by the peeling algorithm. Since 𝑆∗ is the optimal vertex set, we have 𝑔(𝑆∗) ≤ 𝑤𝑢𝑖 (𝑆 ∗ ), (10) because the optimal value 𝑔(𝑆∗) must be less than or equal to the weight contribution of any single vertex in 𝑆∗, in particular 𝑢𝑖 . Since 𝑆∗ ⊆ 𝑆 ′, it is clear that 𝑤𝑢𝑖 (𝑆 ∗ ) ≤ 𝑤𝑢𝑖 (𝑆 ′ ) (11) By the peeling condition, we have 𝑤𝑢𝑖 (𝑆 ′ ) ≤ 𝑘(1 + 𝜖)𝑔(𝑆 ′) (12) Therefore, we have 𝑔(𝑆∗) ≤ 𝑤𝑢𝑖 (𝑆 ∗ ) ≤ 𝑤𝑢𝑖 (𝑆 ′ ) ≤ 𝑘(1 + 𝜖)𝑔(𝑆 ′) ≤ 𝑘(1 + 𝜖)𝑔(𝑆𝑝 ) (13) Hence, we can conclude that 𝑔(𝑆𝑝 ) ≥ 𝑔(𝑆∗) 𝑘(1+𝜖) . □ Summary. Dupin can easily balance efficiency and the worst-case approximation ratio with a single parameter, 𝜖 . A larger 𝜖 allows more vertices to be peeled in parallel, reducing the number of peeling iterations. Meanwhile, the worst-case approximation ratio of parallel peeling is only affected by a factor of (1 + 𝜖) compared to the ratio for sequential peeling w.r.t. all density metrics studied in this work.',\n",
       " '5 LONG-TAIL PRUNING': 'We observe that the parallel peeling algorithm encounters two sig- nificant issues. First, there is a long-tail problem where subsequent iterations fail to produce denser subgraphs. Instead, each iteration only peels off a small fraction of vertices, which significantly impacts the overall runtime efficiency. Second, during batch peeling, each peeling iteration often results in disparate structures. These are pri- marily caused by the peeling of certain vertices, which leaves their neighboring structure disconnected and fragmented. This phenome- non affects the continuity and coherence of the subgraphs. As shown in Table 2, on the la dataset (the largest social network dataset tested in our experiments), DG, DW, and FD experience 24.67%, 46.84%, and 3.01% of rounds as ineffective long-tail iterations, severely limiting response time. Additionally, during the peeling process, if the batch size is large (e.g., 𝜖 = 0.5), approximately 25.34%, 29.46%, and 7.16% of nodes become disconnected and sparse, signifi- cantly impacting the quality of the detected subgraphs. To address the challenges of high iteration counts in peeling algorithms, Dupin introduces two long-tail pruning techniques: Global Peeling Optimization (GPO) to maintain a global peeling threshold, and Local Peeling Optimization (LPO) to improve the density of the current peeling iteration’s subgraph, thereby increasing the peeling threshold. We tested three algorithms, DG, DW, and FD, and found that they required 17,637, 150,223, and 112,074 iterations respectively, which is significantly high. By strategically pruning long-tail ver- tices and sparse vertices, the number of iterations can be reduced by up to 46.84%. Additionally, LPO can further reduce the number of iterations by up to 92.79%. These optimizations significantly enhance the efficiency of the peeling process, as discussed in this section. 5.1 Global Peeling Optimization Initially, we define what constitutes a long-tail vertex during the peeling process. Peeling these long-tail vertices results in ineffective peeling iterations. To address this, we maintain a global maximum peeling threshold to determine which vertices can be peeled directly. When the global peeling threshold is greater than the current itera- tion’s threshold, we use the global peeling threshold for peeling. Algorithm 3: DupinGPO: Global Peeling Optimization Input:𝐺 = (𝑉 , 𝐸), a density metric 𝑔(𝑆), and 𝜖 ≥ 0 Output: A subset of vertices 𝑆𝑝 that maximizes the density metric 𝑔(𝑆) 1 𝑆0 ← 𝑉 /* Initialize the subset with all vertices */ 2 𝑖 ← 1 3 𝜏max ← 𝑔(𝑆 0 ) 𝑘(1+𝜖) /* Initialize the threshold */ 4 while 𝑆𝑖−1 ̸= ∅ do 5 𝜏 ← max{𝜏max, 𝑘(1 + 𝜖) · 𝑔(𝑆𝑖−1)} /* Peeling threshold */ 6 𝑈 ← {𝑢 | 𝑤𝑢 (𝑆𝑖−1) ≤ 𝜏 } /* Vertices to be peeled */ 7 𝑆𝑖 ← 𝑆𝑖−1 \\\\𝑈 /* Update the subset */ 8 𝜏max = max{𝜏max, 𝑔(𝑆𝑖 ) 𝑘(1+𝜖) } /* Refine the threshold */ 9 𝑖 ← 𝑖 + 1 10 return arg max𝑆𝑖 𝑔(𝑆𝑖 ) /* The subset that maximizes 𝑔(𝑆) */ Definition 5.1 (Long-Tail vertex). Given an 𝛼-𝑎𝑝𝑝𝑟𝑜𝑥𝑖𝑚𝑎𝑡𝑖𝑜𝑛 DSD algorithm and a set of vertices 𝑆 ⊆ 𝑉 , if𝑤𝑢 (𝑆) < 𝑔(𝑆∗) 𝛼 and 𝑆∗ is the optimal vertex set, then 𝑢 is a long-tail vertex. Lemma 5.1. If ∃𝑢 ∈ 𝑆𝑖 is a long-tail vertex, then 𝑆𝑖 ̸= 𝑆𝑃 . Proof. We prove it in contradiction by assuming that 𝑆𝑖 = 𝑆𝑃 . Since 𝑢 is a long-tail vertex in 𝑆𝑖 ,𝑤𝑢 (𝑆𝑖 ) < 𝑔(𝑆∗) 𝛼 < 𝑔(𝑆𝑃 ). By peeling 𝑢 from 𝑆𝑃 , we have the following. 𝑔(𝑆𝑃 \\\\ {𝑢}) = 𝑓 (𝑆𝑃 ) −𝑤𝑢 (𝑆𝑃 ) |𝑆𝑃 |−1 > 𝑓 (𝑆𝑃 ) − 𝑔(𝑆𝑃 ) |𝑆𝑃 |−1 = 𝑓 (𝑆𝑃 ) − 𝑓 (𝑆𝑃 ) |𝑆𝑃 | |𝑆𝑃 |−1 = 𝑔(𝑆𝑃 ) (14) A denser subgraph can be obtained by peeling 𝑢 from 𝑆𝑃 . This contradicts that 𝑆𝑃 is the optimal solution. Hence, 𝑆𝑖 ̸= 𝑆𝑃 . □ Identifying Long-Tail Vertices. Based on Lemma 5.2, we can also establish that a vertex𝑢 in set 𝑆 can be classified as a long-tail vertex if 𝑤𝑢 (𝑆) < 𝑔(𝑆∗) 𝑘(1+𝜖) . This insight allows us to implement a global peeling threshold, denoted as 𝜏max. During each iteration, we compare 𝑔(𝑆𝑖 ) 𝑘(1+𝜖) with 𝜏max. If the former exceeds the latter, Dupin updates 𝜏max to 𝑔(𝑆𝑖 ) 𝑘(1+𝜖) , thereby enhancing the efficiency of the peeling. Peeling with Global Threshold (Algorithm 3). Using FD as an example, we can set 𝑘 to 2. In Lemma 5.2, we recognize that long-tail vertices in set 𝑆𝑖 can be peeled directly during iteration 𝑖 for FD. To facilitate this, Dupin initiates with a global peeling threshold 𝜏max (Line 3). After identifying a denser subgraph post-peeling, 𝜏max is updated to 𝑔(𝑆𝑖 ) 2(1+𝜖) , refining the peeling process (Line 8). Example 5.1. In the initial depiction provided in the leftmost panel of Figure 6 (the 𝑖-th iteration), the density of the graph is 4.28. Traditionally, this scenario would necessitate two additional iterations to complete the peeling process, which could notably increase computational time. As illustrated, vertex 𝑢 cannot be peeled in the (𝑖 + 1)-th iteration and would typically require an extra iteration, the (𝑖 + 2)-th, for its removal. However, Dupin can promptly peel vertices that fall below this threshold by employing our algorithm’s global peeling threshold, which is calculated as half of the current density (𝜏max = 4.28/2 = 2.14). This approach enables the simultaneous peeling of three nodes in the (𝑖 + 1)-th iteration. Without this pruning strategy, an additional iteration would be essential to peel vertex 𝑢, underscoring our method’s enhanced efficiency in the peeling process. This optimization obviates the necessity for a prolonged iterative sequence. The example herein substantiates the efficacy of our tai- lored optimization for long-tail distribution, substantially enhancing the algorithm’s time efficiency. 5.2 Local Peeling Optimization Algorithm 4: DupinLPO: Local Peeling Optimization Input: A graph𝐺 = (𝑉 , 𝐸), a density metric 𝑔(𝑆), and 𝜖 ≥ 0 Output: A subset of vertices 𝑆𝑝 representing the densest subgraph 1 𝑆0 ← 𝑉 /* Initialize the subset with all vertices */ 2 𝜏max ← 𝑔(𝑆 0 ) 𝑘(1+𝜖) /* Initialize the threshold */ 3 while 𝑆𝑖−1 ̸= ∅ do 4 𝜏1 ← max{𝜏max, 𝑘(1 + 𝜖) · 𝑔(𝑆𝑖−1)} /* Peeling threshold */ 5 𝑈1 ← {𝑢 ∈ 𝑆 | 𝑤𝑢 (𝑆) ≤ 𝜏1 } 6 𝑆𝑖 ← 𝑆𝑖−1 \\\\𝑈1 7 while ∃𝑢 ∈ 𝑆𝑖 , 𝑤𝑢 (𝑆𝑖 ) < 𝑔(𝑆𝑖 ) do 8 𝜏2 ← max{𝜏max, 𝑔(𝑆𝑖 )} /* Update the threshold */ 9 𝑈2 ← {𝑢 | 𝑤𝑢 (𝑆𝑖 ) < 𝜏2 } 10 𝑆𝑖 ← 𝑆𝑖 \\\\𝑈2 11 𝜏max = max{𝜏max, 𝑔(𝑆𝑖 ) 𝑘(1+𝜖) } /* Update the threshold */ 12 return arg max𝑆𝑖 𝑔(𝑆𝑖 ) /* The subset that maximizes 𝑔(𝑆) */ It is important to note that each peeling iteration often results in disparate structures. This is primarily caused by the peeling of cer- tain vertices, which leaves their neighboring structures disconnected and fragmented. This not only affects the density of the detected sub- graph but also lowers the peeling threshold. Lemma 5.2 establishes a foundational property of vertices within a dense graph. Lemma 5.2. ∀𝑢 ∈ 𝑆 , if𝑤𝑢 (𝑆) < 𝑔(𝑆), 𝑔(𝑆 \\\\ {𝑢}) > 𝑔(𝑆). Proof. By peeling 𝑢 from 𝑆 , we have the following. 𝑔(𝑆 \\\\ {𝑢}) = 𝑓 (𝑆) −𝑤𝑢 (𝑆) |𝑆 |−1 > 𝑓 (𝑆) − 𝑔(𝑆𝑖 ) |𝑆 |−1 = 𝑓 (𝑆) − 𝑔(𝑆) |𝑆∗ |−1 = 𝑓 (𝑆) − 𝑓 (𝑆) |𝑆 | |𝑆 |−1 = 𝑔(𝑆) (15) Therefore, a denser graph can be obtained by peeling 𝑢 from 𝑆 . □ According to Lemma 5.2, we introduce local peeling optimization within the iteration. After each peeling iteration, if the peeling weight of a vertex is smaller than the current density, Dupin will trim it. We implement a local peeling optimization (Lines 7-10, Algorithm 4) before the next iteration. After a vertex 𝑢 is peeled, Dupin invokes updateNgh to update the peeling weights of 𝑢’s neighboring nodes. If any of these neighbors have a peeling weight less than the current density, they are also peeled, thereby increasing the density of the current iteration. Consequently, when an iteration yields globally optimal results, a denser structure is obtained. The benefits of this approach extend beyond density improvements; performance is also enhanced. By trimming these vertices, we reduce the number of vertices to traverse in subsequent iterations, and a higher density enables more efficient iterations due to the possibility of using larger peeling thresholds. Lemma 5.3. If 𝑢𝑖 ∈ 𝑆𝑝 is the first vertex in 𝑆∗ removed by Dupin, 𝑢𝑖 will not be removed during the local peeling optimization process. Proof. Assume, for the sake of contradiction, that 𝑢𝑖 is trimmed during the local peeling optimization process. Let 𝑆 ′ be the set of vertices left before 𝑢𝑖 is trimmed. By definition, we have: 𝑤𝑢𝑖 (𝑆 ′ ) < 𝑔(𝑆∗). Since 𝑢𝑖 is the first vertex in 𝑆∗ to be removed, 𝑆∗ ⊆ 𝑆 ′, therefore: 𝑤𝑢𝑖 (𝑆 ∗ ) ≤ 𝑤𝑢𝑖 (𝑆 ′ ) < 𝑔(𝑆∗). This contradicts Lemma 5.2, which states that ∀𝑢𝑖 ∈ 𝑆∗: 𝑤𝑢𝑖 (𝑆 ∗ ) ≥ 𝑔(𝑆∗). Otherwise, a better solution can be obtained by removing 𝑢 from 𝑆∗, which contradicts the fact that 𝑆∗ is the optimal solution. Thus, 𝑢𝑖 cannot be trimmed. Hence,𝑢𝑖 will not be removed during the local peeling optimization process. □ Due to Lemma 5.3, 𝑢𝑖 will only be removed during the peeling process. Hence, the approximation ratios (Theorem 4.2) for different density metrics are preserved. 6 EXPERIMENTAL STUDY Our experiments are run on a machine that has an X5650 CPU, 512 GB RAM. The implementation is made memory-resident and implemented in C++. All codes are compiled by GCC-9.3.0 with -𝑂3. Datasets. We report the datasets used in our experiment in Table 3. One industrial dataset is from Anon (gfg). Datasets links-anon(la) and Twitter-rv (rv)were obtained from Stanford NetworkDataset Collection [31]. Datasets kron-logn21(kron), soc-twitter(soc), Web-uk-2005(uk) and Web-sk-2005(sk) were obtained from Network Data Repository [38]. Dataset bio was from the BIOMINE [35]. Baseline Methods. In this section, we introduce five commonly used density metrics pivotal in analyzing network structures:DG [6], DW [23], FD [24], TDS [46], and𝑘CLiDS [14].We implemented these metrics in our framework, Dupin, and compared their performance with frameworks including Spade [28], kCLIST [14], GBBS [16], PBBS [41], PKMC [32], FWA [15] and ALENEX [44]. kCLIST [14] and PBBS [41] are parallel 𝑘CLiDS algorithms, while GBBS [16] supports the DG density metric. Although GBBS does not support weighted graphs, we precompute the peeling weights offline and then import them into GBBS. The time spent on this offline computation is not included in our reported runtime. FWA, ALENEX and PKMC support the DG density metric. For clarity, we denote the incremental peeling process, which involves the iterative removal of graph elements based on density scores, as Spade-X, where X corresponds to each metric, under the same experimental condi- tions described in [28], where the batch size is set to 1K, consistent with the default setting in [28]. We report the average runtime per batch for Spade. DupinGPO-X and DupinLPO-X represent our par- allel peeling algorithms that incorporate global and local peeling optimization. Default Settings. In our experiments, the parameter 𝜖 is set to 0.1 by default, and the number of threads 𝑡 is configured to 128. These settings are used consistently unless otherwise specified. 6.1 Efficiency of Dupin Spade vs Dupin. Our initial comparison focuses on the performance disparity between the incremental and parallel peeling algorithms. Specifically, Dupin-DG (resp. Dupin-DW, Dupin-FD, Dupin-TDS, andDupin-𝑘CLiDS) demonstrates a speedup factor of 6.97 (resp. 7.71, 7.71, 7.65, and 8.46) over Spade-DG (resp. Spade-DW, Spade-FD, Spade-TDS, and Spade-𝑘CLiDS). Notably, Spade-TDS and Spade𝑘CLiDS only complete computations within 7,200 seconds on gfg. The bottleneck for Spade arises in larger graphs where initial calculations of triangle and 𝑘-Clique counting are unable to finish. Dupin outpaces Spade primarily because Spade suffers from frequent re- ordering of peeling sequences when there are many increments, which proves to be slower than recalculating. GBBS vs Dupin. Previous works have not provided simultaneous support for all density metrics. To bridge this gap, we implemented DG, DW, and FD within the parallel GBBS framework. On average, Dupin-DG, Dupin-DW, and Dupin-FD demonstrated speedup factors of 6.33, 12.51, and 17.07 times, respectively, over GBBS-DG, GBBS-DW, andGBBS-FD. This performance advantage is attributed to the fundamental difference in the peeling process between the two frameworks. In GBBS, the basic unit of each peel is a bucket, where all nodes within the same bucket have an identical peeling weight. This design limits the parallelism in weighted graphs like those using DW and FD, where many buckets may contain only a single node. In contrast, Dupin peels batches of nodes at each step, achieving higher parallelism and thus consistently outperforming GBBS, particularly in weighted graph scenarios where the speedup is even more pronounced. kCLIST vs PBBS vs Dupin. Both kCLIST and PBBS support the density metrics TDS and 𝑘CLiDS. In our comparisons, Dupin demonstrates notable performance advantages. Specifically, Dupin-TDS is faster than kCLIST-TDS and PBBS-TDS by 39.85 and 62.71 times, respectively. Furthermore, Dupin-𝑘CLiDS also outperforms kCLIST𝑘CLiDS by 4.16 times. A significant finding is that PBBS-𝑘CLiDS fails to complete computations within 7200 seconds on most datasets, highlighting the efficiency of Dupin in handling more complex met- rics under stringent time constraints. Impact of DupinGPO. Next, we evaluate the acceleration effect of DupinGPO, using Dupin as a baseline for comparison. Notably, DupinGPO-DG is found to be 1.14 times faster, DupinGPO-DW is 1.11 times faster,DupinGPO-TDS is 1.10 times faster, andDupinGPO𝑘CLiDS is 1.35 times faster than their respective Dupin implementations. The acceleration is particularly significant for 𝑘CLiDS on larger datasets, such as soc, rv, kron, and sk. DupinGPO-𝑘CLiDS is 1.40, 1.54, 1.51, and 1.61 times faster than Dupin-𝑘CLiDS on soc, rv, kron, and sk, respectively. This is attributed to the greater number of iterations required due to the larger dataset sizes and the higher count of 𝑘-Cliques, necessitating more peeling iterations. In such scenarios,DupinGPO effectively prunes the tail iterations early, thereby reducing the number of rounds needed. Impact of DupinLPO. We next compared the efficiency of Dupin andDupinLPO. On average, thoughDupinLPO-DGwas 10.09% slower than Dupin-DG, the speed is still comparable. For example, on the soc dataset, DupinLPO-DG was 5.94% faster than Dupin-DG. This is because DupinLPO requires some lock operations to update the peeling weights of neighboring vertices during local peeling optimization. However, the advantage is that DupinLPO can detect up to 26.26% denser subgraphs, as detailed in Section 6.2. On larger datasets, such as soc, rv, and la, the vertices being trimmed have relatively low correlations with each other, so the impact of locks is minimal. For example, on the soc (resp. rv), DupinLPO-DG was 5.94% (resp. 9.94%) faster than Dupin-DG. In contrast, on smaller datasets, such as bio and kron, the extra cost of locks is higher. Impact of the Number of Threads 𝑡 . All methods are influenced by the concurrency level, i.e., the number of threads. Generally, the greater the number of threads, the faster the execution speed. In our experiments, using the la dataset as a benchmark, we varied the number of threads 𝑡 from 2 to 128 to observe the impact on runtime performance. For GBBS-DG, runtime stabilizes and accelerates by 3.33 times as thread count increases from 2 to 8. For GBBS-DW, performance deteriorates when the thread count exceeds 32, but from 2 to 32 threads, it accelerates by 3.50 times. Similarly, GBBS-FD shows no significant change in runtime beyond 32 threads, with an acceleration of 3.09 times from 2 to 32 threads. GBBS-TDS and GBBS-𝑘CLiDS fail to complete computations on the la dataset. In contrast, all five methods tested onDupin exhibit good scalability. As the number of threads increases from 2 to 128,Dupin-DG accelerates by 9.91 times, Dupin-DW by 12.69 times, Dupin-FD by 13.03 times, and Dupin-TDS by 28.06 times. Although Dupin-𝑘CLiDS fails to produce results when the thread count is below 16, it accelerates by 1.32 times from 32 to 128 threads. These results demonstrate Dupin’s strong scalability across varied threading levels. 6.2 Effectiveness of Dupin In addition to performance speedup, we evaluate the density of the subgraphs detected by different frameworks. Detailed density comparisons are presented in Table 5. GBBS vs Spade vs Dupin. Firstly, we compare the densities of the subgraphs identified by GBBS-DG (respectively GBBS-DW, and GBBS-FD) to those detected by Dupin in parallel execution. Our experimental results indicate that Dupin maintains comparable subgraph densities with GBBS, signifying that the increase in com- putational efficiency does not compromise the accuracy of the de- tected subgraphs. Specifically, the density of the subgraph detected byGBBS-DG (respectivelyGBBS-DW, andGBBS-FD) is 7.08% (resp. 6.48% and 7.43%) greater than that detected by Dupin-DG (respectively Dupin-DW, and Dupin-FD) on average. Spade shares similar densities with GBBS. Due to Spade’s tendency to accumulate er- rors over time, its density can become inflated. We will explain its limitations in practical applications in more detail in the case study. kCLIST vs PBBS vs Dupin. a) For 𝑘CLiDS, PBBS-𝑘CLiDS could not complete on most of our test datasets, so we did not compare their densities. kCLIST-𝑘CLiDS could not complete on the sk dataset. On the other datasets, Dupin-𝑘CLiDS’s density was only 6.97% smaller than kCLIST-𝑘CLiDS. On some datasets, such as bio and kron,Dupin𝑘CLiDS even detected subgraphs with greater density. b) For TDS, PBBS-TDS could detect subgraphs on smaller graphs but failed on billion-scale graphs such as rv, sk, and la. Dupin-TDS’s density was 2.03% greater than kCLIST-TDS. Therefore, we conclude that Dupin is comparable to existing parallel methods in terms of the density of the detected dense subgraphs. Ablation Study of Dupin. This experiment delves into the effectiveness of our optimizations, DupinGPO and DupinLPO. Our analysis primarily focuses on comparing the densities of dense subgraphs detected by both DupinGPO and DupinLPO in various settings. DupinGPO achieves the same density as Dupin across five den- sity metrics because it trims the long tail, enhancing efficiency. For DupinLPO-DG (resp. DupinLPO-DW, DupinLPO-FD, DupinLPOTDS, and DupinGPO-𝑘CLiDS), we observe that the detected sub- graphs exhibit densities 11.09% (resp. 7.32%, 8.00%, 1.01%, and 26.26%) greater than those detected by Dupin-DG, Dupin-DW, Dupin-FD, Dupin-TDS, andDupin-𝑘CLiDS, respectively, demonstrating notable outcomes. This is because DupinLPO trims the graph in each iteration, resulting in denser subgraphs compared to Dupin. Impact of the Approximation Ratio 𝜖. The approximation ratio 𝜖 serves to control accuracy. In our experiment, we vary 𝜖 from 0.1 to 1 to investigate its influence on the accuracy of Dupin. We observe that as 𝜖 increases from 0.1 to 1, the density of subgraphs detected by Dupin, DupinGPO, and DupinLPO generally decreases. ForDG, the densities ofDupin-DG,DupinGPO-DG, andDupinLPODG decrease by 22.71%, 22.71%, and 11.99%, respectively. The trends for DW and DG are similar, with the worst performance observed at 𝜖 = 0.6 for both density metrics. The trends for FD, TDS, and 𝑘CLiDS are also similar. DupinLPO detects denser subgraphs across most density metrics and is less affected by 𝜖 , due to its iterative trimming. For example, for FD, DupinLPO’s density decreases by only 6.23%, while DupinGPO and Dupin’s densities decrease by 17.37%. 6.3 Case study We also compare the performance in real-world scenarios, focusing on three key aspects: accuracy, latency, and prevention ratio. Accuracy. As discussed in Section 1, although Spade can quickly respond to new transactions, it tends to accumulate errors over time. Our analysis utilizes transaction data from Anon, with the transaction network comprising |𝐸 |= 2 billion edges. Under the incremental computation framework of Spade, the assumption is made that the weights of existing edges remain constant, thereby neglecting the impact of newly inserted edges on these weights, leading to error accumulation. For instance, using FD density metric, we observe that errors accumulate up to 24.4% within a day and increase to 30.3% over a week, as illustrated in Figure 9. Although Dupin sacrifices some accuracy to achieve parallel computational efficiency, our experiments show that Dupin’s errors remain relatively stable between 3% and 5%. Hence, the detection precision of Spade declines from 87.9% to 68.3%, while Dupin’s precision remains above 86%. Latency and Prevention Ratio. If a transaction is identified as fraudulent, subsequent related transactions are blocked to mitigate potential losses. We define the ratio of successfully prevented sus- picious transactions to the total number of suspicious transactions as R. As shown in Figure 6, the prevention ratio R continues to decrease as latency increases on Anon’s datasets. Using the default density metric, FD, in Anon, our results show thatDupin-FD can prevent 94.5% of fraudulent activities, GBBS-FD can prevent 3.0%, and Spade-FD can prevent 45.3%. The reason for the lower performance of GBBS-FD is its low parallelism in the peeling process, which results in long peeling times and thus an average latency of 6,014 seconds, delaying the prevention of many fraudulent activities. Although Spade-FD performs well in reordering techniques on smaller datasets, in industrial scenarios with billion-scale graphs, latency significantly worsens due to the extensive reordering range required in the peeling sequences as normal users transition to fraudsters, increasing the cost of incremental computations. Additional tests were conducted with other density metrics; for DG, Dupin-DG can prevent 78.8%, while other methods prevent varying amounts. Similarly, for DW, Dupin-DW can prevent 86.1%, while other methods demonstrate different prevention capabilities.',\n",
       " '7 CONCLUSION AND FUTUREWORKS': 'In this paper, we presented Dupin, a novel parallel fraud detection framework tailored for massive graphs. Our experiments demonstrated that Dupin’s advanced peeling algorithms and long-tail pruning techniques, DupinGPO and DupinLPO, significantly enhance the detection efficiency of dense subgraphs without sacrificing accuracy. Comparative studies with GBBS, PBBS, kCLIST, Spade, and Dupin highlighted Dupin’s superior performance in terms of com- putational efficiency and accuracy. Through case studies, we also validated that Dupin achieves lower latency and higher detection ac- curacy compared to existing fraud detection systems on billion-scale graphs. Overall, Dupinproves to be a powerful tool for large-scale fraud detection, providing a scalable and efficient solution for real-time applications. Future work will explore the integration of additional suspiciousness functions and further optimization to handle even larger datasets and more complex fraud scenarios.',\n",
       " 'REFERENCES': '[1] Distil networks: The 2019 bad bot report. https://www.bluecubesecurity.com/wp- content/uploads/bad-bot-report-2019LR.pdf. [2] B. Bahmani, R. Kumar, and S. Vassilvitskii. Densest subgraph in streaming and mapreduce. Proceedings of the VLDB Endowment, 5(5), 2012. [3] Y. Ban, X. Liu, T. Zhang, L. Huang, Y. Duan, X. Liu, and W. Xu. Badlink: Combining graph and information-theoretical features for online fraud group detection. arXiv preprint arXiv:1805.10053, 2018. [4] A. Beutel, W. Xu, V. Guruswami, C. Palow, and C. Faloutsos. Copycatch: stopping group attacks by spotting lockstep behavior in social networks. In Proceedings of the 22nd international conference on World Wide Web, pages 119–130, 2013. [5] D. Boob, Y. Gao, R. Peng, S. Sawlani, C. Tsourakakis, D.Wang, and J.Wang. Flowless: Extracting densest subgraphs without flow computations. In Proceedings of The Web Conference 2020, pages 573–583, 2020. [6] M. Charikar. Greedy approximation algorithms for finding dense components in a graph. In International Workshop on Approximation Algorithms for Combinatorial Optimization, pages 84–95. Springer, 2000. [7] M. Charikar. Greedy approximation algorithms for finding dense components in a graph. In Approximation Algorithms for Combinatorial Optimization, Third International Workshop, APPROX 2000, Saarbrücken, Germany, September 5-8, 2000, Proceedings, volume 1913 of Lecture Notes in Computer Science, pages 84–95. Springer, 2000. [8] C. Chekuri, K. Quanrud, and M. R. Torres. Densest subgraph: Supermodularity, iterative peeling, and flow. In Proceedings of the 2022 Annual ACM-SIAM Symposium on Discrete Algorithms (SODA), pages 1531–1555. SIAM, 2022. [9] J. Chen and Y. Saad. Dense subgraph extraction with application to community detection. IEEE Transactions on knowledge and data engineering, 24(7):1216–1230, 2010. [10] Y. Chen, J. Jiang, S. Sun, B. He, and M. Chen. Rush: Real-time burst subgraph detection in dynamic graphs. Proceedings of the VLDB Endowment, 17(11):3657– 3665, 2024. [11] N. Chiba and T. Nishizeki. Arboricity and subgraph listing algorithms. SIAM Journal on computing, 14(1):210–223, 1985. [12] L. Chu, Y. Zhang, Y. Yang, L. Wang, and J. Pei. Online density bursting subgraph detection from temporal graphs. Proceedings of the VLDB Endowment, 12(13):2353– 2365, 2019. [13] L. Dagum and R. Menon. Openmp: an industry standard api for shared-memory programming. IEEE computational science and engineering, 5(1):46–55, 1998. [14] M. Danisch, O. Balalau, and M. Sozio. Listing k-cliques in sparse real-world graphs. In Proceedings of the 2018 World Wide Web Conference, pages 589–598, 2018. [15] M. Danisch, T.-H. H. Chan, and M. Sozio. Large scale density-friendly graph decomposition via convex programming. In Proceedings of the 26th International Conference on World Wide Web, pages 233–242, 2017. [16] L. Dhulipala, J. Shi, T. Tseng, G. E. Blelloch, and J. Shun. The graph based benchmark suite (gbbs). In Proceedings of the 3rd Joint International Workshop on Graph Data Management Experiences & Systems (GRADES) and Network Data Analytics (NDA), pages 1–8, 2020. [17] Y. Dourisboure, F. Geraci, and M. Pellegrini. Extraction and classification of dense communities in the web. In Proceedings of the 16th international conference on World Wide Web, pages 461–470, 2007. [18] A. Epasto, S. Lattanzi, and M. Sozio. Efficient densest subgraph computation in evolving graphs. In Proceedings of the 24th international conference on world wide web, pages 300–310, 2015. [19] D. Gibson, R. Kumar, and A. Tomkins. Discovering large dense subgraphs in massive graphs. In Proceedings of the 31st international conference on Very large data bases, pages 721–732. Citeseer, 2005. [20] A. Gionis and C. E. Tsourakakis. Dense subgraph discovery: Kdd 2015 tutorial. In Proceedings of the 21th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, pages 2313–2314, 2015. [21] A. V. Goldberg. Finding a maximum density subgraph. 1984. [22] A. V. Goldberg and R. E. Tarjan. A new approach to the maximum-flow problem. Journal of the ACM (JACM), pages 921–940, 1988. [23] N. V. Gudapati, E. Malaguti, and M. Monaci. In search of dense subgraphs: How good is greedy peeling? Networks, 77(4):572–586, 2021. [24] B. Hooi, H. A. Song, A. Beutel, N. Shah, K. Shin, and C. Faloutsos. Fraudar: Bounding graph fraud in the face of camouflage. In Proceedings of the 22nd ACM SIGKDD international conference on knowledge discovery and data mining, pages 895–904, 2016. [25] M. Jaggi. Revisiting frank-wolfe: Projection-free sparse convex optimization. In International conference on machine learning, pages 427–435. PMLR, 2013. [26] J. Jiang, Y. Chen, B. He, M. Chen, and J. Chen. Spade+: A generic real-time fraud detection framework on dynamic graphs. IEEE Transactions on Knowledge and Data Engineering, 2024. [27] M. Jiang, P. Cui, A. Beutel, C. Faloutsos, and S. Yang. Inferring strange behavior from connectivity pattern in social networks. In Pacific-Asia conference on knowledge discovery and data mining, pages 126–138. Springer, 2014. [28] J. Jiaxin, L. Yuan, H. Bingsheng, H. Bryan, C. Jia, and J. K. Z. Kang. A real-time fraud detection framework on evolving graphs. PVLDB, 2023. [29] S. Kumar, W. L. Hamilton, J. Leskovec, and D. Jurafsky. Community interaction and conflict on the web. In Proceedings of the 2018 world wide web conference, pages 933–943, 2018. [30] V. E. Lee, N. Ruan, R. Jin, and C. Aggarwal. A survey of algorithms for dense subgraph discovery. In Managing and mining graph data, pages 303–336. Springer, 2010. [31] J. Leskovec and A. Krevl. SNAP Datasets: Stanford large network dataset collection. http://snap.stanford.edu/data, June 2014. [32] W. Luo, Z. Tang, Y. Fang, C. Ma, and X. Zhou. Scalable algorithms for densest subgraph discovery. In 2023 IEEE 39th International Conference on Data Engineering (ICDE), pages 287–300. IEEE, 2023. [33] C. Ma, Y. Fang, R. Cheng, L. V. Lakshmanan, and X. Han. A convex-programming approach for efficient directed densest subgraph discovery. In Proceedings of the 2022 International Conference on Management of Data, pages 845–859, 2022. [34] M. Mitzenmacher, J. Pachocki, R. Peng, C. Tsourakakis, and S. C. Xu. Scalable large near-clique detection in large-scale networks via sampling. In Proceedings of the 21th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, pages 815–824, 2015. [35] V. Podpečan, Ž. Ramšak, K. Gruden, H. Toivonen, and N. Lavrač. Interactive explo- ration of heterogeneous biological networks with biomine explorer. Bioinformatics, 06 2019. [36] H. Qin, R.-H. Li, Y. Yuan, G. Wang, L. Qin, and Z. Zhang. Mining bursting core in large temporal graphs. Proceedings of the VLDB Endowment, 2022. [37] Y. Ren, H. Zhu, J. Zhang, P. Dai, and L. Bo. Ensemfdet: An ensemble approach to fraud detection based on bipartite graph. In 2021 IEEE 37th International Conference on Data Engineering (ICDE), pages 2039–2044. IEEE, 2021. [38] R. A. Rossi and N. K. Ahmed. The network data repository with interactive graph analytics and visualization. In AAAI, 2015. [39] A. E. Sarıyüce andA. Pinar. Peeling bipartite networks for dense subgraph discovery. In Proceedings of the Eleventh ACM International Conference on Web Search and Data Mining, pages 504–512, 2018. [40] S. B. Seidman. Network structure and minimum degree. Social networks, 5(3):269– 287, 1983. [41] J. Shi, L. Dhulipala, and J. Shun. Parallel clique counting and peeling algorithms. In SIAM Conference on Applied and Computational Discrete Algorithms (ACDA21), pages 135–146. SIAM, 2021. [42] K. Shin, T. Eliassi-Rad, and C. Faloutsos. Corescope: Graph mining using k-core analysis—patterns, anomalies and algorithms. In 2016 IEEE 16th international conference on data mining (ICDM), pages 469–478. IEEE, 2016. [43] K. Shin, B. Hooi, J. Kim, and C. Faloutsos. Densealert: Incremental dense-subtensor detection in tensor streams. In Proceedings of the 23rd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, pages 1057–1066, 2017. [44] P. Sukprasert, Q. C. Liu, L. Dhulipala, and J. Shun. Practical parallel algorithms for near-optimal densest subgraphs on massive graphs. In 2024 Proceedings of the Symposium on Algorithm Engineering and Experiments (ALENEX), pages 59–73. SIAM, 2024. [45] B. Sun, M. Danisch, T. H. Chan, and M. Sozio. Kclist++: A simple algorithm for finding k-clique densest subgraphs in large graphs. Proceedings of the VLDB Endowment (PVLDB), 2020. [46] C. Tsourakakis. The k-clique densest subgraph problem. In Proceedings of the 24th international conference on world wide web, pages 1122–1132, 2015. [47] C. Ye, Y. Li, B. He, Z. Li, and J. Sun. Gpu-accelerated graph label propagation for real-time fraud detection. In Proceedings of the 2021 International Conference on Management of Data, pages 2348–2356, 2021.'}"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "parse_document(full_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "import re\n",
    "\n",
    "def parse_document(text):\n",
    "    # Preprocess the text to ensure consistent heading formatting\n",
    "    # Replace any newline followed by whitespace(s) and digit(s) with a newline and the digits\n",
    "    text = re.sub(r'\\n\\s*(\\d+)', r'\\n\\1', text)\n",
    "\n",
    "    # Replace any occurrence where a number is not at the start of a line\n",
    "    text = re.sub(r'(?<!\\n)(\\d+\\s+[A-Z].*)', r'\\n\\1', text)\n",
    "\n",
    "    # Split text into lines\n",
    "    lines = text.split('\\n')\n",
    "\n",
    "    sections = {}\n",
    "    current_section = None\n",
    "    current_content = []\n",
    "\n",
    "    # Define patterns for main headings and known headings\n",
    "    main_heading_pattern = re.compile(r'^(\\d+(?:\\.\\d+)*)\\s+(.*)$')\n",
    "    known_headings = {'Abstract', 'Acknowledgements', 'References', 'Conclusion', 'Introduction', 'Background',\n",
    "                      'Related Work', 'Methodology', 'Results', 'Discussion', 'Appendix'}\n",
    "    known_headings_lower = {h.lower() for h in known_headings}\n",
    "\n",
    "    for line in lines:\n",
    "        line = line.strip()\n",
    "        if not line:\n",
    "            continue  # Skip empty lines\n",
    "\n",
    "        # Check if the line is a known heading\n",
    "        if line.lower() in known_headings_lower:\n",
    "            # Save current section\n",
    "            if current_section is not None:\n",
    "                content = ' '.join(current_content).strip()\n",
    "                sections[current_section] = content\n",
    "            # Start new section\n",
    "            current_section = line\n",
    "            current_content = []\n",
    "            continue\n",
    "\n",
    "        # Check if the line is a numbered heading\n",
    "        m = main_heading_pattern.match(line)\n",
    "        if m:\n",
    "            # Save current section\n",
    "            if current_section is not None:\n",
    "                content = ' '.join(current_content).strip()\n",
    "                sections[current_section] = content\n",
    "            # Start new section\n",
    "            section_number = m.group(1)\n",
    "            section_title = m.group(2)\n",
    "            current_section = f\"{section_number} {section_title}\"\n",
    "            current_content = []\n",
    "            continue\n",
    "\n",
    "        # Otherwise, it's content\n",
    "        if current_section is None:\n",
    "            # If no section yet, start with 'Abstract'\n",
    "            current_section = 'Abstract'\n",
    "        current_content.append(line)\n",
    "\n",
    "    # Save the last section\n",
    "    if current_section is not None and current_content:\n",
    "        content = ' '.join(current_content).strip()\n",
    "        sections[current_section] = content\n",
    "\n",
    "    return sections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Abstract': 'ABSTRACT Detecting fraudulent activities in financial and e-commerce transac- tion networks is crucial, and one effective method for this is Densest Subgraph Discovery (DSD). However, deploying DSD methods in production systems faces substantial scalability challenges due to the predominantly sequential nature of existing methods, which impedes their ability to handle large-scale transaction networks and results in significant detection delays. In this paper, we introduce Dupin, a novel parallel processing framework designed for efficient DSD processing in billion-scale graphs. Dupin is powered by a processing engine that exploits the unique properties of the peeling process, with theoretical guarantees on detection quality and efficiency.Dupin provides user-friendly APIs for flexible customization of DSD objectives and ensures robust adaptability to diverse fraud detection scenarios. Empirical evaluations indicate that Dupin outperforms several existing DSD methods, with performance improvements observed in specific scenarios reaching up to two orders of magnitude. On billion-scale graphs, Dupin demonstrates the potential to enhance the prevention of fraudulent transactions by approximately 49.5 basis points and reduce density error from 30% to below 5%, as supported by our experimental results. ACM Reference Format: Anonymous Author(s). 2024. Dupin: A Parallel Densest Subgraph Discovery Framework on Massive Graphs. In Proceedings of ACM Conference (Conference’17). ACM, New York, NY, USA, 14 pages. https://doi.org/10.1145/ nnnnnnn.nnnnnnn',\n",
       " '1 INTRODUCTION': 'Graph structures pervade numerous contemporary applications, ranging from transaction and communication networks to expan- sive social media platforms. The study of densest subgraph discovery (DSD), first explored by [21], has since proven instrumental in various domains: link spam identification [4, 19], community de- tection [9, 17], and notably, fraud detection [8, 24, 42]. Central to this domain is the peeling process [2, 5, 8, 24, 46], which operates by iteratively removing vertices based on density metrics, such as vertex degree or the cumulative weight of adjacent edges. Their widespread adoption can be attributed to their inherent efficiency, robustness, and proven worst-case performance guarantees for DSD. In practice, business requirements demand the detection of fraud- ulent communities on billion-scale graphs within seconds. Recent Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for components of this work owned by others than ACM must be honored. Abstracting with credit is permitted. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. Request permissions from permissions@acm.org. Conference’17, July 2017, Washington, DC, USA ©',\n",
       " '2024 Association for Computing Machinery. ACM ISBN 978-x-xxxx-xxxx-x/YY/MM. . . $15.00 https://doi.org/10.1145/nnnnnnn.nnnnnnn studies, including those by [1] and [47], have highlighted the perva- sive issue of malicious bot activity in e-commerce, accounting for a substantial 21.4% of all traffic in 2018. These fraudulent activities often happen quickly, making them hard to detect. The inability to efficiently process and analyze such massive graphs not only ham- pers fraud detection efforts but also exposes businesses to substan- tial financial risks and reputational damage. Therefore, addressing these challenges is not merely an academic exercise; it is a press- ing necessity for enhancing the integrity of digital transactions and safeguarding consumer trust. Despite extensive research onDSD for static graphs [20–22, 30, 33, 34], dynamic graphs [2, 18, 26, 28, 43], and parallel approaches [14, 16, 41, 42], existing methods face significant challenges for practical de- ployment in real-world fraud detection systems. For instance, many existing algorithms primarily rely on static score functions for DSD, lacking the flexibility required for dynamic and evolving fraud detec- tion scenarios. Methods such as [4] and [27] are designed with fixed detection criteria, which can become less effective as fraudulent be- haviors adapt over time. Additionally, incremental algorithms remain inefficient at handling billion-scale graphs with high transaction volumes, leading to increased latency. Parallel DSD frameworks often lack efficient pruning techniques, resulting in excessive iterations that degrade performance on large-scale graphs. This paper addresses the following research question: How can we enhance the efficiency and scalability of DSD methods to effectively detect fraud in massive graphs? Challenges. We evaluated existing methods with our industry partner, Anon (anonymized for review). The following issues surfaced: Firstly, existing algorithms focus on static score functions for DSD and lack flexible customization. While this may be adequate for short-term detection, it becomes less effective over time. For example, as shown in Figure 1a, graph density can accumulate sig- nificant errors within a single day, highlighting the need for more adaptive and customizable detection methods. Secondly, existing incremental algorithms are still inefficient at handling billion-scale graphs with high transaction volumes, leading to increased latency. This limitation is evident in Figure 1b, which illustrates the system load distribution for a specific day’s transactions, underscoring the inability of current methods to efficiently manage high transaction volumes without significant delays. Thirdly, existing parallel densest subgraph discovery frameworks lack efficient pruning techniques, re- sulting in a large number of iterations. This inefficiency is especially problematic on billion-scale graphs, where performance degrades substantially due to the excessive computational overhead caused by numerous iterations. Adapting to Evolving Research Needs. Recognizing the dynamic nature of fraud detection and the rapid evolution of fraudulent tactics, we have adapted our research focus to address these challenges. Our approach incorporates flexible customization options for DSD methods, allowing for adjustments based on the changing landscape of fraudulent activities. This adaptability is crucial for maintaining the effectiveness of fraud detection systems in an environment where new threats continuously emerge. Contributions. In this work, we propose a DSD framework for fraud detection. Our key contributions are as follows: • A DSD Parallelization Framework. We propose Dupin, a novel parallel processing framework for densest subgraph discovery (DSD) that breaks the sequential dependencies inherent in tra- ditional density metrics, significantly enhancing scalability and efficiency on both weighted and unweighted graphs. Unlike existing methods that focus on specific density metrics, Dupin allows developers to define custom fraud detection semantics through flexible suspiciousness functions for edges and vertices. It supports a wide range of DSD semantics, including DG [6], DW [23], FD [24], TDS [46], and 𝑘CLiDS [14]. Importantly, Dupin not only leverages parallelism but also provides theoretical guarantees on approximation ratios for the supported density metrics. To the best of our knowledge, this is the first generic parallel processing framework that accommodates a broad class of density metrics in DSD, offering both flexibility and theoretical assurance. • Exploiting Long-Tail Properties.We identify and address the long-tail problem in the peeling process of DSD algorithms, where late iterations remove only a small fraction of vertices and con- tribute little to the quality of the densest subgraph, leading to inefficiency. To tackle this, we introduce two novel optimizations: Global Pruning Optimization, which proactively removes low-contribution vertices within each peeling iteration to reduce computational overhead, and Local Pruning Optimization, which eliminates disparate structures formed after vertex removal to enhance the density and quality of the detected subgraph. These optimizations significantly reduce the number of iterations re- quired, improving both performance and result quality. • Empirical Validation and Real-World Impact: We conduct comprehensive experiments on large-scale industry datasets to validate the effectiveness of Dupin. Our results show that Dupin achieves up to 100 times faster fraud detection compared to the state-of-the-art incremental method Spade [28] and parallel methods like PBBS [41]. Moreover,Dupin is capable of preventing up to 94.5% of potential fraud, demonstrating significant practical bene- fits and reinforcing system integrity in real-world applications. Organization. The remainder of this paper is structured as follows: Section 2 presents the background and the literature review. Section 3 introduces the architecture of Dupin. In Section 4, we propose the theoretical foundations for parallel peeling-based algorithms for DSD. Section 5 introduces long-tail pruning techniques to improve efficiency and effectiveness. The experimental evaluation of Dupin is presented in Section 6. Finally, Section 7 concludes the paper and outlines avenues for future research.': '',\n",
       " '2 BACKGROUND AND RELATEDWORK': '2.',\n",
       " '1 Preliminary': 'This subsection presents the preliminary. Some frequently used no- tations are summarized in Table 1. Graph 𝐺 . In this work, we operate on a weighted graph denoted as 𝐺 = (𝑉 , 𝐸), where 𝑉 represents the set of vertices and 𝐸 (⊆ 𝑉 ×𝑉 ) represents the set of edges. Each edge (𝑢𝑖 , 𝑢 𝑗 ) ∈ 𝐸 is assigned a nonnegative weight, expressed as 𝑐𝑖 𝑗 . Additionally, 𝑁 (𝑢) denotes the set of neighboring vertices connected to vertex 𝑢. Induced Subgraph. For a given subset 𝑆 of vertices in 𝑉 , we define the induced subgraph as 𝐺[𝑆] = (𝑆, 𝐸[𝑆]), where 𝐸[𝑆] = {(𝑢, 𝑣) | (𝑢, 𝑣) ∈ 𝐸 ∧ 𝑢, 𝑣 ∈ 𝑆}. The size of the subset 𝑆 is denoted by |𝑆 |. Density Metrics 𝑔 and Weight Function 𝑓 . We utilize the class of metrics𝑔 as defined in prior works [6, 24, 28], where for a given subset of vertices 𝑆 ⊆ 𝑉 , the density metric is computed as: 𝑔(𝑆) = 𝑓 (𝑆) |𝑆 | . Here, 𝑓 (𝑆) represents the total weight of the induced subgraph 𝐺[𝑆], which is defined as the sum of the weights of vertices in 𝑆 and the weights of edges in 𝐸[𝑆]. More formally: 𝑓 (𝑆) = ∑︁ 𝑢𝑖 ∈𝑆 𝑎𝑖 + ∑︁ (𝑢𝑖 ,𝑢 𝑗 )∈𝐸[𝑆] 𝑐𝑖 𝑗 (1) The weight of a vertex 𝑢𝑖 , denoted as 𝑎𝑖 (𝑎𝑖 ≥ 0), quantifies the suspiciousness of user 𝑢𝑖 . The weight of an edge (𝑢𝑖 , 𝑢 𝑗 ), represented by 𝑐𝑖 𝑗 (𝑐𝑖 𝑗 > 0), reflects the suspiciousness of the transaction between users 𝑢𝑖 and 𝑢 𝑗 . Intuitively, the density metric 𝑔(𝑆) encapsulates the overall suspiciousness density of the induced subgraph𝐺[𝑆]. A larger value of 𝑔(𝑆) indicates a higher density of suspiciousness within𝐺[𝑆]. The vertex set that maximizes the density metric 𝑔 is denoted by 𝑆∗. We next introduce five representative density metrics extensively employed in many fraud detection applications. The density metrics are mainly differed by how the weight function is defined. Dense Subgraphs (DG) [6]. The DG metric is employed to quan- tify the connectivity of substructures within a graph. It has found extensive applications in various domains, such as detecting fake comments in online platforms [29] and identifying fraudulent activities in social networks [3]. Given a subset of vertices 𝑆 ⊆ 𝑉 , the weight function of DG is defined as 𝑓 (𝑆) = |𝐸[𝑆]|, where |𝐸[𝑆]| denotes the number of edges in the induced subgraph 𝐺[𝑆]. Dense Subgraph on Weighted Graphs (DW) [23]. In transac- tion graphs, it is common to have weights assigned to the edges, representing quantities such as the transaction amount. The DW metric extends the concept of dense subgraphs to accommodate these weighted relationships. Given a subset of vertices 𝑆 ⊆ 𝑉 , the weight function of DW is defined as 𝑓 (𝑆) = ∑ (𝑢𝑖 ,𝑢 𝑗 )∈𝐸[𝑆] 𝑐𝑖 𝑗 . Fraudar (FD) [24]. In order to mitigate the effects of fraudulent actors deliberately obfuscating their activities, Hooi et al. [24] introduced the FD algorithm. This approach innovatively incorporates weights for edges and assigns prior suspiciousness scores to vertices, potentially utilizing side information to enhance detection accuracy. Given a subset of vertices 𝑆 ⊆ 𝑉 , the weight function of FD is defined as Equation 1 where 𝑎𝑖 denotes the suspicious score of vertex 𝑢𝑖 and 𝑐𝑖 𝑗 = 1 log(𝑥+𝑐) . Here 𝑥 is the degree of the object vertex between 𝑢𝑖 and 𝑢 𝑗 , and 𝑐 is a positive constant [24]. Triangle Densest Subgraph (TDS) [46]. Triangles serve as a crucial structural motif in graphs, providing valuable insights into the connectivity and cohesiveness of subgraphs. The TDS metric, in par- ticular, leverages the prevalence of triangles to quantify the density of a subgraph. Given a subset of vertices 𝑆 ⊆ 𝑉 , the weight function of TDS is defined as 𝑓 (𝑆) = 𝑡 (𝑆) where 𝑡 (𝑆) represents the total number of triangles within the induced subgraph 𝐺[𝑆]. 𝑘-Clique Densest Subgraph (𝑘CLiDS) [14]. Extending the concept of triangle densest subgraphs, 𝑘CLiDS focuses on identifying dense subgraphs based on the presence of 𝑘-Cliques. A 𝑘-Clique is a complete subgraph of 𝑘 vertices, representing a tightly-knit group of vertices. For a given subset of vertices 𝑆 ⊆ 𝑉 , the weight function of 𝑘CLiDS is defined as 𝑓 (𝑆) = 𝑘(𝑆), where 𝑘(𝑆) denotes the number of 𝑘-Cliques within the induced subgraph 𝐺[𝑆]. 2.',\n",
       " '2 Sequential Peeling Algorithms': 'Peeling algorithms have gained popularity for their efficiency, ro- bustness, and proven worst-case performance in various applica- tions [6, 24, 46]. Next, we provide a concise overview of the typical execution flow of these algorithms. Peeling Weight.We use𝑤𝑢𝑖 (𝑆) to indicate the decrease in the value of the weight function 𝑓 when the vertex𝑢𝑖 is removed from a vertex set 𝑆 , i.e., the peeling weight. Formally, 𝑤𝑢𝑖 (𝑆) = 𝑎𝑖 + ∑︁ (𝑢 𝑗 ∈𝑆) ∧ ((𝑢𝑖 ,𝑢 𝑗 )∈𝐸) 𝑐𝑖 𝑗 + ∑︁ (𝑢 𝑗 ∈𝑆) ∧ ((𝑢 𝑗 ,𝑢𝑖 )∈𝐸) 𝑐 𝑗𝑖 (2) Algorithm 1: Sequential Peeling Algorithm Input: A graph𝐺 = (𝑉 , 𝐸) and a density metric 𝑔(𝑆) Output: A subset of vertices 𝑆 that maximizes the density metric 𝑔(𝑆) 1 𝑆0 ← 𝑉 /* Initialize the subset with all vertices */ 2 for 𝑖 ← 1 to |𝑉 | do 3 𝑢 ← arg max𝑣∈𝑆𝑖−1 𝑔(𝑆𝑖−1 \\\\ {𝑣 }) /* The vertex to be peeled */ 4 𝑆𝑖 ← 𝑆𝑖−1 \\\\ {𝑢 } /* The selected vertex */ 5 return arg max𝑆𝑖 𝑔(𝑆𝑖 ) /* The subset that maximizes 𝑔(𝑆) */ Sequential PeelingAlgorithm (Algorithm1).The peeling process starts with the full vertex set 𝑆0 = 𝑉 (Line 1) and iteratively removes vertices to maximize the density metric 𝑔. In each step 𝑖 , a vertex𝑢𝑖 is removed from 𝑆𝑖−1 to form 𝑆𝑖 , such that 𝑔(𝑆𝑖−1 \\\\ {𝑢𝑖 }) is maximized (Lines 3∼4). This process is repeated until 𝑆𝑖 becomes empty, resulting in a sequence of nested sets 𝑆0, 𝑆1, . . . , 𝑆 |𝑉 | . The algorithm ultimately returns the subset 𝑆𝑖 ∈ {𝑆0, . . . , 𝑆 |𝑉 |} that maximizes 𝑔(𝑆𝑖 ). Example 2.1 (Seqential Peeling Algorithm Process). Consider the graph 𝐺 as shown in Figure 2. The process begins with an initial graph density of 2.33. In the first iteration, vertex 𝑢1 is peeled due to its smallest peeling weight among all vertices. Subsequently, vertices are peeled in the following order: 𝑢2, 𝑢3, 𝑢4, 𝑢5, and 𝑢6 based on the updated criteria after each peeling operation. Notably, after peeling 𝑢2, the graph density increases, which is a characteristic observation in the peeling process. The sequence of peeling continues until the last vertex, 𝑢6, is peeled, resulting in a final graph density of 0. The peeling sequence, therefore, is 𝑂 = [𝑢1, 𝑢2, 𝑢3, 𝑢4, 𝑢5, 𝑢6], completely disassembling the graph. The induced subgraph𝐺[𝑆] of 𝑆 = [𝑢3, 𝑢4, 𝑢5, 𝑢6] has the greatest density of 2.75, thus 𝐺[𝑆] is returned. Approximation. An algorithm 𝑄 is defined as an 𝛼-approximation for the Densest Subgraph Discovery (DSD), where 𝛼 ≥ 1, if for any given graph it returns a subset 𝑆 satisfying the condition: 𝑔(𝑆) ≥ 𝑔(𝑆∗) 𝛼 , where 𝑆∗ is the optimal subset that maximizes the density metric 𝑔. Theorem 2.1 ([24]). For the vertex set 𝑆𝑝 returned by Algorithm 1 and the optimal vertex set 𝑆∗, it holds that 𝑔(𝑆𝑝 ) ≥ 𝑔(𝑆∗) 2 for DG, DW and FD as the density metrics. We extend the analysis to TDS and 𝑘CLiDS. The proof of the following theorem is omitted due to space limitation. Theorem 2.2. For the vertex set 𝑆𝑝 returned by Algorithm 1 and the optimal vertex set 𝑆∗, it holds that 𝑔(𝑆𝑝 ) ≥ 𝑔(𝑆∗) 𝑘 for TDS and 𝑘CLiDS as the density metrics where 𝑘 = 3 for TDS. 2.',\n",
       " '3 Related work': 'Densest Subgraph Discovery (DSD). Densest subgraph discov- ery represents a core area of research in graph theory, extensively explored in literature [7, 8, 20, 30, 39, 40]. The foundational max-flow- based algorithm by Goldberg et al. [21] set a benchmark for exact densest subgraph detection, with subsequent studies introducing more scalable exact solutions [33, 34]. While speed enhancements have been achieved through various greedy approaches [6, 8, 24, 33], these methods are inherently sequential and difficult to parallelize. For example, they often peel one vertex at a time, which results in too many peeling iterations. This sequential nature limits their scalability and efficiency on massive graphs. In graph-based fraud de- tection, techniques like COPYCATCH [4] and GETTHESCOOP [27] leverage local search heuristics to identify dense subgraphs in bi- partite networks, frequently applied in fraud, spam, and community detection across social and review platforms [24, 37, 42]. Despite their widespread use, these methods suffer from efficiency issues in very large graphs, often involving numerous iterations and lack- ing effective pruning techniques, which exacerbates the long-tail problem. Moreover, they are limited to specific definitions of dense subgraphs. Contrarily, Dupin offers a versatile, parallel approach to densest subgraph detection. It facilitates the simultaneous peeling of multiple vertices, drastically reducing the computational overhead. Additionally, Dupin provides APIs that allow users to customize defi- nitions of dense subgraphs, ensuring that the resultant graph density adheres to theoretical bounds. DSD on Dynamic Graphs. To tackle real-time fraud detection, sev- eral variants have been designed for dynamic graphs [10, 18, 26, 28, 43]. [28] addresses real-time fraud community detection on evolving graphs (edge insertions) using an incremental peeling sequence re- ordering method, while [26] extends this approach to fully dynamic graphs (edge insertions and deletions). [43] proposes an incremental algorithm that maintains and updates a dense subtensor in a ten- sor stream. [12] and [36] utilize sliding windows to detect dense subgraphs and bursting cores within specific time windows. These methods have three limitations. First, response time is an issue. Al- though they respond quickly to benign communities, the results can change drastically for newly formed fraudulent communities, caus- ing significant delays in incremental calculations for fraud detection. Second, efficiency is problematic. While incremental processing is faster than recomputation for a single transaction on billion-scale graphs, frequent incremental evaluations can be slower than a full recomputation. Third, in terms of flexibility, most methods support only a single semantic. In contrast, Dupin introduces a unified parallel framework for peeling-based dense subgraph detection.Dupin can respond within seconds on billion-scale graphs and defines multiple density metrics that can be used on the Dupin platform. Parallel DSD. Several parallel methods for DSD have been pro- posed [2, 14, 16, 39, 41, 42]. Bahmani et al. [2] proposed a parallel algorithm for DG using the MapReduce framework. FWA [15] proposed a parallel algorithm for DW based on Frank-Wolfe algo- rithm [25] using OpenMP [13]. Some works propose parallel 𝑘-core algorithms [32, 41, 44]. Others introduce parallel algorithms for 𝑘- Clique detection, with approaches offering a (1 + 𝜖)-approximation ratio and parallel implementations for the 𝑘CLiDS problem [16, 45]. There are also algorithms for 𝑘-Clique densest subgraph detection using OpenMP [14], and methods for dense subtensor detection [42]. However, these systems have two main limitations. First, they typi- cally support only a single-densitymetric and do not handle weighted graphs. In real-world applications, transactions between users can have varying degrees of importance based on amounts, frequen- cies [28], and other factors. Extending these systems to support other dense metrics with theoretical guarantees is non-trivial. Second, they often lack efficient pruning techniques, requiring numerous itera- tions and suffering from the long-tail issue, which hinders scalability on massive graphs. In contrast, Dupin defines the characteristics of a set of supported dense metrics and accommodates weighted sub- graphs, allowing for more nuanced analysis reflective of real-world data. Our method introduces efficient pruning strategies by peeling multiple vertices in parallel based on a threshold mechanism, signifi- cantly reducing the number of iterations and mitigating the long-tail problem. This makes Dupin more effective and scalable in diverse real-world scenarios compared to previous approaches.',\n",
       " '3 THE Dupin FRAMEWORK': '3.',\n",
       " '1 Motivations': 'A notable example of advanced fraud detection is seen in Anon, a prominent technology company in Southeast Asia known for its digital payment and food delivery services. As delineated in [28], the detection process begins with forming a transaction graph,𝐺 , derived from the platform’s transactions. This graph is incrementally updated, represented as 𝐺 = 𝐺 ⊕ ∆𝐺 . Subsequent stages utilize densest subgraph discovery algorithms to uncover fraudulent communities. A typical fraud scenario on the platform involves customer-merchant collusion, where fake accounts are created to exploit promotional incentives. These fraudulent activities manifest as densest subgraphs within the transaction graph. Efficiency. Our empirical findings reveal that the incremental execution of DSD algorithms, such as Spade [28], on a graph with 2 billion edges requires approximately 198 seconds per batch (default setting: one batch of 1K edge insertions). However, in practical business sce- narios, the number of incoming edges can peak at 400K per minute. Clearly, using incremental evaluation on billion-scale graphs does not provide sufficient throughput to meet business requirements. Latency and Prevention Ratio. Existing DSD systems face sig- nificant challenges in terms of latency and prevention ratio. Upon identification of fraudulent activities, actions such as account bans or freezes are implemented to prevent further losses. However, the aver- age latency for these systems to detect fraud and take action is often high, typically around 200 to 300 seconds for incremental algorithms and more than 1000 seconds for existing parallel algorithms, which is insufficient for real-time requirements (operational demand requires detection within seconds). Moreover, when the graph size increases from million-scale to billion-scale, the prevention ratio decreases from 92.47% to 45%. These issues underscore the necessity for more efficient and scalable algorithms that can maintain low latency and high prevention ratios, even under heavy transaction loads. Parallelization offers a viable solution to these challenges by dis- tributing the computational load across multiple processors. Mod- ern CPUs are equipped with numerous cores that are cost-effective, enabling parallel processing. If the peeling process of DSD is paral- lelized, different processors can simultaneously evaluate and peel different nodes, thereby increasing scalability and efficiency. For example, if we can improve processing speed by one to two orders of magnitude, the system would be able to meet industry requirements for real-time fraud detection, completing detection within seconds. Problem Statement. Given a graph 𝐺 = (𝑉 , 𝐸) and a community detection semantic with density metric 𝑔(𝑆), our goal is to find a vertex subset 𝑆∗ ⊆ 𝑉 that maximizes 𝑔(𝑆∗). 3.',\n",
       " '2 System Architecture': 'To enable users to design complex DSD algorithms to detect fraudsters based on our framework, Dupin, we have introduced the archi- tecture, APIs, and supported density metrics in this section. Architecture of Dupin. Figure 3 presents the architecture of Dupin with user-defined functions for detecting suspicious activities.Dupin enables users to define their fraud detection semantics through a set of provided APIs. Within Dupin’s computational engine, we sup- port both sequential and parallel peeling algorithms. Additionally, algorithms for long-tail pruning techniques are implemented to ac- celerate convergence and enhance the density of subgraphs in each iteration. Dupin identifies fraudulent communities, which are then flagged for further investigation by moderators. APIs of Dupin (Figure 3). Dupin simplifies the process of defining detection semantics by allowing users to specify a few simple functions. Additionally, Dupin offers APIs that enable users to balance detection speed and accuracy effectively. The key APIs encompass several components designed to facilitate the customization and op- timization of fraud detection processes. These components provide a robust framework for users to implement their fraud detection strate- gies, ensuring both high performance and precision in identifying fraudulent activities. Details are listed below: • pWeight and updateNgh. These functions are designed for the dynamic adjustment of weights within the network. They allow the user to define the peeling weight of a vertex and specify the method for updating the peeling weight of neighboring vertices after a vertex has been peeled. • VSusp and ESusp. Given a vertex/edge, these components are re- sponsible for deciding the suspiciousness of the endpoint of the edge or the edge with a user-defined strategy. • isBenign. This component is used to decide whether a vertex is benign during the whole peeling process (Section 5). If the vertex is benign, it is peeled within the current iteration. • setEpsilon. This function is utilized to control the precision of fraud detection. During periods of high system load, moderators can increase 𝜖 to achieve higher throughput. Conversely, when the system load is low, 𝜖 can be decreased to enhance accuracy. • setK. This function specifies the 𝑘-Clique parameter, enabling the definition of the 𝑘-Clique for analysis. 1 double vsusp(const Vertex& v, const Graph& g) { 2 return g.weight[v]; // Side information on vertex 3 } 4 double esusp(const Edge& e, const Graph& g) { 5 return 1.0 / log(g.deg[e.src] + 5.0); 6 } 7 int main() {',\n",
       " '8 Dupin dupin; 9 dupin.VSusp(vsusp); // Plug in vsusp 10 dupin.ESusp(esusp); // Plug in esusp 11 dupin.setEpsilon (0.1); 12 dupin.LoadGraph(\"graph_sample_path\"); 13 vector <Vertex > fraudsters = dupin.ParDetect (); 14 return 0; 15 } Listing 1: Implementation of FD on Dupin Example (Listing 1). To implement FD [24] on Dupin, users simply need to plug in the suspiciousness function vsusp for the vertices by calling VSusp and the suspiciousness function esusp for the edges by calling ESusp. Specifically, 1) vsusp is a constant function, i.e., given a vertex 𝑢, vsusp(𝑢) = 𝑎𝑖 and 2) esusp is a logarithmic function such that given an edge (𝑢𝑖 , 𝑢 𝑗 ), esusp(𝑢𝑖 , 𝑢 𝑗 ) = 1 log(𝑥+𝑐) , where 𝑥 is the degree of the object vertex between 𝑢𝑖 and 𝑢 𝑗 , and 𝑐 is a positive constant [24]. Developers can easily implement customized peeling algorithms with Dupin, significantly reducing the engineering effort. Density Metrics Support in Dupin. To illustrate the versatility of Dupin in handling various density metrics, we establish comprehensive sufficient conditions. These conditions ensure Dupin’s adaptability in integrating a wide range of metrics to assess graph density effectively. This foundational approach enablesDupin to sup- port diverse density calculations, crucial for nuanced fraud detection and network analysis tasks. Property 3.1. Dupin supports a density metric 𝑔(𝑆) if the following conditions are met: (1) 𝑔(𝑆) = 𝑓 (𝑆) |𝑆 | , where 𝑓 is a monotone increasing function. (2) 𝑎𝑖 ≥ 0, ensuring non-negative vertex weight function. (3) 𝑐𝑖 𝑗 ≥ 0, ensuring non-negative edge weight function.': '',\n",
       " '4 PARALLELABLE PEELING ALGORITHMS': 'For sequential peeling algorithms, only one vertex can be removed at a time, with each vertex’s removal depending on the removal of previous vertices. This dependency prevents the algorithm from executing multiple vertex removals simultaneously. In this section, we introduce a parallel peeling paradigm to break this dependency chain. This new approach allowsmultiple vertices to be peeled in parallel without affecting the programming abstractions that Dupin provides to end users. We then demonstrate how this paradigm offers fine-grained trade-offs between parallelism and ap- proximation guarantees for all density metrics studied in this work. 4.',\n",
       " '1 Parallel Peeling Paradigm': 'Instead of peeling the vertex with the maximum peeling weight, we can peel all vertices that lead to a significant decrease in the density scores of the remaining graph. This decrease is controlled by a tunable parameter 𝜖 . A larger 𝜖 allows more vertices to be peeled in parallel, while a smaller 𝜖 provides a better approximation ratio. Algorithm 2: Dupin: Parallel Peeling Input:𝐺 = (𝑉 , 𝐸), a density metric 𝑔(𝑆), and 𝜖 ≥',\n",
       " '0 Output: A subset of vertices 𝑆 that maximizes the density metric 𝑔(𝑆) 1 𝑆0 ← 𝑉 /* Initialize the subset with all vertices */ 2 𝑖 ← 1 /* Initialize the index */ 3 while 𝑆𝑖−1 ̸= ∅ do 4 𝜏 = 𝑘(1 + 𝜖) · 𝑔(𝑆𝑖−1) /* The threshold for peeling */ 5 𝑈 ← {𝑢 | 𝑤𝑢 (𝑆𝑖−1) ≤ 𝜏 } /* Vertices to be peeled */ 6 𝑆𝑖 ← 𝑆𝑖−1 \\\\𝑈 /* Update the subset */ 7 𝑖 ← 𝑖 + 1 /* Increment the index */ 8 return arg max𝑆𝑖 𝑔(𝑆𝑖 ) /* The subset that maximizes 𝑔(𝑆) */ Parallel Peeling (Algorithm 2). Let 𝑆𝑖 denote the vertex set after the 𝑖-th peeling iteration. Initially, 𝑆0 = 𝑉 (Line 1). In each iteration, it removes a subset of vertices𝑈 from 𝑆𝑖−1, where ∀𝑢 ∈ 𝑈 ,𝑤𝑢 (𝑆𝑖−1) ≤ 𝑘(1 + 𝜖)𝑔(𝑆𝑖−1). Parameter 𝑘 is determined by the metric used. 𝑘 = 2 works for DG, DW and FD. 𝑘 = 3 works for TDS and 𝑘 is the size of the cliques for 𝑘CLiDS. This process is repeated recursively until no vertices are left, resulting in a series of vertex sets 𝑆0, . . . , 𝑆𝑅 , where 𝑅 is the number of iterations. The algorithm then returns the set 𝑆𝑖 (𝑖 ∈ [0, 𝑅]) that maximizes the density metric 𝑔(𝑆𝑖 ), denoted as 𝑆𝑝 . In what follows, we also show a case where 𝑘CLiDS is the density metric for DSD. 3 3 6 6 5 4 21 0 00 3 3 6 6 3 3 0 0 Density: 0.91 Density: 1.33 Density: 0 Iteration i Iteration i+1 Iteration i+2 Iteration i+3 Density: 0 τ = 3× 0.91 = 2.73 τ = 3× 1.33 = 3.99 τ = 0 τ = 0': '4.',\n",
       " '2 Theoretical Analysis': 'In this section, we analyze the theoretical properties of the parallel peeling process. We begin by examining the number of iterations required for parallel peeling. Built upon the result, we then determine the time complexity and the approximation ratio of the parallel peeling process. Lemma 4.1. Given any 𝜖 > 0, 𝑅 < log 1+𝜖 |𝑉 |. Proof. We prove that for any 𝜖 ≥ 0, the size of the remaining vertex set is bounded by |𝑆𝑖 |< 1 1+𝜖 |𝑆𝑖−1 | for all 𝑖 ∈ [1, 𝑅]. We first examine the peeling weights of the vertices in 𝑆𝑖−1 for the density metrics DG. DW and FD where 𝑘 = 2. As each edge weight in the peeling weight calculation contributes twice (once for each endpoint), and the vertex weights contribute once, we have∑︁ 𝑢∈𝑆𝑖−1 𝑤𝑢 (𝑆𝑖−1) ≤ 2𝑓 (𝑆𝑖−1), (3) The peeling weights of the vertices in 𝑆𝑖−1 are calculated as follows:∑︁ 𝑢∈𝑆𝑖−1 𝑤𝑢 (𝑆𝑖−1) = ∑︁ 𝑢∈𝑆𝑖−1\\\\𝑆𝑖 𝑤𝑢 (𝑆𝑖−1) + ∑︁ 𝑢∈𝑆𝑖 𝑤𝑢 (𝑆𝑖−1) ≥ ∑︁ 𝑢∈𝑆𝑖 𝑤𝑢 (𝑆𝑖−1) > 2(1 + 𝜖)|𝑆𝑖 |𝑔(𝑆𝑖−1) (4) Combining Equation 3 and Equation 4, we have: 2(1 + 𝜖)|𝑆𝑖 |𝑔(𝑆𝑖−1) < 2𝑓 (𝑆𝑖−1) = 2|𝑆𝑖−1 |𝑔(𝑆𝑖−1) (5) For all 𝑖 ∈ [1, 𝑅], |𝑆𝑖 |< 1 1 + 𝜖 |𝑆𝑖−1 | (6) Then we have: |𝑉 |= |𝑆0 |> 1 1 + 𝜖 |𝑆1 |> . . . > ( 1 1 + 𝜖 ) 𝑅 |𝑆𝑅 | (7) Therefore, we have 𝑅 < log 1+𝜖 |𝑉 |. For the density metrics TDS and 𝑘CLiDS, we use the fact that each edge contributes to the peeling weight calculation exactly 𝑘 times due to the counting of each edge within every 𝑘-Clique in the graph (𝑘 = 3 for TDS). This multiplicity of edge contributions leads to the relationship: ∑︁ 𝑢∈𝑆𝑖−1 𝑤𝑢 (𝑆𝑖−1) ≤ 𝑘 𝑓 (𝑆𝑖−1), (8) We then adapt Equations 4-7 and the proof follows. □ Time Complexity. Algorithm 2 operates for at most 𝑅 = log 1+𝜖 |𝑉 | iterations, with each iteration scanning all vertices, taking 𝑂(|𝑉 |) time. As vertices are peeled, updates are necessitated for the peeling weights of their neighboring vertices. Over the course of 𝑅 iterations, there are no more than |𝐸 | such updates for DG, DW and FD. Consequently, Algorithm 3 incurs 𝑂((|𝐸 |+|𝑉 |) log 1+𝜖 |𝑉 |) node/edge weight updates. For DG, DW and FD, the update costs are 𝑂(1). For TDS and 𝑘CLiDS, we follow previous studies [11, 14] to compute the peeling weights, and the complexity is 𝑂(𝑘 |𝐸 |𝛼(𝐺) 𝑘−2 ), where 𝛼(𝐺) is the arboricity of the graph. Hence, the overall complexity is 𝑂(𝑘 |𝐸 |𝛼(𝐺) 𝑘−2 (|𝐸 |+|𝑉 |) log 1+𝜖 |𝑉 |). The cost of peeling weight up- dates is orthogonal to the Dupin framework. Approximation Ratio. Next, we are ready to show that Dupin provides a theoretical guarantee with a 𝑘(1 + 𝜖)-approximation for all density metrics studied in this work. We denote 𝑢𝑖 ∈ 𝑆𝑝 as the first vertex in 𝑆∗ peeled by Dupin, and 𝑆 ′ as the peeling subsequence starting from 𝑢𝑖 . Theorem 4.2. For the vertex set 𝑆𝑝 returned by Dupin and the optimal vertex set 𝑆∗, the relationship 𝑔(𝑆𝑝 ) ≥ 𝑔(𝑆∗) 𝑘(1+𝜖) holds. Proof. We first prove that ∀𝑢𝑖 ∈ 𝑆∗, 𝑤𝑢𝑖 (𝑆 ∗ ) ≥ 𝑔(𝑆∗). We prove it in contradiction. We assume that 𝑤𝑢𝑖 (𝑆 ∗ ) < 𝑔(𝑆∗). By peeling 𝑢𝑖 from 𝑆∗, we have the following. 𝑔(𝑆∗ \\\\ {𝑢𝑖 }) = 𝑓 (𝑆∗) −𝑤𝑢𝑖 (𝑆 ∗ ) |𝑆∗ |−1 > 𝑓 (𝑆∗) − 𝑔(𝑆𝑖 ) |𝑆∗ |−1 = 𝑓 (𝑆∗) − 𝑔(𝑆∗) |𝑆∗ |−1 = 𝑓 (𝑆∗) − 𝑓 (𝑆∗) |𝑆∗ | |𝑆∗ |−1 = 𝑔(𝑆∗) (9) Therefore, a better solution can be obtained by removing 𝑢𝑖 from 𝑆∗, which contradicts the fact that 𝑆∗ is the optimal solution. We can conclude that𝑤𝑢𝑖 (𝑆 ∗ ) ≥ 𝑔(𝑆∗). Next, we assume that 𝑢𝑖 ∈ 𝑆𝑝 is the first vertex in 𝑆∗ peeled by the peeling algorithm. Since 𝑆∗ is the optimal vertex set, we have 𝑔(𝑆∗) ≤ 𝑤𝑢𝑖 (𝑆 ∗ ), (10) because the optimal value 𝑔(𝑆∗) must be less than or equal to the weight contribution of any single vertex in 𝑆∗, in particular 𝑢𝑖 . Since 𝑆∗ ⊆ 𝑆 ′, it is clear that 𝑤𝑢𝑖 (𝑆 ∗ ) ≤ 𝑤𝑢𝑖 (𝑆 ′ ) (11) By the peeling condition, we have 𝑤𝑢𝑖 (𝑆 ′ ) ≤ 𝑘(1 + 𝜖)𝑔(𝑆 ′) (12) Therefore, we have 𝑔(𝑆∗) ≤ 𝑤𝑢𝑖 (𝑆 ∗ ) ≤ 𝑤𝑢𝑖 (𝑆 ′ ) ≤ 𝑘(1 + 𝜖)𝑔(𝑆 ′) ≤ 𝑘(1 + 𝜖)𝑔(𝑆𝑝 ) (13) Hence, we can conclude that 𝑔(𝑆𝑝 ) ≥ 𝑔(𝑆∗) 𝑘(1+𝜖) . □ Summary. Dupin can easily balance efficiency and the worst-case approximation ratio with a single parameter, 𝜖 . A larger 𝜖 allows more vertices to be peeled in parallel, reducing the number of peeling iterations. Meanwhile, the worst-case approximation ratio of parallel peeling is only affected by a factor of (1 + 𝜖) compared to the ratio for sequential peeling w.r.t. all density metrics studied in this work.',\n",
       " '5 LONG-TAIL PRUNING': 'We observe that the parallel peeling algorithm encounters two sig- nificant issues. First, there is a long-tail problem where subsequent iterations fail to produce denser subgraphs. Instead, each iteration only peels off a small fraction of vertices, which significantly impacts the overall runtime efficiency. Second, during batch peeling, each peeling iteration often results in disparate structures. These are pri- marily caused by the peeling of certain vertices, which leaves their neighboring structure disconnected and fragmented. This phenome- non affects the continuity and coherence of the subgraphs. As shown in Table 2, on the la dataset (the largest social network dataset tested in our experiments), DG, DW, and FD experience 24.67%, 46.84%, and 3.01% of rounds as ineffective long-tail iterations, severely limiting response time. Additionally, during the peeling process, if the batch size is large (e.g., 𝜖 = 0.5), approximately 25.34%, 29.46%, and 7.16% of nodes become disconnected and sparse, signifi- cantly impacting the quality of the detected subgraphs. To address the challenges of high iteration counts in peeling algorithms, Dupin introduces two long-tail pruning techniques: Global Peeling Optimization (GPO) to maintain a global peeling threshold, and Local Peeling Optimization (LPO) to improve the density of the current peeling iteration’s subgraph, thereby increasing the peeling threshold. We tested three algorithms, DG, DW, and FD, and found that they required 17,637, 150,223, and 112,074 iterations respectively, which is significantly high. By strategically pruning long-tail ver- tices and sparse vertices, the number of iterations can be reduced by up to 46.84%. Additionally, LPO can further reduce the number of iterations by up to 92.79%. These optimizations significantly enhance the efficiency of the peeling process, as discussed in this section. 5.',\n",
       " '1 Global Peeling Optimization': 'Initially, we define what constitutes a long-tail vertex during the peeling process. Peeling these long-tail vertices results in ineffective peeling iterations. To address this, we maintain a global maximum peeling threshold to determine which vertices can be peeled directly. When the global peeling threshold is greater than the current itera- tion’s threshold, we use the global peeling threshold for peeling. Algorithm 3: DupinGPO: Global Peeling Optimization Input:𝐺 = (𝑉 , 𝐸), a density metric 𝑔(𝑆), and 𝜖 ≥',\n",
       " '0 Output: A subset of vertices 𝑆𝑝 that maximizes the density metric 𝑔(𝑆) 1 𝑆0 ← 𝑉 /* Initialize the subset with all vertices */ 2 𝑖 ← 1 3 𝜏max ← 𝑔(𝑆 0 ) 𝑘(1+𝜖) /* Initialize the threshold */ 4 while 𝑆𝑖−1 ̸= ∅ do 5 𝜏 ← max{𝜏max, 𝑘(1 + 𝜖) · 𝑔(𝑆𝑖−1)} /* Peeling threshold */ 6 𝑈 ← {𝑢 | 𝑤𝑢 (𝑆𝑖−1) ≤ 𝜏 } /* Vertices to be peeled */ 7 𝑆𝑖 ← 𝑆𝑖−1 \\\\𝑈 /* Update the subset */ 8 𝜏max = max{𝜏max, 𝑔(𝑆𝑖 ) 𝑘(1+𝜖) } /* Refine the threshold */ 9 𝑖 ← 𝑖 + 1 10 return arg max𝑆𝑖 𝑔(𝑆𝑖 ) /* The subset that maximizes 𝑔(𝑆) */ Definition 5.1 (Long-Tail vertex). Given an 𝛼-𝑎𝑝𝑝𝑟𝑜𝑥𝑖𝑚𝑎𝑡𝑖𝑜𝑛 DSD algorithm and a set of vertices 𝑆 ⊆ 𝑉 , if𝑤𝑢 (𝑆) < 𝑔(𝑆∗) 𝛼 and 𝑆∗ is the optimal vertex set, then 𝑢 is a long-tail vertex. Lemma 5.1. If ∃𝑢 ∈ 𝑆𝑖 is a long-tail vertex, then 𝑆𝑖 ̸= 𝑆𝑃 . Proof. We prove it in contradiction by assuming that 𝑆𝑖 = 𝑆𝑃 . Since 𝑢 is a long-tail vertex in 𝑆𝑖 ,𝑤𝑢 (𝑆𝑖 ) < 𝑔(𝑆∗) 𝛼 < 𝑔(𝑆𝑃 ). By peeling 𝑢 from 𝑆𝑃 , we have the following. 𝑔(𝑆𝑃 \\\\ {𝑢}) = 𝑓 (𝑆𝑃 ) −𝑤𝑢 (𝑆𝑃 ) |𝑆𝑃 |−1 > 𝑓 (𝑆𝑃 ) − 𝑔(𝑆𝑃 ) |𝑆𝑃 |−1 = 𝑓 (𝑆𝑃 ) − 𝑓 (𝑆𝑃 ) |𝑆𝑃 | |𝑆𝑃 |−1 = 𝑔(𝑆𝑃 ) (14) A denser subgraph can be obtained by peeling 𝑢 from 𝑆𝑃 . This contradicts that 𝑆𝑃 is the optimal solution. Hence, 𝑆𝑖 ̸= 𝑆𝑃 . □ Identifying Long-Tail Vertices. Based on Lemma 5.2, we can also establish that a vertex𝑢 in set 𝑆 can be classified as a long-tail vertex if 𝑤𝑢 (𝑆) < 𝑔(𝑆∗) 𝑘(1+𝜖) . This insight allows us to implement a global peeling threshold, denoted as 𝜏max. During each iteration, we compare 𝑔(𝑆𝑖 ) 𝑘(1+𝜖) with 𝜏max. If the former exceeds the latter, Dupin updates 𝜏max to 𝑔(𝑆𝑖 ) 𝑘(1+𝜖) , thereby enhancing the efficiency of the peeling. Peeling with Global Threshold (Algorithm 3). Using FD as an example, we can set 𝑘 to 2. In Lemma 5.2, we recognize that long-tail vertices in set 𝑆𝑖 can be peeled directly during iteration 𝑖 for FD. To facilitate this, Dupin initiates with a global peeling threshold 𝜏max (Line 3). After identifying a denser subgraph post-peeling, 𝜏max is updated to 𝑔(𝑆𝑖 ) 2(1+𝜖) , refining the peeling process (Line 8). Example 5.1. In the initial depiction provided in the leftmost panel of Figure 6 (the 𝑖-th iteration), the density of the graph is 4.28. Traditionally, this scenario would necessitate two additional iterations to complete the peeling process, which could notably increase computational time. As illustrated, vertex 𝑢 cannot be peeled in the (𝑖 + 1)-th iteration and would typically require an extra iteration, the (𝑖 + 2)-th, for its removal. However, Dupin can promptly peel vertices that fall below this threshold by employing our algorithm’s global peeling threshold, which is calculated as half of the current density (𝜏max = 4.28/2 = 2.14). This approach enables the simultaneous peeling of three nodes in the (𝑖 + 1)-th iteration. Without this pruning strategy, an additional iteration would be essential to peel vertex 𝑢, underscoring our method’s enhanced efficiency in the peeling process. This optimization obviates the necessity for a prolonged iterative sequence. The example herein substantiates the efficacy of our tai- lored optimization for long-tail distribution, substantially enhancing the algorithm’s time efficiency.': '5.',\n",
       " '2 Local Peeling Optimization': 'Algorithm 4: DupinLPO: Local Peeling Optimization Input: A graph𝐺 = (𝑉 , 𝐸), a density metric 𝑔(𝑆), and 𝜖 ≥',\n",
       " '0 Output: A subset of vertices 𝑆𝑝 representing the densest subgraph 1 𝑆0 ← 𝑉 /* Initialize the subset with all vertices */ 2 𝜏max ← 𝑔(𝑆 0 ) 𝑘(1+𝜖) /* Initialize the threshold */ 3 while 𝑆𝑖−1 ̸= ∅ do 4 𝜏1 ← max{𝜏max, 𝑘(1 + 𝜖) · 𝑔(𝑆𝑖−1)} /* Peeling threshold */ 5 𝑈1 ← {𝑢 ∈ 𝑆 | 𝑤𝑢 (𝑆) ≤ 𝜏1 } 6 𝑆𝑖 ← 𝑆𝑖−1 \\\\𝑈1 7 while ∃𝑢 ∈ 𝑆𝑖 , 𝑤𝑢 (𝑆𝑖 ) < 𝑔(𝑆𝑖 ) do 8 𝜏2 ← max{𝜏max, 𝑔(𝑆𝑖 )} /* Update the threshold */ 9 𝑈2 ← {𝑢 | 𝑤𝑢 (𝑆𝑖 ) < 𝜏2 } 10 𝑆𝑖 ← 𝑆𝑖 \\\\𝑈2 11 𝜏max = max{𝜏max, 𝑔(𝑆𝑖 ) 𝑘(1+𝜖) } /* Update the threshold */ 12 return arg max𝑆𝑖 𝑔(𝑆𝑖 ) /* The subset that maximizes 𝑔(𝑆) */ It is important to note that each peeling iteration often results in disparate structures. This is primarily caused by the peeling of cer- tain vertices, which leaves their neighboring structures disconnected and fragmented. This not only affects the density of the detected sub- graph but also lowers the peeling threshold. Lemma 5.2 establishes a foundational property of vertices within a dense graph. Lemma 5.2. ∀𝑢 ∈ 𝑆 , if𝑤𝑢 (𝑆) < 𝑔(𝑆), 𝑔(𝑆 \\\\ {𝑢}) > 𝑔(𝑆). Proof. By peeling 𝑢 from 𝑆 , we have the following. 𝑔(𝑆 \\\\ {𝑢}) = 𝑓 (𝑆) −𝑤𝑢 (𝑆) |𝑆 |−1 > 𝑓 (𝑆) − 𝑔(𝑆𝑖 ) |𝑆 |−1 = 𝑓 (𝑆) − 𝑔(𝑆) |𝑆∗ |−1 = 𝑓 (𝑆) − 𝑓 (𝑆) |𝑆 | |𝑆 |−1 = 𝑔(𝑆) (15) Therefore, a denser graph can be obtained by peeling 𝑢 from 𝑆 . □ According to Lemma 5.2, we introduce local peeling optimization within the iteration. After each peeling iteration, if the peeling weight of a vertex is smaller than the current density, Dupin will trim it. We implement a local peeling optimization (Lines 7-10, Algorithm 4) before the next iteration. After a vertex 𝑢 is peeled, Dupin invokes updateNgh to update the peeling weights of 𝑢’s neighboring nodes. If any of these neighbors have a peeling weight less than the current density, they are also peeled, thereby increasing the density of the current iteration. Consequently, when an iteration yields globally optimal results, a denser structure is obtained. The benefits of this approach extend beyond density improvements; performance is also enhanced. By trimming these vertices, we reduce the number of vertices to traverse in subsequent iterations, and a higher density enables more efficient iterations due to the possibility of using larger peeling thresholds. Lemma 5.3. If 𝑢𝑖 ∈ 𝑆𝑝 is the first vertex in 𝑆∗ removed by Dupin, 𝑢𝑖 will not be removed during the local peeling optimization process. Proof. Assume, for the sake of contradiction, that 𝑢𝑖 is trimmed during the local peeling optimization process. Let 𝑆 ′ be the set of vertices left before 𝑢𝑖 is trimmed. By definition, we have: 𝑤𝑢𝑖 (𝑆 ′ ) < 𝑔(𝑆∗). Since 𝑢𝑖 is the first vertex in 𝑆∗ to be removed, 𝑆∗ ⊆ 𝑆 ′, therefore: 𝑤𝑢𝑖 (𝑆 ∗ ) ≤ 𝑤𝑢𝑖 (𝑆 ′ ) < 𝑔(𝑆∗). This contradicts Lemma 5.2, which states that ∀𝑢𝑖 ∈ 𝑆∗: 𝑤𝑢𝑖 (𝑆 ∗ ) ≥ 𝑔(𝑆∗). Otherwise, a better solution can be obtained by removing 𝑢 from 𝑆∗, which contradicts the fact that 𝑆∗ is the optimal solution. Thus, 𝑢𝑖 cannot be trimmed. Hence,𝑢𝑖 will not be removed during the local peeling optimization process. □ Due to Lemma 5.3, 𝑢𝑖 will only be removed during the peeling process. Hence, the approximation ratios (Theorem 4.2) for different density metrics are preserved. 6 EXPERIMENTAL STUDY Our experiments are run on a machine that has an X5650 CPU, 512 GB RAM. The implementation is made memory-resident and implemented in C++. All codes are compiled by GCC-9.3.0 with -𝑂3. Datasets. We report the datasets used in our experiment in Table 3. One industrial dataset is from Anon (gfg). Datasets links-anon(la) and Twitter-rv (rv)were obtained from Stanford NetworkDataset Collection [31]. Datasets kron-logn21(kron), soc-twitter(soc), Web-uk-2005(uk) and Web-sk-2005(sk) were obtained from Network Data Repository [38]. Dataset bio was from the BIOMINE [35]. Baseline Methods. In this section, we introduce five commonly used density metrics pivotal in analyzing network structures:DG [6], DW [23], FD [24], TDS [46], and𝑘CLiDS [14].We implemented these metrics in our framework, Dupin, and compared their performance with frameworks including Spade [28], kCLIST [14], GBBS [16], PBBS [41], PKMC [32], FWA [15] and ALENEX [44]. kCLIST [14] and PBBS [41] are parallel 𝑘CLiDS algorithms, while GBBS [16] supports the DG density metric. Although GBBS does not support weighted graphs, we precompute the peeling weights offline and then import them into GBBS. The time spent on this offline computation is not included in our reported runtime. FWA, ALENEX and PKMC support the DG density metric. For clarity, we denote the incremental peeling process, which involves the iterative removal of graph elements based on density scores, as Spade-X, where X corresponds to each metric, under the same experimental condi- tions described in [28], where the batch size is set to 1K, consistent with the default setting in [28]. We report the average runtime per batch for Spade. DupinGPO-X and DupinLPO-X represent our par- allel peeling algorithms that incorporate global and local peeling optimization. Default Settings. In our experiments, the parameter 𝜖 is set to 0.1 by default, and the number of threads 𝑡 is configured to 128. These settings are used consistently unless otherwise specified. 6.1 Efficiency of Dupin Spade vs Dupin. Our initial comparison focuses on the performance disparity between the incremental and parallel peeling algorithms. Specifically, Dupin-DG (resp. Dupin-DW, Dupin-FD, Dupin-TDS, andDupin-𝑘CLiDS) demonstrates a speedup factor of 6.97 (resp. 7.71, 7.71, 7.65, and 8.46) over Spade-DG (resp. Spade-DW, Spade-FD, Spade-TDS, and Spade-𝑘CLiDS). Notably, Spade-TDS and Spade𝑘CLiDS only complete computations within 7,200 seconds on gfg. The bottleneck for Spade arises in larger graphs where initial calculations of triangle and 𝑘-Clique counting are unable to finish. Dupin outpaces Spade primarily because Spade suffers from frequent re- ordering of peeling sequences when there are many increments, which proves to be slower than recalculating. GBBS vs Dupin. Previous works have not provided simultaneous support for all density metrics. To bridge this gap, we implemented DG, DW, and FD within the parallel GBBS framework. On average, Dupin-DG, Dupin-DW, and Dupin-FD demonstrated speedup factors of 6.33, 12.51, and 17.07 times, respectively, over GBBS-DG, GBBS-DW, andGBBS-FD. This performance advantage is attributed to the fundamental difference in the peeling process between the two frameworks. In GBBS, the basic unit of each peel is a bucket, where all nodes within the same bucket have an identical peeling weight. This design limits the parallelism in weighted graphs like those using DW and FD, where many buckets may contain only a single node. In contrast, Dupin peels batches of nodes at each step, achieving higher parallelism and thus consistently outperforming GBBS, particularly in weighted graph scenarios where the speedup is even more pronounced. kCLIST vs PBBS vs Dupin. Both kCLIST and PBBS support the density metrics TDS and 𝑘CLiDS. In our comparisons, Dupin demonstrates notable performance advantages. Specifically, Dupin-TDS is faster than kCLIST-TDS and PBBS-TDS by 39.85 and 62.71 times, respectively. Furthermore, Dupin-𝑘CLiDS also outperforms kCLIST𝑘CLiDS by 4.16 times. A significant finding is that PBBS-𝑘CLiDS fails to complete computations within 7200 seconds on most datasets, highlighting the efficiency of Dupin in handling more complex met- rics under stringent time constraints. Impact of DupinGPO. Next, we evaluate the acceleration effect of DupinGPO, using Dupin as a baseline for comparison. Notably, DupinGPO-DG is found to be 1.14 times faster, DupinGPO-DW is 1.11 times faster,DupinGPO-TDS is 1.10 times faster, andDupinGPO𝑘CLiDS is 1.35 times faster than their respective Dupin implementations. The acceleration is particularly significant for 𝑘CLiDS on larger datasets, such as soc, rv, kron, and sk. DupinGPO-𝑘CLiDS is 1.40, 1.54, 1.51, and 1.61 times faster than Dupin-𝑘CLiDS on soc, rv, kron, and sk, respectively. This is attributed to the greater number of iterations required due to the larger dataset sizes and the higher count of 𝑘-Cliques, necessitating more peeling iterations. In such scenarios,DupinGPO effectively prunes the tail iterations early, thereby reducing the number of rounds needed. Impact of DupinLPO. We next compared the efficiency of Dupin andDupinLPO. On average, thoughDupinLPO-DGwas 10.09% slower than Dupin-DG, the speed is still comparable. For example, on the soc dataset, DupinLPO-DG was 5.94% faster than Dupin-DG. This is because DupinLPO requires some lock operations to update the peeling weights of neighboring vertices during local peeling optimization. However, the advantage is that DupinLPO can detect up to 26.26% denser subgraphs, as detailed in Section 6.2. On larger datasets, such as soc, rv, and la, the vertices being trimmed have relatively low correlations with each other, so the impact of locks is minimal. For example, on the soc (resp. rv), DupinLPO-DG was 5.94% (resp. 9.94%) faster than Dupin-DG. In contrast, on smaller datasets, such as bio and kron, the extra cost of locks is higher. Impact of the Number of Threads 𝑡 . All methods are influenced by the concurrency level, i.e., the number of threads. Generally, the greater the number of threads, the faster the execution speed. In our experiments, using the la dataset as a benchmark, we varied the number of threads 𝑡 from 2 to 128 to observe the impact on runtime performance. For GBBS-DG, runtime stabilizes and accelerates by 3.33 times as thread count increases from 2 to 8. For GBBS-DW, performance deteriorates when the thread count exceeds 32, but from 2 to 32 threads, it accelerates by 3.50 times. Similarly, GBBS-FD shows no significant change in runtime beyond 32 threads, with an acceleration of 3.09 times from 2 to 32 threads. GBBS-TDS and GBBS-𝑘CLiDS fail to complete computations on the la dataset. In contrast, all five methods tested onDupin exhibit good scalability. As the number of threads increases from 2 to 128,Dupin-DG accelerates by 9.91 times, Dupin-DW by 12.69 times, Dupin-FD by 13.03 times, and Dupin-TDS by 28.06 times. Although Dupin-𝑘CLiDS fails to produce results when the thread count is below 16, it accelerates by 1.32 times from 32 to 128 threads. These results demonstrate Dupin’s strong scalability across varied threading levels. 6.2 Effectiveness of Dupin In addition to performance speedup, we evaluate the density of the subgraphs detected by different frameworks. Detailed density comparisons are presented in Table 5. GBBS vs Spade vs Dupin. Firstly, we compare the densities of the subgraphs identified by GBBS-DG (respectively GBBS-DW, and GBBS-FD) to those detected by Dupin in parallel execution. Our experimental results indicate that Dupin maintains comparable subgraph densities with GBBS, signifying that the increase in com- putational efficiency does not compromise the accuracy of the de- tected subgraphs. Specifically, the density of the subgraph detected byGBBS-DG (respectivelyGBBS-DW, andGBBS-FD) is 7.08% (resp. 6.48% and 7.43%) greater than that detected by Dupin-DG (respectively Dupin-DW, and Dupin-FD) on average. Spade shares similar densities with GBBS. Due to Spade’s tendency to accumulate er- rors over time, its density can become inflated. We will explain its limitations in practical applications in more detail in the case study. kCLIST vs PBBS vs Dupin. a) For 𝑘CLiDS, PBBS-𝑘CLiDS could not complete on most of our test datasets, so we did not compare their densities. kCLIST-𝑘CLiDS could not complete on the sk dataset. On the other datasets, Dupin-𝑘CLiDS’s density was only 6.97% smaller than kCLIST-𝑘CLiDS. On some datasets, such as bio and kron,Dupin𝑘CLiDS even detected subgraphs with greater density. b) For TDS, PBBS-TDS could detect subgraphs on smaller graphs but failed on billion-scale graphs such as rv, sk, and la. Dupin-TDS’s density was 2.03% greater than kCLIST-TDS. Therefore, we conclude that Dupin is comparable to existing parallel methods in terms of the density of the detected dense subgraphs. Ablation Study of Dupin. This experiment delves into the effectiveness of our optimizations, DupinGPO and DupinLPO. Our analysis primarily focuses on comparing the densities of dense subgraphs detected by both DupinGPO and DupinLPO in various settings. DupinGPO achieves the same density as Dupin across five den- sity metrics because it trims the long tail, enhancing efficiency. For DupinLPO-DG (resp. DupinLPO-DW, DupinLPO-FD, DupinLPOTDS, and DupinGPO-𝑘CLiDS), we observe that the detected sub- graphs exhibit densities 11.09% (resp. 7.32%, 8.00%, 1.01%, and 26.26%) greater than those detected by Dupin-DG, Dupin-DW, Dupin-FD, Dupin-TDS, andDupin-𝑘CLiDS, respectively, demonstrating notable outcomes. This is because DupinLPO trims the graph in each iteration, resulting in denser subgraphs compared to Dupin. Impact of the Approximation Ratio 𝜖. The approximation ratio 𝜖 serves to control accuracy. In our experiment, we vary 𝜖 from 0.1 to 1 to investigate its influence on the accuracy of Dupin. We observe that as 𝜖 increases from 0.1 to 1, the density of subgraphs detected by Dupin, DupinGPO, and DupinLPO generally decreases. ForDG, the densities ofDupin-DG,DupinGPO-DG, andDupinLPODG decrease by 22.71%, 22.71%, and 11.99%, respectively. The trends for DW and DG are similar, with the worst performance observed at 𝜖 = 0.6 for both density metrics. The trends for FD, TDS, and 𝑘CLiDS are also similar. DupinLPO detects denser subgraphs across most density metrics and is less affected by 𝜖 , due to its iterative trimming. For example, for FD, DupinLPO’s density decreases by only 6.23%, while DupinGPO and Dupin’s densities decrease by 17.37%.': '6.',\n",
       " '3 Case study': 'We also compare the performance in real-world scenarios, focusing on three key aspects: accuracy, latency, and prevention ratio. Accuracy. As discussed in Section 1, although Spade can quickly respond to new transactions, it tends to accumulate errors over time. Our analysis utilizes transaction data from Anon, with the transaction network comprising |𝐸 |= 2 billion edges. Under the incremental computation framework of Spade, the assumption is made that the weights of existing edges remain constant, thereby neglecting the impact of newly inserted edges on these weights, leading to error accumulation. For instance, using FD density metric, we observe that errors accumulate up to 24.4% within a day and increase to 30.3% over a week, as illustrated in Figure 9. Although Dupin sacrifices some accuracy to achieve parallel computational efficiency, our experiments show that Dupin’s errors remain relatively stable between 3% and 5%. Hence, the detection precision of Spade declines from 87.9% to 68.3%, while Dupin’s precision remains above 86%. Latency and Prevention Ratio. If a transaction is identified as fraudulent, subsequent related transactions are blocked to mitigate potential losses. We define the ratio of successfully prevented sus- picious transactions to the total number of suspicious transactions as R. As shown in Figure 6, the prevention ratio R continues to decrease as latency increases on Anon’s datasets. Using the default density metric, FD, in Anon, our results show thatDupin-FD can prevent 94.5% of fraudulent activities, GBBS-FD can prevent 3.0%, and Spade-FD can prevent 45.3%. The reason for the lower performance of GBBS-FD is its low parallelism in the peeling process, which results in long peeling times and thus an average latency of 6,014 seconds, delaying the prevention of many fraudulent activities. Although Spade-FD performs well in reordering techniques on smaller datasets, in industrial scenarios with billion-scale graphs, latency significantly worsens due to the extensive reordering range required in the peeling sequences as normal users transition to fraudsters, increasing the cost of incremental computations. Additional tests were conducted with other density metrics; for DG, Dupin-DG can prevent 78.8%, while other methods prevent varying amounts. Similarly, for DW, Dupin-DW can prevent 86.1%, while other methods demonstrate different prevention capabilities.',\n",
       " '7 CONCLUSION AND FUTUREWORKS': 'In this paper, we presented Dupin, a novel parallel fraud detection framework tailored for massive graphs. Our experiments demonstrated that Dupin’s advanced peeling algorithms and long-tail pruning techniques, DupinGPO and DupinLPO, significantly enhance the detection efficiency of dense subgraphs without sacrificing accuracy. Comparative studies with GBBS, PBBS, kCLIST, Spade, and Dupin highlighted Dupin’s superior performance in terms of com- putational efficiency and accuracy. Through case studies, we also validated that Dupin achieves lower latency and higher detection ac- curacy compared to existing fraud detection systems on billion-scale graphs. Overall, Dupinproves to be a powerful tool for large-scale fraud detection, providing a scalable and efficient solution for real-time applications. Future work will explore the integration of additional suspiciousness functions and further optimization to handle even larger datasets and more complex fraud scenarios.',\n",
       " 'REFERENCES': '[1] Distil networks: The 2019 bad bot report. https://www.bluecubesecurity.com/wp- content/uploads/bad-bot-report-2019LR.pdf. [2] B. Bahmani, R. Kumar, and S. Vassilvitskii. Densest subgraph in streaming and mapreduce. Proceedings of the VLDB Endowment, 5(5), 2012. [3] Y. Ban, X. Liu, T. Zhang, L. Huang, Y. Duan, X. Liu, and W. Xu. Badlink: Combining graph and information-theoretical features for online fraud group detection. arXiv preprint arXiv:1805.10053, 2018. [4] A. Beutel, W. Xu, V. Guruswami, C. Palow, and C. Faloutsos. Copycatch: stopping group attacks by spotting lockstep behavior in social networks. In Proceedings of the 22nd international conference on World Wide Web, pages 119–130, 2013. [5] D. Boob, Y. Gao, R. Peng, S. Sawlani, C. Tsourakakis, D.Wang, and J.Wang. Flowless: Extracting densest subgraphs without flow computations. In Proceedings of The Web Conference 2020, pages 573–583, 2020. [6] M. Charikar. Greedy approximation algorithms for finding dense components in a graph. In International Workshop on Approximation Algorithms for Combinatorial Optimization, pages 84–95. Springer, 2000. [7] M. Charikar. Greedy approximation algorithms for finding dense components in a graph. In Approximation Algorithms for Combinatorial Optimization, Third International Workshop, APPROX 2000, Saarbrücken, Germany, September 5-8, 2000, Proceedings, volume 1913 of Lecture Notes in Computer Science, pages 84–95. Springer, 2000. [8] C. Chekuri, K. Quanrud, and M. R. Torres. Densest subgraph: Supermodularity, iterative peeling, and flow. In Proceedings of the'}"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "parse_document(full_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "camelot-py",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
